{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "https://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp.add_pipe('merge_entities', after = 'ner')\n",
    "nlp.add_pipe('entityfishing', after = 'merge_entities')\n",
    "\n",
    "display(pd.DataFrame({'spaCy pipeline components': nlp.pipe_names}).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def get_dataset_dirpath(cwd):\n",
    "    \n",
    "    two_up = os.path.dirname(os.path.dirname(cwd))\n",
    "    \n",
    "    return os.path.join(two_up, 'dataset')\n",
    "\n",
    "dataset_dirpath = get_dataset_dirpath(os.getcwd())\n",
    "\n",
    "binladenpath = os.path.join(dataset_dirpath, 'Osama bin Laden/')\n",
    "bushpath = os.path.join(dataset_dirpath, 'George Bush/')\n",
    "\n",
    "\n",
    "Bush_FileList = [\n",
    "    '20010914-Remarks at the National Day of Prayer & Remembrance Service.txt',\n",
    "    '20010915-First Radio Address following 911.txt',\n",
    "    '20010917-Address at Islamic Center of Washington, D.C..txt',\n",
    "    '20010920-Address to Joint Session of Congress Following 911 Attacks.txt',\n",
    "    '20010911-911 Address to the Nation.txt',\n",
    "    '20011007-Operation Enduring Freedom in Afghanistan Address to the Nation.txt',\n",
    "    '20011011-911 Pentagon Remembrance Address.txt',\n",
    "    '20011011-Prime Time News Conference on War on Terror.txt',\n",
    "    '20011026-Address on Signing the USA Patriot Act of 2001.txt',\n",
    "    '20011110-First Address to the United Nations General Assembly.txt',\n",
    "    '20011211-Address to Citadel Cadets.txt',\n",
    "    '20011211-The World Will Always Remember 911.txt',\n",
    "    '20020129-First (Official) Presidential State of the Union Address.txt'\n",
    "]\n",
    "\n",
    "text = ''\n",
    "\n",
    "raw = \"\"\n",
    "for file in Bush_FileList:\n",
    "    with open(os.path.join(bushpath, file), 'r') as text:\n",
    "        raw = raw + text.read()\n",
    "        \n",
    "print(f'doc length: {len(raw)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process text using spaCy\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from typing import List\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def create_doc_array(doc: Doc) -> List:    \n",
    "    \n",
    "    doc_array = []\n",
    "    \n",
    "    for sent in tqdm(doc.sents):\n",
    "        \n",
    "        sent_array = []\n",
    "        \n",
    "        for token in sent:\n",
    "            \n",
    "            if token.is_punct:\n",
    "                continue\n",
    "\n",
    "            if token.is_stop:\n",
    "                continue\n",
    "\n",
    "            if token.is_space:\n",
    "                continue\n",
    "\n",
    "            if '\\n' in token.text:\n",
    "                continue\n",
    "                \n",
    "            text = token.lemma_.lower()\n",
    "            \n",
    "            if token._.normal_term:\n",
    "                text = token._.normal_term.lower()\n",
    "                \n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "            sent_array.append(text)\n",
    "                        \n",
    "        yield sent_array\n",
    "\n",
    "doc = nlp(raw)\n",
    "doc_array = create_doc_array(doc)\n",
    "\n",
    "data = {\n",
    "    'Original': [sent.text for sent in list(doc.sents)[0:4]],\n",
    "    'Pre-Processed': [' '.join([token for token in sent]) for sent in list(doc_array)[0:4]]\n",
    "}\n",
    "\n",
    "display(pd.DataFrame(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "doc_array = list(create_doc_array(doc))\n",
    "model = Word2Vec(sentences=doc_array, vector_size=500, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "def most_similar_terms(seed_terms, model):\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for seed_term in seed_terms:\n",
    "        try:\n",
    "            data[seed_term] = [f'{sim[0].title()}, ({round(sim[1], 3)})' for sim in model.wv.most_similar(seed_term.lower(), topn=10)]\n",
    "        except:\n",
    "            print(f'{seed_term} not in vocab')\n",
    "            pass\n",
    "        \n",
    "    return pd.DataFrame(data)\n",
    "    \n",
    "seed_terms_good = ['friend', 'good']\n",
    "seed_terms_bad = ['enemy', 'terrorist', 'terror', 'bad', 'evil', 'murder']\n",
    "seed_terms = seed_terms_good + seed_terms_bad\n",
    "df = most_similar_terms(seed_terms, model)\n",
    "display(df)\n",
    "dfi.export(df, 'seed_terms.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgroups = ['al Qaeda', 'Taliban', 'Usama bin Laden', 'the Egyptian Islamic Jihad', 'the Islamic Movement of Uzbekistan', 'North Korea', 'Iran', 'Iraq', 'axis of evil']\n",
    "df = most_similar_terms(outgroups)\n",
    "display(df)\n",
    "dfi.export(df, 'outgroup_terms.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for outgroup in outgroups:\n",
    "        \n",
    "    data[outgroup] = {}\n",
    "    data[outgroup]['count'] = 0\n",
    "    \n",
    "    for sentence in create_doc_array(doc):\n",
    "        if outgroup.lower() in sentence:\n",
    "            \n",
    "            index = sentence.index(outgroup.lower())\n",
    "\n",
    "            if index - 5 >= 0:\n",
    "                left = index - 5\n",
    "            else:\n",
    "                left = 0\n",
    "\n",
    "            if index + 5 <= len(sentence):\n",
    "                right = index + 5\n",
    "            else:\n",
    "                right = len(sentence)\n",
    "\n",
    "            new_sent = [token for token in sentence[left : right]]\n",
    "            \n",
    "            data[outgroup]['count'] += 1\n",
    "            data[outgroup]['Co-Occurring with seed term'] = outgroup in new_sent\n",
    "            \n",
    "display(pd.DataFrame(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingroups = ['America', 'Americans', 'Great Britain', 'The United States', 'The United States of America', 'the United States']\n",
    "df = most_similar_terms(ingroups)\n",
    "display(df)\n",
    "dfi.export(df, 'ingroup_terms.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = ['The United States of America', 'Americans', 'al Qaeda', 'Taliban', 'Usama bin Laden', 'the Egyptian Islamic Jihad', 'the Islamic Movement of Uzbekistan']\n",
    "df = most_similar_terms(entities)\n",
    "display(df)\n",
    "dfi.export(df, 'entities.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_of_interest = ['GPE', 'ORG', 'NORP', 'PERSON']\n",
    "ents_refined = [ent.text for ent in doc.ents if ent.label_ in ents_of_interest]\n",
    "pd.set_option('display.max_rows', 20)\n",
    "display(most_similar_terms(ents_refined).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot(model, set_array):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    print('creating tokens and labels')\n",
    "#     for word in model.wv.key_to_index:\n",
    "#         tokens.append(model.wv[word])\n",
    "#         labels.append(word)\n",
    "    \n",
    "    set_array = [token.lower() for token in set_array]\n",
    "    \n",
    "    for word in set_array:\n",
    "        if word in model.wv.index_to_key:\n",
    "            tokens.append(model.wv[word])\n",
    "            labels.append(word)\n",
    "    \n",
    "    \n",
    "    print('building tsne model')\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    \n",
    "    print('constructing graph')\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(10, 10)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        \n",
    "        if labels[i] in set_array:\n",
    "            plt.annotate(labels[i],\n",
    "                xy=(x[i], y[i]),\n",
    "                xytext=(5, 2),\n",
    "                textcoords='offset points',\n",
    "                ha='right',\n",
    "                va='bottom')\n",
    "            \n",
    "    plt.savefig('vector_distribution.png')\n",
    "    plt.show()\n",
    "    \n",
    "terms = seed_terms + ingroups + outgroups\n",
    "tsne_plot(model, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = set([span.text for span in doc.noun_chunks])\n",
    "tsne_plot(model, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgroups = ['Taliban', 'al Qaeda']\n",
    "seed_terms = ['terrorist', 'terror', 'murder', 'regime']\n",
    "    \n",
    "def get_word_contexts(iterable, terms_of_interest, seed_terms):\n",
    "    \n",
    "    for sentence in iterable:\n",
    "    \n",
    "        for term in terms_of_interest:\n",
    "\n",
    "            term = term.lower()\n",
    "\n",
    "            if term in sentence:\n",
    "\n",
    "                index = sentence.index(term)\n",
    "\n",
    "                if index - 5 >= 0:\n",
    "                    left = index - 5\n",
    "                else:\n",
    "                    left = 0\n",
    "\n",
    "                if index + 5 <= len(sentence):\n",
    "                    right = index + 5\n",
    "                else:\n",
    "                    right = len(sentence)\n",
    "                    \n",
    "                new_sent = [token for token in sentence[left : right]]\n",
    "                \n",
    "                for seed_term in seed_terms:\n",
    "                    new_sent.insert(0, seed_term in new_sent)\n",
    "                    break\n",
    "                    \n",
    "                yield new_sent\n",
    "                \n",
    "with pd.option_context('display.max_rows', 100, 'display.max_colwidth', None):\n",
    "        \n",
    "    display(pd.DataFrame(get_word_contexts(create_doc_array(doc), outgroups, seed_terms)).fillna(value=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_array = [outgroups, ingroups, seed_terms_bad]\n",
    "\n",
    "\n",
    "for terms in terms_array:\n",
    "    data = {}\n",
    "    for term in terms:\n",
    "        data[term] = {}\n",
    "        data[term]['Occurence count'] = doc.text.lower().count(term.lower())\n",
    "    \n",
    "    display(pd.DataFrame(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy_v3",
   "language": "python",
   "name": "spacy_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
