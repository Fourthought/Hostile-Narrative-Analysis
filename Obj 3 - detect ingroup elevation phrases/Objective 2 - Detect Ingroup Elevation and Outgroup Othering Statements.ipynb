{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import importlib\n",
    "import cndlib.pipeline\n",
    "importlib.reload(cndlib.pipeline)\n",
    "\n",
    "cnd = cndlib.pipeline.CND()\n",
    "\n",
    "print([name for name in cnd.nlp.pipe_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import importlib\n",
    "from IPython.display import clear_output\n",
    "import cndlib.cndobjects\n",
    "importlib.reload(cndlib.cndobjects)\n",
    "\n",
    "\n",
    "dirpath = r'C:\\\\Users\\\\spa1e17\\\\OneDrive - University of Southampton\\\\hostile-narrative-analysis\\\\dataset'\n",
    "\n",
    "orators = cndlib.cndobjects.Dataset(cnd, dirpath)\n",
    "clear_output(wait=True)\n",
    "\n",
    "display(orators.summarise())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create .csv Files of Sentences of Each Orator for Annotation\n",
    "\n",
    "In this experiment we test sentiment analysis to detect the ingroup elevation and outgroup other phrases. For this experiment, each sentence of Bush and bin Laden’s datasets have been annotated as either ingroup elevation or outgroup othering. Accordingly, annotation was based on two criteria. Firstly, the sentence must contain reference to a named entity, whether explicitly or by either noun phrase or pronoun. Secondly, the reference must be associated with a term in the sentence which either elevates or others the reference entity. For example, with an implicit reference to al Qaeda, the following two sentences from Bush are annotated as othering, “These terrorists don't represent peace”, “They represent evil and war”. Equally, from both datasets the clauses, “God bless America” or “Allah blessed be upon him” are annotated as elevation.\n",
    "\n",
    "An extra annotation was also added for hostile and anti-sematic sentences. Hostile sentences are those containing a threat of violence. For example from bin Laden, “And whoever has killed our civilians, then we have the right to kill theirs”, or from Bush, “We are sending a signal to the world as we speak that if you harbor a terrorist, there will be a price to pay.”. Some hostile sentences are veiled threat, but in the context of the narrative are determined to be threatening. Bin Laden’s explicit outgroup are Jews and Israel, as such, may of his sentences have been annotated as Ant-Sematic. The International Holocaust Remembrance Alliance (IHRC) definition of anti-Semitism was used as a guide for these annotations . An example of one annotation is, “Behind them stand the Jews, who control your policies, media and economy”. In this sentence bin Laden suggests Jewish people control the wealthy Americans, which conforms with the IHRC’s anti-Semitic characterisation of Jewish control of “Jews controlling the media, economy, government or other societal institutions”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "docs = {\"bush\" : {\"name\" : \"George Bush\", \"filename\" : \"bush_sentences_gold.txt\", \"sentences\" : []},\n",
    "       \"laden\" : {\"name\" : \"Osama bin Laden\", \"filename\" : \"bush_sentences_gold.txt\", \"sentences\" : []},\n",
    "       \"king\" : {\"name\" : \"Martin Luther King\", \"sentences\" : []},\n",
    "       \"hitler\" : {\"name\" : \"Adolf Hitler\", \"sentences\" : []}}\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "\n",
    "for orator, texts in orators.orators_dict.items():\n",
    "    \n",
    "    \n",
    "    for text in texts.texts:\n",
    "        for sentence in text.doc.sents:\n",
    "            sent = {\"function\" : \"\", \"hostile\" : \"\", \"text\" : sentence.text.replace('\\n', ' ').strip()}\n",
    "            docs[orator]['sentences'].append(sent)\n",
    "    \n",
    "    filename = f\"{orator}_sentences.csv\"\n",
    "    df = pd.DataFrame(docs[orator]['sentences'])\n",
    "    filepath = os.path.join(dirpath, filename)\n",
    "    \n",
    "    df.to_csv(filepath, sep=',',index=False)\n",
    "\n",
    "            \n",
    "pd.DataFrame([{\"Orator\" : doc['name'], \n",
    "               \"Number of Sentences\" : len(doc['sentences'])} \n",
    "              for doc in docs.values()\n",
    "             ])\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Annotation Results for Scoring by Sentiment Analysis APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from cndlib.visuals import display_side_by_side\n",
    "\n",
    "docs = {\n",
    "    \"bush\" : {\"name\" : \"George Bush\", \"filename\" : \"bush_sentences_gold.txt\", \"sentences\" : None},\n",
    "    \"laden\" : {\"name\" : \"Osama bin Laden\", \"filename\" : \"laden_sentences_gold.txt\", \"sentences\" : None}\n",
    "}\n",
    "\n",
    "for orator in docs.values():\n",
    "    filename = filename = os.path.join(os.getcwd(), orator['filename'])\n",
    "\n",
    "    with open(filename, newline = \"\") as fp:\n",
    "        data = csv.DictReader(fp, delimiter = '\\t')\n",
    "\n",
    "        orator['sentences'] = [row for row in data]\n",
    "        \n",
    "df = pd.DataFrame()\n",
    "for orator in docs.values():\n",
    "    \n",
    "    summary = []\n",
    "    summary.append({\"Number of Sentences\" : len(orator['sentences'])})\n",
    "    df2 = pd.DataFrame(orator['sentences'])\n",
    "    summary.extend([{f\"{k.title()} Sentences\" : str(v) for k, v in df2['function'].value_counts().items() if k}])\n",
    "    summary.extend([{f\"{k.title()} Sentences\" : str(v) for k, v in df2['hostile'].value_counts().items() if k}])\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame({k:v for x in summary for k,v in x.items()}, index = [orator['name']])])\n",
    "    \n",
    "display_side_by_side([df.fillna('')], [\"Elevation and Othering Annotation Results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Google API Results\n",
    "\n",
    "https://cloud.google.com/natural-language/docs/basics#:~:text=score%20of%200.8%20.-,Interpreting%20sentiment%20analysis%20values,the%20length%20of%20the%20document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# instantiate Google Sentiment Analysis\n",
    "from google.cloud import language_v1\n",
    "client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    \n",
    "# iterate through each orator() object\n",
    "for orator in docs.values():\n",
    "   \n",
    "    # iterate over each Text() of the orator() object\n",
    "    for sent_obj in tqdm(orator['sentences'], total = len(orator['sentences']), desc = orator['name']):\n",
    "\n",
    "        text = sent_obj['text']\n",
    "#         document = language_v1.Document(content=text, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "        \n",
    "#         sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment\n",
    "#         sent_obj['google sentiment score'] = sentiment.score\n",
    "#         sent_obj['google sentiment magnitude'] = sentiment.magnitude\n",
    "\n",
    "display(pd.DataFrame([obj for obj in docs['laden']['sentences']]))\n",
    "    \n",
    "# google_document_results = document_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get IBM Watson API Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, SentimentOptions, EmotionOptions\n",
    "\n",
    "apikey = 'D3ptPkoLkoQNJvIav-reiA5137cr3m8Y1f-mhX1bLile'\n",
    "url = 'https://api.eu-gb.natural-language-understanding.watson.cloud.ibm.com/instances/204e6ba7-952c-41ae-99e9-fe4e8208bfde'\n",
    "\n",
    "authenticator = IAMAuthenticator(apikey)\n",
    "service = NaturalLanguageUnderstandingV1(version='2019-07-12', authenticator=authenticator)\n",
    "service.set_service_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for orator in docs.values():\n",
    "    \n",
    "    for sent_obj in tqdm(orator['sentences'], total = len(orator['sentences']), desc = orator['name']):\n",
    "  \n",
    "        text = sent_obj['text']\n",
    "        analytics = service.analyze(text=text, features=Features(\n",
    "                                    sentiment=SentimentOptions(), \n",
    "                                    emotion=EmotionOptions()),\n",
    "                                    language = \"en\").get_result()\n",
    "        \n",
    "        sent_obj['watson sentiment'] = analytics['sentiment']['document']['score']\n",
    "        emotion = analytics['emotion']['document']['emotion']\n",
    "        sent_obj.update({f\"Watson {k}\" : v for k, v in emotion.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Microsoft Azure API Results\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/client-libraries-rest-api?tabs=version-3-1&pivots=programming-language-python#sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "filename = \"C:\\\\Users\\\\spa1e17\\\\OneDrive - University of Southampton\\\\CNDWip\\\\APIKeys\\\\AzureKeys.json\"\n",
    "\n",
    "with open(filename, 'r') as fp:\n",
    "    keys = json.load(fp)\n",
    "    apikey = keys['KEY 1']\n",
    "    endpoint = keys['Endpoint']\n",
    "\n",
    "credential = AzureKeyCredential(apikey)\n",
    "endpoint=endpoint\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(endpoint, credential)\n",
    "\n",
    "for orator in docs.values():\n",
    "    \n",
    "    for sent_obj in tqdm(orator['sentences'], total = len(orator['sentences']), desc = orator['name']):\n",
    "  \n",
    "        text = [sent_obj['text']]\n",
    "        \n",
    "        response = text_analytics_client.analyze_sentiment(text, language=\"en\")\n",
    "        label = response[0].sentiment\n",
    "        score = response[0].confidence_scores[label]\n",
    "\n",
    "        if label == \"negative\":\n",
    "            score = score*-1\n",
    "        \n",
    "        sent_obj['azure sentiment'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get TextBlob API Results\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for orator in docs.values():\n",
    "    \n",
    "    for sent_obj in tqdm(orator['sentences'], total = len(orator['sentences']), desc = orator['name']):\n",
    "  \n",
    "        text = sent_obj['text']\n",
    "        \n",
    "        sent_obj['textblob sentiment'] = TextBlob(text).sentiment[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Results to Disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cndlib.cndutils import dump_jsonl\n",
    "import json\n",
    "print(type(docs))\n",
    "filename = os.path.join(os.getcwd(), \"gold_results.json\")\n",
    "with open(filename, 'w') as file:\n",
    "     file.write(json.dumps(docs))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc for doc in docs['laden']['sentences'])\n",
    "\n",
    "labels = ['text', 'function', 'hostile'] + [label for label in df.keys() if 'sentiment' in label and 'magnitute' not in label]\n",
    "display(df[labels][df.function.eq('elevation')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function(orator, entity):\n",
    "\n",
    "    \"\"\"\n",
    "    function to get the grouping of an entity from the orator's groupings\n",
    "    \"\"\"\n",
    "    if entity in docs[orator]['text']['groups']['ingroup']:\n",
    "        return \"ingroup\"\n",
    "    if entity in docs[orator]['text']['groups']['outgroup']:\n",
    "        return \"outgroup\"\n",
    "    return \"not found\"\n",
    "\n",
    "def assessment_test(col1, col2):\n",
    "\n",
    "    \"\"\"\n",
    "    function to test whether a sentiment scores matches ingroup/outgroup\n",
    "    \"\"\"\n",
    "\n",
    "    if col1 == \"positive\" or col1 == \"neutral\" and col2 == \"ingroup\":\n",
    "        return \"pass\"\n",
    "    if col1 == \"negative\" and col2 == \"ingroup\":\n",
    "        return \"fail\"\n",
    "    if col1 == \"negative\" and col2 == \"outgroup\":\n",
    "        return \"pass\"\n",
    "    if col1 == \"positive\" or col1 == \"neutral\" and col2 == \"outgroup\":\n",
    "        return \"fail\"\n",
    "    \n",
    "# create new dataframe based on filtered columns\n",
    "scores = lambda table, column, labels: table[table.column.isin(labels)], ignore_index = True)\n",
    "\n",
    "## iterate through the docs\n",
    "for orator in docs:\n",
    "    \n",
    "    # capture results\n",
    "    results = pd.DataFrame(docs[orator]['sentences'])\n",
    "    \n",
    "    ## create a dataframe for positive and negative results\n",
    "    dfs = dict()\n",
    "    dfs = {\"elevation\" : {\"result\" : None, \"df\" : scores(results, 'function', ['elevation'])}, \n",
    "           \"othering\" : {\"result\" : None, \"df\" : scores(results, 'function', ['othering'])}}\n",
    "\n",
    "    for obj in dfs.values():\n",
    "        \n",
    "        df = obj[\"df\"]\n",
    "            \n",
    "        # get the grouping for each entity\n",
    "        df[\"grouping\"] = df.apply(lambda x: get_group(orator, x[\"text\"]), axis = 1)\n",
    "        \n",
    "        # test whether sentiment score matches ingroup/outgroup        \n",
    "        df[\"test result\"] = df.apply(lambda x: assessment_test(x[\"label\"], x[\"grouping\"]), axis=1)\n",
    "        \n",
    "        # get the success scores for ingroup and outgroup\n",
    "        obj[\"result\"] = format(df[\"test result\"].value_counts(normalize = True)[\"pass\"], '.0%')\n",
    "        \n",
    "        # format dataframe\n",
    "        df.drop('mixed', axis = 1, inplace = True)\n",
    "        df['text'] = df['text'].str.title()\n",
    "        df.rename(columns = {\"score\" : \"sentiment score\", \"text\" : \"entity text\"}, inplace = True)\n",
    "        df.columns = df.columns.str.title()\n",
    "\n",
    "    docs[orator]['text']['analytics']['sentiment']['dfs'] = dfs\n",
    "    \n",
    "#     # display the outputs\n",
    "#     display_side_by_side([output[\"df\"] for output in dfs.values()],\n",
    "#                          [f\"{key.title()} scores for {docs[orator]['name']} has a True Positive Score of {obj['result']} from a total of {len(obj['df'])} Entities\"\n",
    "#                          for key, obj in dfs.items()])\n",
    "#     print()\n",
    "\n",
    "dfs = []\n",
    "captions = []\n",
    "for orator in docs.values():\n",
    "    for group, df in orator['text']['analytics']['sentiment']['dfs'].items():\n",
    "        dfs.append(df['df'])\n",
    "        captions.append(f\"{group.title()} scores for {orator['name']} has a Success of {df['result']} from a total of {len(df['df'])} Entities\")\n",
    "        \n",
    "display_side_by_side(dfs, captions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy_v3",
   "language": "python",
   "name": "spacy_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
