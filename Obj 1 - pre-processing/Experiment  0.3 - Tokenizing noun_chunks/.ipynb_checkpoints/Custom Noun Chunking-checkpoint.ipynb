{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Noun Chunking\n",
    "-----\n",
    "\n",
    "There is a problem whereby spaCy's inbuilt noun_chunks is too course grained for the chunking required for detecting the ingroups and outgroups.\n",
    "\n",
    "For the purposes of the methodology, a more fine grained noun chunking algorithm is required.\n",
    "\n",
    "There are several examples in the test ingroup and outgroup sentences named entities are chunked with other nouns when they would preferable be kept separate.\n",
    "\n",
    "There are also several examples where a noun chunk contains more than one noun of a custom attribute, therefore, the chunk needs to be resolved to a single instance\n",
    "\n",
    "This notebook adapts spaCy's noun_chunk source code and adapt for the specific purpose of this pipeline.\n",
    "\n",
    "Source code at these links:\n",
    "\n",
    "    Noun Chunker Code\n",
    "    \n",
    "    https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py\n",
    "\n",
    "    Class extensions\n",
    "\n",
    "    https://github.com/explosion/spaCy/blob/9ce059dd067ecc3f097d04023e3cfa0d70d35bb8/spacy/tokens/doc.pyx\n",
    "\n",
    "    https://github.com/explosion/spaCy/blob/f49e2810e6ea5c8b848df5b0f393c27ee31bb7f4/spacy/tokens/span.pyx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "the results of developing the custom chunker are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-built success rate: 50.0%\n",
      "custom success rate: 94.0%\n",
      "a 44.0% improvement in using the new chunker\n"
     ]
    }
   ],
   "source": [
    "success_rate = lambda part, whole:round(100 * (float(part) / float(whole)), 0)\n",
    "original = success_rate(orig_success, total_chunks)\n",
    "custom = success_rate(custom_success, total_chunks)\n",
    "print(f'in-built success rate: {original}%')\n",
    "print(f'custom success rate: {custom}%')\n",
    "print(f'a {custom - original}% improvement in using the new chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The existing spacy code for noun chunks is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crux of this code is in this section, the purpose of this note book is to determine how this code blob for spaCy's noun chunker should be modified to create more fine-grained noun chunks with the correct named concepts:\n",
    "\n",
    "`\n",
    "\n",
    "    if word.pos not in (NOUN, PROPN, PRON):\n",
    "                continue\n",
    "            # Prevent nested chunks from being produced\n",
    "            if word.left_edge.i <= prev_end:\n",
    "                continue\n",
    "            if word.dep in np_deps:\n",
    "                prev_end = word.i\n",
    "                yield word.left_edge.i, word.i + 1, np_label\n",
    "            elif word.dep == conj:\n",
    "                head = word.head\n",
    "                while head.dep == conj and head.head.i < head.i:\n",
    "                    head = head.head\n",
    "                # If the head is an NP, and we're coordinated to it, we're an NP\n",
    "                if head.dep in np_deps:\n",
    "                    prev_end = word.i\n",
    "                    yield word.left_edge.i, word.i + 1, np_label\n",
    "                    \n",
    "`\n",
    "\n",
    "`np_deps` is a list of dependency labels denoting a noun token\n",
    "\n",
    "`prev_end` is a index to ensure subsequent chunks do not overlap with existing chunks\n",
    "\n",
    "`word.left_edge.i` creates a chunk from the root token and all other tokens in its leftwards facing dependency tree.\n",
    "\n",
    "Where `word.left_edge.i` is too course grained, the custom chunk will expand the number of tests to become a more fine grained chunker. For example: \n",
    "\n",
    "There are rightward facing noun chunks also of interest, for example: \n",
    "- \"weapons of mass destruction\": with weapon as the root, the chunk is rightward facing.\n",
    "\n",
    "There are noun chunks containin multiple tokens of interest that need to be resolved to a single annotation, for example:\n",
    "- \"the occupying American enemy\": needs to be resolved to a merged noun chunk annotated as an outgroup\n",
    "- \"the alliance of Jews, Christians, and their agents\": with alliance as the root, this is a rightwards facing group noun chunk\n",
    "\n",
    "Additional functionality for custom attributes will have to be added and there is the need to remove predicate terms for the hearst pattern detection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data\n",
    "\n",
    "The ingroup and outgroup and outgroup files for each orator comprise sentences of interest each of which contain feature phrases relevant to the methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dict object of all the ingroup/outgroup sentences\n",
    "import os\n",
    "import cndutils as ut\n",
    "path = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\"\n",
    "\n",
    "sent_dict = dict()\n",
    "jsonl_files = [f for f in os.listdir(path) if os.path.splitext(f)[1] == \".jsonl\" and \"group\" in f]\n",
    "for file in jsonl_files:\n",
    "    data_list = ut.load_jsonl(os.path.join(path, file))\n",
    "    for entry in data_list:\n",
    "        for value in entry.values():\n",
    "            sent_dict[len(sent_dict)] = value\n",
    "            \n",
    "print(jsonl_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through Test Data for Sentences of Interest\n",
    "\n",
    "These sentences will be used to tune the existing spaCy noun chunker for the purposes of this methodology.\n",
    "\n",
    "This code block selects some of these sentences to create test data for the new chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import cndutils\n",
    "importlib.reload(cndutils)\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\"\n",
    "ss = cndutils.sent_select(path = path, file = \"test_sents\")\n",
    "output = ss(cnd.nlp, sent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core_web_md\n",
      "['tagger', 'parser', 'ner', 'Named Entity Matcher', 'merge_entities', 'Concept Matcher']\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "import pipeline\n",
    "importlib.reload(pipeline)\n",
    "cnd = pipeline.CND()\n",
    "\n",
    "print(cnd.nlp.meta['name'])\n",
    "print([pipe for pipe in cnd.nlp.pipe_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the new code for the custom noun chunker\n",
    "\n",
    "The custom noun chunker in this code block is developed while iterating through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy import displacy\n",
    "from spacy import explain\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import visuals\n",
    "importlib.reload(visuals)\n",
    "from pipeline import ConceptMatcher\n",
    "\n",
    "cust_stopwords = [\n",
    "        'able', 'available', 'brief', 'certain',\n",
    "        'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "        'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "        'its', 'last', 'latter', 'least', 'less', 'likely', \n",
    "        'little', 'mainly', 'many', 'ml', 'more', 'most', 'mostly', 'much', \n",
    "        'my', 'necessary', 'new', 'next', 'non', 'notably', 'old', 'other', \n",
    "        'our', 'ours', 'own', 'particular', 'particularly', 'principally',\n",
    "        'past', 'possible', 'present', 'proud', 'recent', 'same', 'several', \n",
    "        'significant', 'similar', 'some', 'such', 'sup', 'sure', 'these', 'those'\n",
    "    ]\n",
    "\n",
    "def custom_chunk_iterator(doclike):\n",
    "    \"\"\"\n",
    "    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n",
    "    \n",
    "    This is a modification of the spaCy's noun chunker.\n",
    "    \n",
    "    Instead of using the <.left_edge.i> property to capture the span, this chunker uses <<subtree>>.\n",
    "    \n",
    "    Signifying custom chunks, the Span objects are labeled with \"CC\"\n",
    "    \n",
    "    source code: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/syntax_iterators.py\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = [\n",
    "        \"nsubj\",\n",
    "        \"dobj\",\n",
    "        \"nsubjpass\",\n",
    "        \"pcomp\",\n",
    "        \"pobj\",\n",
    "        \"dative\",\n",
    "        \"appos\",\n",
    "        \"attr\",\n",
    "        \"ROOT\",\n",
    "        \"conj\"\n",
    "    ]\n",
    "    \n",
    "    doc = doclike.doc  # Ensure works on both Doc and Span.\n",
    "\n",
    "    if not doc.is_parsed:\n",
    "        raise ValueError(Errors.E029)\n",
    "\n",
    "    np_deps = [doc.vocab.strings.add(label) for label in labels]\n",
    "    conj = doc.vocab.strings.add(\"conj\")\n",
    "    pobj = doc.vocab.strings.add(\"pobj\")\n",
    "    relcl = doc.vocab.strings.add(\"relcl\")\n",
    "    acl = doc.vocab.strings.add(\"acl\")\n",
    "    prep = doc.vocab.strings.add(\"prep\")\n",
    "    ADP = doc.vocab.strings.add(\"ADP\")\n",
    "    advmod = doc.vocab.strings.add(\"advmod\")\n",
    "    cc_label = doc.vocab.strings.add(\"CC\")\n",
    "    \n",
    "    def ADP_head(word):\n",
    "        \n",
    "        \"\"\"\n",
    "        function to check whether a word is the head of an adpositional phrase\n",
    "        if there is a nested adpositional phrase, returns false\n",
    "        \"\"\"\n",
    "        \n",
    "        if word.n_rights > 0:\n",
    "            adp_i = list(word.rights)[0].i\n",
    "            if doc[adp_i].pos == ADP and doc[adp_i].text not in [\"to\", \"in\"] and doc[adp_i].n_rights > 0:\n",
    "                pobj_i = list(doc[adp_i].children)[0].i\n",
    "                if doc[pobj_i].dep == pobj and doc[pobj_i].n_rights == 0:\n",
    "                    return True\n",
    "                if doc[pobj_i].dep == pobj and doc[pobj_i].n_rights > 0:\n",
    "                    if list(doc[pobj_i].rights)[0].pos != ADP:\n",
    "                        return True\n",
    "                \n",
    "#                 :\n",
    "#                     return True\n",
    "#                 if doc[pobj_i].conjuncts or doc[pobj_i].pos_ not in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "#                     return True\n",
    "            \n",
    "        return False\n",
    "        \n",
    "    def get_right_edge(word):\n",
    "        \n",
    "        \"\"\"\n",
    "        function to get the immediate right edge of a adpositional phrase\n",
    "        \"\"\"\n",
    "        \n",
    "        adp_i = None\n",
    "        adp_i = list(word.rights)[0].i\n",
    "        if doc[adp_i].n_rights > 0:\n",
    "            pobj_i = list(doc[adp_i].rights)[-1].i\n",
    "            if doc[pobj_i].pos_ not in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "                return doc[pobj_i].right_edge.i\n",
    "            \n",
    "            return list(doc[adp_i].children)[-1].i\n",
    "        \n",
    "    def get_gold(gold_start):\n",
    "\n",
    "        \"\"\"\n",
    "        function to get the start and end indicies of a noun_chunk\n",
    "        \"\"\"\n",
    "        gold_end = None\n",
    "        \n",
    "        if doc[gold_start].pos_ == \"DET\":\n",
    "            gold_start += 1\n",
    "\n",
    "        if word.conjuncts and word.dep_ != \"conj\":\n",
    "            gold_end = word.i + 1 # word is a conjunction head therefore return head index\n",
    "        \n",
    "        elif word.dep_ == \"conj\" and list(word.rights) and \"conj\" in [t.dep for t in word.rights]:\n",
    "            gold_end = word.i + 1 # word is a sub-conjunction head therefore return sub-conjunction head index\n",
    "        \n",
    "        elif word.dep_ in [\"nsubj\", \"dobj\", \"nsubjpass\", \"pcomp\", \"pobj\", \"dative\", \"appos\", \"attr\", \"ROOT\", \"conj\"]:\n",
    "            gold_end = word.right_edge.i + 1 # capture the full noun chunk\n",
    "            for index in range(gold_start, gold_end):\n",
    "                if doc[index].text.lower() in [\",\", \"--\", \":\", \"with\", \"which\"] or doc[index].pos_ in [\"SCONJ\", \"CCONJ\"] or doc[index].dep_ in [\"cc\"]: # split noun chunks comprising lists\n",
    "                    gold_end = index\n",
    "                    break\n",
    "        \n",
    "        return gold_start, gold_end\n",
    "    \n",
    "    prev_end = -1\n",
    "    \n",
    "    for word in doclike:\n",
    "        \n",
    "        if word.pos_ not in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "            continue\n",
    "       \n",
    "        if word.left_edge.i <= prev_end:\n",
    "            continue\n",
    "            \n",
    "        # if the token is an apositional head\n",
    "        elif ADP_head(word):\n",
    "            \n",
    "            right_edge = word.right_edge.i\n",
    "            \n",
    "            if word.n_rights > 0:\n",
    "                right_edge = get_right_edge(word)\n",
    "                    \n",
    "            elif word.n_rights > 0 and word.conjuncts:\n",
    "                right_edge = get_right_edge(word)\n",
    "                \n",
    "            prev_end = right_edge \n",
    "            yield word.left_edge.i, right_edge + 1, cc_label\n",
    "            \n",
    "        # for when the word is not an apositional head    \n",
    "        elif word.dep in np_deps:\n",
    "            prev_end = word.i                    \n",
    "            yield word.left_edge.i, word.i + 1, cc_label\n",
    "                \n",
    "Doc.set_extension(\"custom_chunk_iterator\", getter = custom_chunk_iterator, force = True)\n",
    "                \n",
    "get = ConceptMatcher()\n",
    "    \n",
    "def is_modifier(token):\n",
    "\n",
    "    \"\"\"\n",
    "    function to determine whether a token modifies a span\n",
    "    \"\"\"\n",
    "\n",
    "    tag_modifiers = [\"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "    dep_modifiers = [\"amod\", \"poss\", \"pobj\", \"npadvmod\", \"appos\", \"compound\"]\n",
    "\n",
    "    if token.tag_ in tag_modifiers and token.dep_ in dep_modifiers:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_span_modifier(self, span):\n",
    "\n",
    "    \"\"\"\n",
    "    Getter function to for any modifying tokens of the root.\n",
    "    \"\"\"       \n",
    "\n",
    "    word = span.root\n",
    "\n",
    "    # when the token is a conjunct head, need to isolate only terms to its left\n",
    "    # if the word has conjuncts but does not have a `conj` dependency it is the head of the main conjunction.\n",
    "    if word.conjuncts and word.dep_ != \"conj\":\n",
    "        for token in word.lefts:\n",
    "#             print(f\"from ({word}) testing ({token}) from root.lefts ({list(word.lefts)})\")\n",
    "            if self.is_modifier(token) and token.i != word.i:\n",
    "                return token\n",
    "    else:\n",
    "        # when the token is not a conjunct head, can iterate over terms to the left and right\n",
    "        for token in span:\n",
    "#             print(f\"from ({word}) testing ({token}) from root.subtree ({list(word.subtree)})\")\n",
    "            if self.is_modifier(token) and token.i != word.i:\n",
    "                return token\n",
    "\n",
    "def get_span_type(self, span):\n",
    "        \n",
    "        \"\"\"\n",
    "        getter function to define the span entity type for any named entities modifying the root token\n",
    "        \n",
    "        iterates through left facing tokens to the root to identify any modifier terms\n",
    "        returns: ent_type_ of any modifier named entities\n",
    "        else returns the span root ent_type_\n",
    "        \"\"\"\n",
    "        \n",
    "        #iterate through the span and return any named concepts other than those related to the root.\n",
    "\n",
    "        for token in span:\n",
    "            if self.is_modifier(token) and token.ent_type_:\n",
    "                return token.ent_type_\n",
    "            \n",
    "        return span.root.ent_type_\n",
    "\n",
    "def custom_chunks(doc):\n",
    "    \n",
    "    \"\"\"\n",
    "    Yields base customised noun-phrase `Span` objects from the custom chunk \n",
    "    iterator, if the document has been syntactically parsed. \n",
    "    Different to spaCy's inbuilt noun_chunks which uses the <.left_edge.i> property to capture the span, \n",
    "    this chunker uses the <.subtree> property.\n",
    "    \n",
    "    YIELDS (Span): Base customised chunk `Span` objects\n",
    "    \"\"\"\n",
    "    \n",
    "    if not doc.is_parsed:\n",
    "            raise ValueError(Errors.E029)\n",
    "        # Accumulate the result before beginning to iterate over it. This\n",
    "        # prevents the tokenisation from being changed out from under us\n",
    "        # during the iteration. The tricky thing here is that Span accepts\n",
    "        # its tokenisation changing, so it's okay once we have the Span\n",
    "        # objects. See Issue #375\n",
    "    spans = []\n",
    "    \n",
    "    get = pipeline.ConceptMatcher()\n",
    "    Span.set_extension(\"CONCEPT\", default = \"\", force = True)\n",
    "    Span.set_extension(\"ATTRIBUTE\", getter = get.get_attribute, force = True)\n",
    "    Span.set_extension(\"IDEOLOGY\", getter = get.get_ideology, force = True)\n",
    "\n",
    "    Span.set_extension(\"span_type\", getter = get.get_span_type, force = True)\n",
    "    Span.set_extension(\"span_CONCEPT\", getter = get.get_span_concept, force = True)\n",
    "\n",
    "    Span.set_extension(\"modifier\", getter = get.get_span_modifier, force = True)\n",
    "    Span.set_extension(\"set_attrs\", method = get.set_attrs, force = True)\n",
    "    \n",
    "    \n",
    "    if doc._.custom_chunk_iterator is not None:\n",
    "        for start, end, label in doc._.custom_chunk_iterator:\n",
    "            \n",
    "            # remove stopword tokens from left of the span\n",
    "            for index in range(start, end):\n",
    "                if doc[index].pos_ in [\"PROPN\", \"NOUN\", \"PRON\", \"ADJ\"]:\n",
    "                    break\n",
    "                if doc[index].lower_ in cust_stopwords or doc[index].is_stop:\n",
    "                    start += 1\n",
    "                    \n",
    "            span = Span(doc, start, end, label=label)\n",
    "            if span.root._.CONCEPT:\n",
    "                span._.CONCEPT = span.root._.CONCEPT\n",
    "            else:\n",
    "                span._.CONCEPT = span._.modifier._.CONCEPT\n",
    "            \n",
    "            spans.append(span)\n",
    "                  \n",
    "    for span in spans:\n",
    "        yield span\n",
    "\n",
    "Doc.set_extension(\"custom_chunks\", getter = custom_chunks, force = True)\n",
    "\n",
    "\n",
    "######################\n",
    "# testing of the custom functions\n",
    "#####################\n",
    "\n",
    "text = \"we are the USA and our enemy is the Taliban Regime who are a terrorist organisation\"\n",
    "# separate noun chunks\n",
    "text = \"In this trial, we have been reminded and the world has seen that our fellow Americans are generous and kind, resourceful and brave.\" #\n",
    "# # # # right facing chunks - how to attach freedom to defender\n",
    "# text = \"They have attacked America because we are freedom's home and defender, and the commitment of our Fathers is now the calling of our time.\"\n",
    "# # # # how to parse conjunctions)\n",
    "# text = \"Both Americans and Muslim friends and citizens, tax-paying citizens, and Muslims in nations were just appalled and could not believe what -- what we saw on our TV screens.\"\n",
    "# # # # removal of stopwords\n",
    "# text = \"The enemy of America is not our many Muslim friends; it is not our many Arab friends.\" \n",
    "# # # # what to do about chunks joined by punctuation\n",
    "# text = \"We are joined in this operation by our staunch friend, Great Britain.\" \n",
    "# # # # long right facing chunk - missing PROPN between <billion> and <worldwide>. Extend the noun chunk to be \"a billion worldwide who practice the Islamic faith.\"\n",
    "# text = \"The United States of America is a friend to the Afghan people, and we are the friends of almost a billion worldwide who practice the Islamic faith.\"\n",
    "# # # # resolve a nested prep>pobj patterns - problem sentence - resolving these nested prep>pobj patterns contradicts others\n",
    "# text = \"I would like to report to the American people on the state of our war against terror, and then I'll be happy to take questions from the White House press corps.\"\n",
    "# # # # clipping a noun chunk with who\n",
    "# text = \"At the same time, we are showing the compassion of America by delivering food and medicine to the Afghan people who are, themselves, the victims of a repressive regime.\"\n",
    "# # # # how to parse out <diligent and determined work> to link it with the other propn in the sentence. FBI is not detected. FBI is not part of the correct list.\n",
    "# text = \"We may never know what horrors our country was spared by the diligent and determined work of our police forces, the FBI, ATF agents, federal marshals, Custom officers, Secret Service, intelligence professionals and local law enforcement officials, under the most trying conditions.\"\n",
    "# # # # currently ADP phrase currently ends at \"orgnizations\", if al Qaeda traceable to outgroup can revise to word.right_edge.i\n",
    "# text = \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\"\n",
    "# # # # \"nations\" is not connected to \"every continent on the earth\"\n",
    "# text = \"Our staunch friends, Great Britain, our neighbors Canada and Mexico, our NATO allies, our allies in Asia, Russia and nations from every continent on the Earth have offered help of one kind or another -- from military assistance to intelligence information, to crack down on terrorists' financial networks.\"\n",
    "# # # # split a list of chunks\n",
    "# text = \"A terrorist underworld -- including groups like Hamas, Hezbollah, Islamic Jihad, Jaish-i-Mohammed -- operates in remote jungles and deserts, and hides in the centers of large cities.\"\n",
    "# # # # \"weapon\" is not dependency linked to \"mass destruction\"\n",
    "# text = \"North Korea is a regime arming with missiles and weapons of mass destruction, while starving its citizens.\"\n",
    "# # # # split a conjunct from an adpositional phrase. \"people\" is not dependency linked to \"big and small\". \"People of all walks of life\" conflicts with \"state of our war on terror\"\n",
    "# text = \"We want to study the ways which could be used to rectify matters and restore rights to their owners as people have been subjected to grave danger and harm to their religion and their lives, people of all walks of life, civilians, military, security men, employees, merchants, people big and small, school and university students, and unemployed university graduates, in fact hundreds of thousands who constitute a broad sector of the society.\"\n",
    "# # # # capturing adpositional conjuncts\n",
    "text = \"O protectors of monotheism and guardians of the faith; O successors of those who spread the light of guidance in the world; O grandsons of Sa'd Bin-Abi-Waqqas, al-Muthanna Bin-Harithah al-Shibani, al-Qa'qa' Bin-'Amr al-Tamimi, and the companions who fought alongside them: You rushed to join the Army and the Guard merely to join the jihad for the cause of God in order to spread the word of God and to defend Islam and the land of the two holy mosques against invaders and occupiers, which is the highest degree of belief in religion.\"\n",
    "# # # # how to link this phrase to be \"sons of Islam and daughters of Islam\"\n",
    "# text = \"Sons and daughters of Islam!\"\n",
    "# # # # not detecting \"group of vanguard Muslims\"\n",
    "# text = \"God has blessed a group of vanguard Muslims, the forefront of Islam, to destroy America.\"\n",
    "# # # connect \"alliance of\" to \"Jews\" and \"Chistians\"\n",
    "# text = \"You are not unaware of the injustice, repression, and aggression that have befallen Muslims through the alliance of Jews, Christians, and their agents, so much so that Muslims' blood has become the cheapest blood and their money and wealth are plundered by the enemies.\"\n",
    "# # # mark \"Jewish-crusade alliance\" as an outgroup \n",
    "# text = \"And so, this Jewish-crusade alliance killed and detained the symbols of the truthful ulema and upholders of the call—and God is above everyone.\"\n",
    "# # # link \"regime\" to \"injustice\" and \"illegitimate actions\"\n",
    "# text = \"They feel that God is tormenting them because they kept quiet about the regime's injustice and illegitimate actions, especially its failure to have recourse to the Shari'ah, its confiscation of people's legitimate rights, the opening of the land of the two holy mosques to the American occupiers, and the arbitrary jailing of the true ulema, heirs of the Prophets.\"\n",
    "# # # link \"nation's enemies\" to \"the American crusader forces\". long noun phrases to link to \"aspects of our plight\"\n",
    "# text = \"Its failure to protect the country, opening it to the nation's enemies, the American crusader forces who have become the main cause of all aspects of our plight, especially the economic aspect as a result of the unjustified heavy expenditure on them and as a result of the policies they impose on the country, and particularly the oil policy determining the quantities of oil to be produced and setting the prices which suit their own economic interests ignoring the country's economic interests, and also as a result of the exorbitant arms deals imposed on the regime, to the point that people are wondering what good, then, is the regime?\"\n",
    "# # missing noun for \"those who fomented internal sedition in their country\". remove \"those\" from stopword list. add \"such\" to stop words list\n",
    "# text = \"That was the only door left open to the public for ending injustice and upholding right and justice, and in whose interests do Prince Sultan and Prince Nayif plunge the country and the people into an internal war that would destroy everything, enlisting the aid and advice of those who fomented internal sedition in their country and using the people's police force to put down the reform movement there and pit members of the public one against the other—leaving the main enemy in the region, namely the Jewish-American alliance, safe and secure, having found such traitors to implement its policies aimed at exhausting the nation's human and financial resources internally.\"\n",
    "# # split civilians and military. dependency identifies sentence as a conjunction, but it is not.\n",
    "# text = \"But, thank God, the vast majority of the people, civilians and military, are aware of that sinister plan and will not allow themselves to be an instrument for strikes against one another in implementation of the policy of the main enemy, namely the Israeli-American alliance, through the Saudi regime, its agent in the country.\"\n",
    "# # split a chunk span by with\n",
    "# text = \"Partition of the country of the two holy mosques with Israel taking the northern part of the land of the two holy mosques is considered to be an urgent demand of the Jewish-crusade alliance, because the existence of a state of such size and with such resources under sound Islamic rule, which, God willing, is coming, would be a threat to the Jewish entity in Palestine, for the land of the two holy mosques would be a symbol for the unity of the Islamic world because of the presence of the holy Ka'bah, the qiblah of all Muslims.\"\n",
    "# # split a long noun chunk of sub clauses. not picking up Islamic world's ulema\n",
    "# text = \"He lied to the ulema who sanctioned the Americans' entry and he lied to the Islamic world's ulema and leaders at the [World Muslim] League's conference in holy Mecca in the wake of the Islamic world's condemnation of the crusader forces' entry into the country of the two holy mosques on the pretext of defending it.\"\n",
    "# # not picking up US Defence Secretary\n",
    "# text = \"A few days ago news agencies carried a statement by the occupier-crusader, the US defense secretary, in which he said that he has learned one lesson from the Riyadh and al-Khubar blasts, namely not to retreat in front of the terrorist cowards.\"\n",
    "# # link \"intentional killing of innocent\" to \"women\" and \"children\" \n",
    "# text = \"And that day, it was confirmed to me that oppression and the intentional killing of innocent women and children is a deliberate American policy.\"\n",
    "\n",
    "cnd.nlp.vocab[\"O\"].is_stop = True\n",
    "doc = cnd(text)\n",
    "display(visuals.chunk_custom_attrs(list(doc._.custom_chunks), json = True))\n",
    "\n",
    "# word = doc[35]\n",
    "# print(word)\n",
    "# if word.n_rights > 0:\n",
    "#     adp_i = list(word.rights)[0].i\n",
    "#     if doc[adp_i].dep_ == \"prep\" and doc[adp_i].n_rights > 0:\n",
    "#         pobj_i = list(doc[adp_i].children)[0].i\n",
    "#         print(\"nearly true\")\n",
    "#         if doc[pobj_i].dep_ == \"pobj\" and doc[pobj_i].n_rights == 0:\n",
    "#             print(\"True\")\n",
    "#         if doc[pobj_i].dep_ == \"pobj\" and doc[pobj_i].n_rights > 0:\n",
    "#             if list(doc[pobj_i].rights)[0].pos_ != \"ADP\":\n",
    "# #                 print(\"very true\")\n",
    "# else:\n",
    "#     print(\"try again\")\n",
    "    \n",
    "# options = {\"compact\": True}\n",
    "# displacy.render(doc, style = \"dep\", options=options)\n",
    "# print(doc.ents)\n",
    "# print(list(doc.noun_chunks))\n",
    "visuals.chunk_custom_attrs(list(doc._.custom_chunks), json = True)\n",
    "\n",
    "print(\"<<< token custom_chunks >>>\")\n",
    "# display(visuals.sent_custom_chunks(doc))\n",
    "print(doc)\n",
    "\n",
    "print(\"<<< custom chunk attributes >>>\")\n",
    "display(visuals.chunk_custom_attrs(list(doc._.custom_chunks)).T)\n",
    "\n",
    "print(\"<<< original chunk attributes >>>\")\n",
    "display(visuals.chunk_custom_attrs(list(doc.noun_chunks)).T)\n",
    "\n",
    "token_index = 16\n",
    "if isinstance(token_index, int) and token_index < len(doc):\n",
    "    print(\"<<< selected token dependency tree attributes >>>\")\n",
    "    display(visuals.token_deps(doc[token_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over the data\n",
    "\n",
    "Iterate over each sentence and review each noun chunk to determine the desired noun chunk, and develop notes to determine what modifications to the noun chunk doc extension is required.\n",
    "\n",
    "While iterating over the data a gold standard dataset is created to test the new chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "import pipeline\n",
    "importlib.reload(pipeline)\n",
    "import cndutils as ut\n",
    "import visuals\n",
    "importlib.reload(visuals)\n",
    "\n",
    "def get_gold(doc, span):\n",
    "    \n",
    "    gold_start = None\n",
    "    gold_end = None\n",
    "    \n",
    "    word = span.root\n",
    "\n",
    "    gold_start = word.left_edge.i\n",
    "    if doc[gold_start].pos_ == \"DET\":\n",
    "        gold_start += 1\n",
    "\n",
    "    if word.conjuncts and word.dep_ != \"conj\":\n",
    "        gold_end = word.i + 1 # word is a conjunction head\n",
    "    elif word.dep_ == \"conj\" and list(word.rights) and \"conj\" in [t.dep for t in word.rights]:\n",
    "        gold_end = word.i + 1 # word is a sub-conjunction head\n",
    "    elif word.dep_ in [\"nsubj\", \"dobj\", \"nsubjpass\", \"pcomp\", \"pobj\", \"dative\", \"appos\", \"attr\", \"ROOT\", \"conj\"]:\n",
    "        gold_end = word.right_edge.i + 1 # capture the full noun chunk\n",
    "        for index in range(gold_start, gold_end):\n",
    "            if doc[index].text in [\",\", \"--\"] or doc[index].pos_ in [\"SCONJ\", \"CCONJ\"] or doc[index].dep_ in [\"cc\"]: # split noun chunks comprising lists\n",
    "                gold_end = index\n",
    "                break\n",
    "        \n",
    "    return gold_start, gold_end\n",
    "\n",
    "#################################\n",
    "# Initialise\n",
    "#################################\n",
    "\n",
    "path = os.getcwd()\n",
    "test_jsonl = \"\"\n",
    "cust_jsonl = \"\"\n",
    "index_str = \"index.json\"\n",
    "test_filepath = os.path.join(path, test_jsonl)\n",
    "cust_filepath = os.path.join(path, cust_jsonl)\n",
    "index_filepath = os.path.join(path, index_str)\n",
    "\n",
    "with jsonlines.open(test_filepath) as f:\n",
    "    test_chunks = list(f.iter())\n",
    "    \n",
    "try:  \n",
    "    with jsonlines.open(cust_filepath) as f:\n",
    "        cust_chunk_list = list(f.iter())\n",
    "    if len(test_chunks) == 0:\n",
    "        cust_chunk_list = list()    \n",
    "\n",
    "except:\n",
    "    cust_chunk_list = list()\n",
    "\n",
    "try:\n",
    "    with open(index_filepath, \"r\") as index_json:\n",
    "        index = json.load(index_json)\n",
    "        \n",
    "except:\n",
    "    index = 0\n",
    "    \n",
    "lookup = pipeline.ConceptMatcher(cnd.nlp)\n",
    "    \n",
    "#################################\n",
    "# main body\n",
    "#################################\n",
    "    \n",
    "while index < len(test_chunks):\n",
    "    \n",
    "    line = test_chunks[index]\n",
    "            \n",
    "    with open(index_filepath, \"wb\") as f:\n",
    "        f.write(json.dumps(index).encode(\"utf-8\"))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "  \n",
    "    #parse document\n",
    "    doc = cnd(line[str(index)])\n",
    "    \n",
    "    # add the original and indexed noun_chunks to the line\n",
    "    line[\"orig_chunks\"] = visuals.chunk_custom_attrs(list(doc.noun_chunks), json = True)\n",
    "    line[\"gold_chunks\"] = visuals.chunk_custom_attrs(list(doc._.custom_chunks), json = True)\n",
    "    \n",
    "    for chunk in line[\"gold_chunks\"]:\n",
    "        if not chunk[\"CONCEPT\"] and chunk[\"span_type\"] == \"GPE\":\n",
    "            chunk[\"CONCEPT\"] = \"TERRITORY\"\n",
    "            chunk[\"ATTRIBUTE\"] = get.get_attribute(chunk[\"CONCEPT\"])\n",
    "            chunk[\"IDEOLOGY\"] = get.get_ideology(chunk[\"CONCEPT\"])\n",
    "        \n",
    "        if not chunk[\"CONCEPT\"] and chunk[\"span_type\"] == \"LOC\":\n",
    "            chunk[\"CONCEPT\"] = \"PLACE\"\n",
    "            chunk[\"ATTRIBUTE\"] = get.get_attribute(chunk[\"CONCEPT\"])\n",
    "            chunk[\"IDEOLOGY\"] = get.get_ideology(chunk[\"CONCEPT\"])\n",
    "        \n",
    "        if not chunk[\"CONCEPT\"] and chunk[\"text\"].lower() in [\"americans\"]:\n",
    "            chunk[\"CONCEPT\"] = \"SOCGROUP\"\n",
    "            chunk[\"ATTRIBUTE\"] = get.get_attribute(chunk[\"CONCEPT\"])\n",
    "            chunk[\"IDEOLOGY\"] = get.get_ideology(chunk[\"CONCEPT\"])\n",
    "            \n",
    "        if not chunk[\"CONCEPT\"] and chunk[\"text\"].lower() in [\"muslims\"]:\n",
    "            chunk[\"CONCEPT\"] = \"RELGROUP\"\n",
    "            chunk[\"ATTRIBUTE\"] = get.get_attribute(chunk[\"CONCEPT\"])\n",
    "            chunk[\"IDEOLOGY\"] = get.get_ideology(chunk[\"CONCEPT\"])\n",
    "            \n",
    "        if not chunk[\"CONCEPT\"] and chunk[\"span_type\"] in [\"DEITY\"]:\n",
    "            chunk[\"CONCEPT\"] = \"RELFIGURE\"\n",
    "            chunk[\"ATTRIBUTE\"] = get.get_attribute(chunk[\"CONCEPT\"])\n",
    "            chunk[\"IDEOLOGY\"] = get.get_ideology(chunk[\"CONCEPT\"])\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        # display dependency parse\n",
    "        options = {\"compact\": True}\n",
    "        displacy.render(doc, style = \"dep\", options=options)\n",
    "\n",
    "        # display sentence attributes\n",
    "        display(visuals.sent_frame(doc, extend = True))\n",
    "\n",
    "        # display both original and gold chunk attributes\n",
    "        print(\"<<< original chunks >>>\")\n",
    "        display(pd.DataFrame(line[\"orig_chunks\"]).T)\n",
    "        \n",
    "        # print sentence text\n",
    "        print(f\"{index} / {len(test_chunks)}\")\n",
    "        print(doc.text)\n",
    "        \n",
    "        print(\"<<< custom chunks >>>\")\n",
    "        display(pd.DataFrame(line[\"gold_chunks\"]).T)\n",
    "\n",
    "        check = \"\"\n",
    "        check = input(\"satisfied? (y/q)\").lower()\n",
    "        if check == \"y\":\n",
    "            \n",
    "            cust_chunk_list.append(line)\n",
    "\n",
    "            #write jsonl object to disk\n",
    "            with jsonlines.open(os.path.join(path, cust_filepath), 'w') as writer:\n",
    "                writer.write_all(cust_chunk_list)\n",
    "        \n",
    "            index += 1\n",
    "            break\n",
    "        \n",
    "        elif check == \"q\":\n",
    "            raise SystemExit(\"Stop right there!\")\n",
    "            \n",
    "        line[\"gold_chunks\"].clear()\n",
    "        \n",
    "        for chunk in doc.noun_chunks:\n",
    "            \n",
    "            gold_span = Span(doc, chunk.start, chunk.end, label = \"CC\")\n",
    "            if gold_span.root._.CONCEPT:\n",
    "                gold_span._.CONCEPT = gold_span.root._.CONCEPT\n",
    "            else:\n",
    "                gold_span._.CONCEPT = gold_span._.modifier._.CONCEPT\n",
    "            \n",
    "            while True:\n",
    "                display(visuals.chunk_custom_attrs([gold_span]))\n",
    "                gold_start, gold_end = get_gold(doc, gold_span)\n",
    "                print(\"gold_span:\", doc[gold_start : gold_end].text)\n",
    "                print()\n",
    "\n",
    "                notes = \"\"\n",
    "                check = input(\"satisfied? (y/q)\").lower()\n",
    "\n",
    "                if check == \"y\":\n",
    "                    line[\"gold_chunks\"].append(*visuals.chunk_custom_attrs([gold_span], json = True))\n",
    "                    line[\"gold_chunks\"][-1][\"notes\"] = notes\n",
    "                    break\n",
    "                elif check == \"q\":\n",
    "                    raise SystemExit(\"Stop right there!\")\n",
    "\n",
    "                # get new custom span\n",
    "                entry = \"s\"\n",
    "\n",
    "                while entry in [\"el\", \"er\", \"dl\", \"dr\", \"s\", \"q\"]:\n",
    "                    text = doc[gold_start : gold_end].text\n",
    "                    # el = expand left (subtract 1 from new_start)\n",
    "                    # er = expand right (add 1 to new_end)\n",
    "                    # dl = decrease left (add 1 to new_start)\n",
    "                    # dr = descrease right (subtract 1 from new_end)\n",
    "                    # sk = skip chunk\n",
    "                    entry = input(f'new chunk text <{text}> (el) (er) (dl), (dr), (q), (sk)')\n",
    "                    if len(entry) == 0:\n",
    "                        break\n",
    "                    elif entry == \"el\":\n",
    "                        gold_start -= 1\n",
    "                    elif entry == \"er\":\n",
    "                        gold_end += 1\n",
    "                    elif entry == \"dl\":\n",
    "                        gold_start += 1\n",
    "                    elif entry == \"dr\":\n",
    "                        gold_end -= 1\n",
    "                    elif entry == \"q\":\n",
    "                        raise SystemExit(\"Stop right there\")\n",
    "                    elif entry == \"sk\":\n",
    "                        break\n",
    "                \n",
    "                if entry == \"sk\":\n",
    "                    break\n",
    "                        \n",
    "                gold_span = Span(doc, gold_start, gold_end, label = \"CC\")\n",
    "\n",
    "                # get new span_type\n",
    "                cust_span_type = get.get_span_type(gold_span)\n",
    "                gold_span._.span_type = input(f'new span_type [{cust_span_type}]').lower()\n",
    "                if len(gold_span._.span_type) == 0:\n",
    "                    gold_span._.span_type = cust_span_type\n",
    "\n",
    "                #get modifier\n",
    "                cust_modifier = get.get_span_modifier(gold_span)\n",
    "                gold_span._.modifier = input(f'new modifier [{cust_modifier}]').lower()\n",
    "                if gold_span._.modifier:\n",
    "                    gold_span._.modifier = cust_modifier\n",
    "\n",
    "                # get new concept    \n",
    "                if gold_span._.CONCEPT:\n",
    "                    cust_concept = gold_span._.CONCEPT\n",
    "                else:                    \n",
    "                    cust_concept = gold_span._.modifier._.CONCEPT\n",
    "                    \n",
    "                if gold_span._.span_type == \"GPE\":\n",
    "                    cust_concept = \"TERRITORY\"\n",
    "                if gold_span._.span_type == \"NORP\":\n",
    "                    cust_concept = \"SOCIALGROUP\"\n",
    "                gold_span._.CONCEPT = input(f'concept [{gold_span.root}: {cust_concept}]:').upper()\n",
    "                if len(gold_span._.CONCEPT) == 0:\n",
    "                    gold_span._.CONCEPT = cust_concept\n",
    "\n",
    "                # get new notes\n",
    "                notes = input(\"notes:\")\n",
    "                if len(notes) > 0 and notes[-1] != \".\":\n",
    "                    notes += \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotation Notes\n",
    "\n",
    "Phrases do not conform to a Hearst Pattern\n",
    "- \"The United States respects the people of Afghanistan.\"\n",
    "- \"America has no truer friend than Great Britain.\"\n",
    "- \"The enemy of America is not our many Muslim friends; it is not our many Arab friends.\"\n",
    "\n",
    "Current chunk is \"friend\", would prefer \"friend to the Afghan People\"\n",
    "- \"The United States of America is a friend to the Afghan people, and we are the friends of almost a billion worldwide who practice the Islamic faith.\"\n",
    "\n",
    "Good sentence for a several Hearst Patterns\n",
    "- \"Our staunch friends, Great Britain, our neighbors Canada and Mexico, our NATO allies, our allies in Asia, Russia and nations from every continent on the Earth have offered help of one kind or another -- from military assistance to intelligence information, to crack down on terrorists' financial networks.\"\n",
    "- \"America and Afghanistan are now allies against terror.\"\n",
    "- \"A terrorist underworld -- including groups like Hamas, Hezbollah, Islamic Jihad, Jaish-i-Mohammed -- operates in remote jungles and deserts, and hides in the centers of large cities.\"\n",
    "\n",
    "Neither method are picking up \n",
    "- \"my fellow americans\" : \"And in this great conflict, my fellow Americans, we will see freedom's victory.\"\n",
    "- \"Usama bin Laden\" : \"This group and its leader -- a person named  -- are linked to many other organizations in different countries, including the Egyptian Islamic Jihad and the Islamic Movement of Uzbekistan.\"\n",
    "\n",
    "Linked through appos dependency\n",
    "- \"We are joined in this operation by our staunch friend, Great Britain.\"\n",
    "\n",
    "\"Close\" is annotated as an ADJ and not VERB.\n",
    "- \"More than two weeks ago, I gave Taliban leaders a series of clear and specific demands: Close terrorist training camps; hand over leaders of the Al Qaeda network; and return all foreign nationals, including American citizens, unjustly detained in your country.\"\n",
    "\n",
    "How to classify the term \"nuclear\" to capture its severity without over-egging\n",
    "\n",
    "\"weapons of mass desctruction\" is split across dependency tree\n",
    "- \"North Korea is a regime arming with missiles and weapons of mass destruction, while starving its citizens.\"\n",
    "\n",
    "\"Ulema\" is recorded as an \"ADJ\" when it should be a \"NOUN\"\n",
    "- \"In the light of the reality we are going through and the blessed, sweeping awakening in the world at large and in the Islamic world in particular, I meet with you today after a long absence dictated by the unjust crusade campaign led by the United States against the ulema and advocates of Islam to prevent them from instigating the Islamic nation against its enemies, as did their predecessors, may God have mercy on their souls, such as Ibn-Taymiyah and al-'Izz Ibn-'Abd-al-Salam.\"\n",
    "\n",
    "\"Military\" is recorded as an \"ADJ\" when it should be a \"NOUN\"\n",
    "- \"We want to study the ways which could be used to rectify matters and restore rights to their owners as people have been subjected to grave danger and harm to their religion and their lives, people of all walks of life, civilians, military, security men, employees, merchants, people big and small, school and university students, and unemployed university graduates, in fact hundreds of thousands who constitute a broad sector of the society.\"\n",
    "\n",
    "not picking up \"infighting\"\n",
    "- \"The Muslims are reminded that they should avoid infighting between sons of the Muslim nation because that will have dire consequences, the most important being:\"\n",
    "\n",
    "should be \"presence of the crusader\" and \"presence of the American military forces\"\n",
    "- \"Destruction of the oil industries, because the presence of the crusader and American military forces in the Islamic Gulf states, on land, in the air, and at sea, represents the greatest danger and harm and the greatest threat to the largest oil reserves in the world.\"\n",
    "\n",
    "should be \"killing of innocent women\" and \"killing of innocent children\"\n",
    "- \"And that day, it was confirmed to me that oppression and the intentional killing of innocent women and children is a deliberate American policy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the results\n",
    "\n",
    "Compute the success rate of the new noun chunker against the gold data and compare to in-built noun chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:03<00:00, 29.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in-built success rate: 50.0%\n",
      "custom success rate: 94.0%\n",
      "a 44.0% improvement\n",
      "\n",
      "The United States of America is a friend to the Afghan people, and we are the friends of almost a billion worldwide who practice the Islamic faith.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>friend to the Afghan people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghan people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We may never know what horrors our country was spared by the diligent and determined work of our police forces, the FBI, ATF agents, federal marshals, Custom officers, Secret Service, intelligence professionals and local law enforcement officials, under the most trying conditions.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diligent and determined work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>our police forces</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diligent and determined work of our police forces</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On my orders, the United States military has begun strikes against Al Qaeda terrorist training camps and military installations of the Taliban regime in Afghanistan.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>strikes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Qaeda terrorist training camps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>military installations of the Taliban regime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>strikes against Al Qaeda terrorist training camps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>military installations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taliban regime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "More than two weeks ago, I gave Taliban leaders a series of clear and specific demands: Close terrorist training camps; hand over leaders of the Al Qaeda network; and return all foreign nationals, including American citizens, unjustly detained in your country.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clear and specific demands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>terrorist training camps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all foreign nationals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>your country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>series of clear and specific demands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Close terrorist training camps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foreign nationals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "North Korea is a regime arming with missiles and weapons of mass destruction, while starving its citizens.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weapons of mass destruction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weapons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mass destruction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the light of the reality we are going through and the blessed, sweeping awakening in the world at large and in the Islamic world in particular, I meet with you today after a long absence dictated by the unjust crusade campaign led by the United States against the ulema and advocates of Islam to prevent them from instigating the Islamic nation against its enemies, as did their predecessors, may God have mercy on their souls, such as Ibn-Taymiyah and al-'Izz Ibn-'Abd-al-Salam.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advocates of Islam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advocates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Islam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "That is why one of the principles of the Sunnah and the [Prophet's] group is to do conquest using every good as well as sinful person, for God supports this in the interest of the cause of religion because, as the Prophet, may God's prayers and blessings be upon him, said, if conquest can only be achieved with the help of sinful princes or very sinful soldiers, then one of two things must be done: Either abandon the conquest with their cooperation, which means the greater harm of others taking over, or proceed with the conquest along with the sinful princes, and that way the greater harm is avoided and most of the rules of Islam are established, if not all.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interest of the cause of religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greater harm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cause of religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greater harm of others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O protectors of monotheism and guardians of the faith; O successors of those who spread the light of guidance in the world; O grandsons of Sa'd Bin-Abi-Waqqas, al-Muthanna Bin-Harithah al-Shibani, al-Qa'qa' Bin-'Amr al-Tamimi, and the companions who fought alongside them: You rushed to join the Army and the Guard merely to join the jihad for the cause of God in order to spread the word of God and to defend Islam and the land of the two holy mosques against invaders and occupiers, which is the highest degree of belief in religion.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>protectors of monotheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>successors of those who spread the light of guidance in the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sa'd Bin-Abi-Waqqas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al-Qa'qa' Bin-'Amr al-Tamimi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the companions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the cause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>land of the two holy mosques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>invaders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>highest degree of belief in religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O protectors of monotheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O successors of those who spread the light of guidance in the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O grandsons of Sa'd Bin-Abi-Waqqas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al-Qa'qa'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bin-'Amr al-Tamimi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>companions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>who</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cause of God</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>holy mosques against invaders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>highest degree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>belief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "That being said, those who say that al-Qaida has won against the administration in the White House or that the administration has lost in this war have not been precise, because when one scrutinises the results, one cannot say that al-Qaida is the sole factor in achieving those spectacular gains.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>administration in the White House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sole factor in achieving those spectacular gains</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the White House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>administration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sole factor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spectacular gains</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Today, from the same land, from Afghanistan, we seek to end the injustice which has befallen the nation at the hands of the Jewish-crusade alliance, especially after the occupation of the land of the ascension of the Prophet [now Israel], may God's prayers and blessings be upon him, and the violation of the land of the two holy mosques.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>land of the ascension of the Prophet [now Israel]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ascension of the Prophet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>now Israel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Its failure to protect the country, opening it to the nation's enemies, the American crusader forces who have become the main cause of all aspects of our plight, especially the economic aspect as a result of the unjustified heavy expenditure on them and as a result of the policies they impose on the country, and particularly the oil policy determining the quantities of oil to be produced and setting the prices which suit their own economic interests ignoring the country's economic interests, and also as a result of the exorbitant arms deals imposed on the regime, to the point that people are wondering what good, then, is the regime?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>failure to protect the country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>policies they impose on the country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exorbitant arms deals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>result of the policies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>result of the exorbitant arms deals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But, thank God, the vast majority of the people, civilians and military, are aware of that sinister plan and will not allow themselves to be an instrument for strikes against one another in implementation of the policy of the main enemy, namely the Israeli-American alliance, through the Saudi regime, its agent in the country.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There is no greater duty after faith than warding [daf'] off [that enemy], namely the Israeli-American alliance occupying the land of the two holy mosques and the land of the ascension of the Prophet, may God's prayers and blessings be upon him.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[daf']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land of the ascension of the Prophet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[daf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ascension of the Prophet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partition of the country of the two holy mosques with Israel taking the northern part of the land of the two holy mosques is considered to be an urgent demand of the Jewish-crusade alliance, because the existence of a state of such size and with such resources under sound Islamic rule, which, God willing, is coming, would be a threat to the Jewish entity in Palestine, for the land of the two holy mosques would be a symbol for the unity of the Islamic world because of the presence of the holy Ka'bah, the qiblah of all Muslims.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>country of the two holy mosques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jewish-crusade alliance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sound Islamic rule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jewish entity in Palestine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>presence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>holy Ka'bah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>holy mosques with Israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>such resources under sound Islamic rule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jewish entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Palestine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>presence of the holy Ka'bah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He lied to the ulema who sanctioned the Americans' entry and he lied to the Islamic world's ulema and leaders at the [World Muslim] League's conference in holy Mecca in the wake of the Islamic world's condemnation of the crusader forces' entry into the country of the two holy mosques on the pretext of defending it.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Islamic world's ulema</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Islamic world's condemnation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He told them that the matter was simple and that the US and coalition troops would leave in a few months.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the US and coalition troops</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coalition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But if the sword falls on the United States after 80 years, hypocrisy raises its head lamenting the deaths of these killers who tampered with the blood, honor, and holy places of the Muslims.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>holy places of the Muslims</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>holy places</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muslims</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The American people are the ones who employ both their men and their women in the American Forces which attack us.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ones who employ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Custom Chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import visuals\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "\n",
    "path = os.getcwd()\n",
    "cust_jsonl = \"cust_chunks.jsonl\"\n",
    "cust_filepath = os.path.join(path, cust_jsonl)\n",
    "\n",
    "with jsonlines.open(cust_filepath) as f:\n",
    "    gold_chunk_list = list(f.iter())\n",
    "    \n",
    "custom_success = 0\n",
    "orig_success = 0\n",
    "failure = 0\n",
    "total_chunks = 0\n",
    "results = []\n",
    "\n",
    "for i, chunk_list in tqdm(enumerate(gold_chunk_list), total = len(gold_chunk_list)):\n",
    "\n",
    "    gold_list = [gold_chunk[\"text\"] for gold_chunk in chunk_list[\"gold_chunks\"]]\n",
    "    total_chunks += len(gold_list)\n",
    "\n",
    "    doc = cnd(chunk_list[str(i)]) \n",
    "    cust_chunks = visuals.chunk_custom_attrs(list(doc._.custom_chunks), json = True)\n",
    "    orig_chunks = visuals.chunk_custom_attrs(list(doc.noun_chunks), json = True)\n",
    "\n",
    "    for orig_chunk in orig_chunks:\n",
    "        if orig_chunk[\"text\"] in gold_list:\n",
    "            orig_success += 1\n",
    "    \n",
    "    error_list = []\n",
    "\n",
    "    for cust_chunk in cust_chunks:\n",
    "        if cust_chunk[\"text\"] in gold_list:\n",
    "            custom_success +=1\n",
    "            gold_list.remove(cust_chunk[\"text\"])\n",
    "        else:\n",
    "            error_list.append(cust_chunk[\"text\"])\n",
    "\n",
    "    if gold_list or error_list:\n",
    "        results.append((chunk_list[str(i)], gold_list, error_list))\n",
    "        \n",
    "print()\n",
    "success_rate = lambda part, whole:round(100 * (float(part) / float(whole)), 0)\n",
    "original = success_rate(orig_success, total_chunks)\n",
    "custom = success_rate(custom_success, total_chunks)\n",
    "print(f'in-built success rate: {original}%')\n",
    "print(f'custom success rate: {custom}%')\n",
    "print(f'a {custom - original}% improvement')\n",
    "print()\n",
    "        \n",
    "for sentence, gold_list, error_list in results:\n",
    "    print(sentence)\n",
    "    display_side_by_side(pd.DataFrame(gold_list, columns = [\"Gold Chunks\"]), pd.DataFrame(error_list, columns = [\"Custom Chunks\"]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
