{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Corrections\n",
    "\n",
    "In this notebook we test the named entity recognition in the spaCy language model.\n",
    "\n",
    "Each sentence in each document is reviewed by displaying the named entities in each.\n",
    "\n",
    "Any errors are noted and a report is produced.\n",
    "\n",
    "The errors are corrected with an custom pipeline component added to the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of doc:  220536\n",
      "completed at: Apr 15 2020 20:10:52\n",
      "Wall time: 8.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "FileList = ['20010114-Remarks at the National Day of Prayer & Remembrance Service.txt',\n",
    "            '20010115-First Radio Address following 911.txt',\n",
    "            '20010117-Address at Islamic Center of Washington, D.C..txt',\n",
    "           '20010120-Address to Joint Session of Congress Following 911 Attacks.txt',\n",
    "           '20010911-Address to the Nation.txt',\n",
    "           '20011007-Operation Enduring Freedom in Afghanistan Address to the Nation.txt',\n",
    "           '20011011-911 Pentagon Remembrance Address.txt',\n",
    "           '20011011-Prime Time News Conference on War on Terror.txt',\n",
    "           '20011026-Address on Signing the USA Patriot Act of 2001.txt',\n",
    "           '20011110-First Address to the United Nations General Assembly.txt',\n",
    "           '20011211-Address to Citadel Cadets.txt',\n",
    "           '20011211-The World Will Always Remember 911.txt',\n",
    "           '20020129-First (Official) Presidential State of the Union Address.txt',\n",
    "           ]\n",
    "raw = ''\n",
    "\n",
    "filepath = 'C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Speeches/'\n",
    "\n",
    "binladenpath = os.path.join(filepath, 'Osama bin Laden/')\n",
    "bushpath = os.path.join(filepath, 'George Bush/')\n",
    "\n",
    "for f in FileList:\n",
    "    with open(bushpath + f, 'r') as text:\n",
    "        raw = raw + text.read()\n",
    "\n",
    "FileList = ['19960823-OBL Declaration.txt',\n",
    "            '20011007-OBL Full Warning.txt',\n",
    "            '20011109-OBL.txt',\n",
    "            '20021124-OBL Letter to America.txt',\n",
    "            '20041101-Al Jazeera Speech.txt'\n",
    "           ]\n",
    "\n",
    "for f in FileList:\n",
    "    with open(binladenpath + f, 'r') as text:\n",
    "        raw = raw + text.read()\n",
    "        \n",
    "# with open(os.path.join(filepath, \"fulltext.txt\"), 'w') as text:\n",
    "#         text.write(raw)\n",
    "\n",
    "print('length of doc: ', len(raw))\n",
    "print(f'completed at: {datetime.datetime.now().strftime(\"%b %d %Y %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup spaCy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:  en_core_web_md\n",
      "completed at: Feb 27 2020 15:07:38\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "model = 'en_core_web_md'\n",
    "print('loading: ', model)\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "print(f'completed at: {datetime.datetime.now().strftime(\"%b %d %Y %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed at: Jul 23 2020 10:40:52\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "# setup object to store entity corrections, which in turn forms the basis for the custom pipeline component.\n",
    "named_entity_corrections = {\n",
    "    \n",
    "    # inbuilt with spaCy\n",
    "    \"PERSON\" : [\"usama bin muhammad bin ladin\"],\n",
    "    \"NORP\" : [\"ahlul-sunnah\", \"infidel\", \"kuffar\", \"kafiroon\", \"kaferoon\", \"muslim\", \"da'ees\", \"ulama\", \"afghan\", \"afghans\", \"Afghans\"],\n",
    "    \"FAC\"  : [\"makka\", \"ka'ba\", \"capitol\", \"guadalcanal\", \"the world trade center\", \\\n",
    "              \"the treaty room of the white house\"],\n",
    "    \"ORG\" : [\"bani quraydah\", \"taliban\", \"al qaeda\", \"egyptian islamic jihad\", \"islamic movement of uzbekistan\", \"FBI\", \\\n",
    "            \"republicans\", \"democrats\", \"mafia\", \"crusaders\", \"mujahideen\", \"mujahidin\", \"halliburton\", \"Jaish-i-Mohammed\", \\\n",
    "            \"ummah\", \"quraysh\", \"bani qainuqa'\"],\n",
    "    \"GPE\" : [\"NATO\", \"arabian peninsula\", \"land of the two holy places\", \"country of the two holy places\", \"land of the two holy mosques\" \\\n",
    "             \"country of the two holy mosques\", \"qana\", \"assam\", \"erithria\", \"chechnia\", \"makka\", \"makkah\", \"qunduz\", \"mazur-e-sharif\", \"rafah\"],\n",
    "    \"LOC\" : [\"dar al-islam\", \"kabal\", \"iwo jima\", \"ground zero\", \"world\", \"dunya\", \"Hindu Kush\"],\n",
    "    \"PRODUCT\" : [\"united 93\", \"global hawk\", \"flight 93\", \"predator\"],\n",
    "    \"EVENT\" : [\"september 11th\"],\n",
    "    \"WORK_OF_ART\" : [\"national anthem\", \"memorandum\", \"flag\", \"the marshall plan\", \"semper fi\", \"allahu akbar\"],\n",
    "    \"LAW\" : [\"constitution\", \"anti-ballistic missile treaty\", \"the treaty of hudaybiyyah\", \"kyoto agreement\", \"\tHuman Rights\"],\n",
    "    \"LANGUAGE\" : [],\n",
    "    \"DATE\" : [\"shawwaal\", \"muharram\", \"rashidoon\"],\n",
    "    \"TIME\" : [],\n",
    "    \"PERCENT\" : [],\n",
    "    \"MONEY\" : [\"riyal\"],\n",
    "    \"QUANTITY\" : [],\n",
    "    \"ORDINAL\" : [],\n",
    "    \"CARDINAL\" : [],\n",
    "    \n",
    "    ##user defined\n",
    "    \"DIRECTVIOLENCE\" : [\"gulf war\"],\n",
    "    \"STRUCTURALVIOLENCE\" : [\"cold war\", \"war on terror\"],\n",
    "    \"RELIGION\" : [\"islam\", \"christianity\"],\n",
    "    \"DEITY\" : [\"hubal\", \"god\", \"Lord\", \"almighty\"],\n",
    "    \"RELIGIOUSFIGURE\" : [\"jesus\", \"abraham\", \"jibreel\", \"ishmael\", \"isaac\", \"allah\", \"imraan\", \"hud\", \"aal-imraan\", \"al-ma'ida\", \\\n",
    "                         \"baqarah\", \"an-nisa\", \"al-ahzab\", \"shu'aib\", \"al'iz ibn abd es-salaam\", \\\n",
    "                        \"ibn taymiyyah\", \"an-noor\", \"majmoo' al fatawa\", \"luqman\", \"al-masjid an-nabawy\", \\\n",
    "                        \"abd ur-rahman ibn awf\", \"abu jahl\", \"aal imraan\", \"the messenger of allah\", \\\n",
    "                        \"Saheeh Al-Jame\", \"at-tirmidhi\", \"at-taubah\", \"haroon ar-rasheed\", \"ameer-ul-mu'mineen\", \\\n",
    "                        \"assim bin thabit\", \"moses\", \"satan\"],\n",
    "    \"RELIGIOUSLAW\" : [\"halal\", \"haram\", \"shari'a\", \"mushrik\", \"fatwa\", \"fatwas\", \"shariah\", \"shari'ah\"],\n",
    "    \"RELIGIOUSCONFLICT\" : [\"jihad\", \"crusade\"],\n",
    "    \"RELIGIOUS_WORK_OF_ART\" : [\"koranic\", \"Quran\", \"quran\", \"Koran\", \"as-sayf\", \"taghut\", \"torah\", \"psalm\", \"qiblah\", \"allahu akbar\"],\n",
    "    \"RELIGIOUS_EVENT\" : [\"Hegira\", \"the Day of Judgment\"],\n",
    "    \"RELIGIOUSENTITY\" : [\"MECCA\"],\n",
    "    \"RELIGIOUS_FAC\" : [\"kaa'ba\", \"ka'bah\"],\n",
    "}\n",
    "\n",
    "filepath = r'C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset'\n",
    "\n",
    "## create file to store entity corrections\n",
    "with open(os.path.join(filepath, \"named_entity_corrections.json\"), \"wb\") as f:\n",
    "    f.write(json.dumps(named_entity_corrections).encode(\"utf-8\"))\n",
    "print(f'completed at: {datetime.datetime.now().strftime(\"%b %d %Y %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import Span\n",
    "from spacy.pipeline import merge_noun_chunks\n",
    "import pandas as pd\n",
    "\n",
    "# create entity ruler for custom pipeline component\n",
    "entities = EntityRuler(nlp, overwrite_ents=True, phrase_matcher_attr = \"LOWER\")\n",
    "\n",
    "for key, value in named_entity_corrections.items():\n",
    "    pattern = {\"label\" : key, \"pattern\" : [{\"LOWER\" : {\"IN\" : value}}]}, #, \"POS\" : {\"IN\": [\"PROPN\", \"NOUN\"]}\n",
    "    entities.add_patterns(pattern)\n",
    "\n",
    "# modify spaCy pipeline with custom component\n",
    "    \n",
    "import json\n",
    "from spacy.pipeline import merge_entities\n",
    "from spacy.strings import StringStore\n",
    "\n",
    "for pipe in nlp.pipe_names:\n",
    "    if pipe not in ['tagger', \"parser\", \"ner\"]:\n",
    "        nlp.remove_pipe(pipe)\n",
    "        \n",
    "for key in named_entity_corrections.keys():\n",
    "    nlp.vocab.strings.add(key)\n",
    "        \n",
    "nlp.add_pipe(entities, after = \"ner\")\n",
    "# nlp.add_pipe(ent_matcher, before = \"ner\")\n",
    "nlp.add_pipe(merge_entities, last = True)\n",
    "#nlp.add_pipe(merge_noun_chunks, last = True)\n",
    "\n",
    "print(\"Pipeline Components\")\n",
    "print(' | '.join(nlp.pipe_names))\n",
    "\n",
    "print(\"processing doc\")\n",
    "doc = nlp(raw)\n",
    "print(\"doc processed\")\n",
    "\n",
    "print('-----')\n",
    "print(\"current corrections\")\n",
    "print('-----')\n",
    "#print out the corrections\n",
    "for label, terms in named_entity_corrections.items():\n",
    "    if len(terms) > 0:\n",
    "        patterns = [text.upper() for text in terms]\n",
    "        print(label, patterns)\n",
    "        \n",
    "#         patterns = [nlp.make_doc(text) for text in pattern[\"pattern\"]] # -- used for PhraseMatcher\n",
    "#         self.matcher.add(pattern[\"label\"], None, *patterns)\n",
    "\n",
    "print(f'completed at: {datetime.datetime.now().strftime(\"%b %d %Y %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Each Sentence to Check for Corrections\n",
    "\n",
    "Iterate through each sentence to review the named entities.\n",
    "\n",
    "Check the named entity against the wikipedia entry.\n",
    "\n",
    "Correct as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_wikisummary(token):\n",
    "\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "    page_py = wiki_wiki.page(token)\n",
    "\n",
    "    if page_py.exists():\n",
    "        return (page_py.title, \" \".join(str(nlp(page_py.summary, disable = ['tokenizer', 'ner']).sents.__next__()).split()))\n",
    "    else:\n",
    "        return ('no wiki reference', 'no wiki reference')\n",
    "\n",
    "\n",
    "filepath = \"C:/Users/Steve/University of Southampton/CulturalViolence/KnowledgeBases/Experiment 2 - Testing Named Entity Recognition in the spaCy models/\"\n",
    "\n",
    "if input(\"Restart from fresh (y/n): \").lower() == 'n':\n",
    "    filename = input('existing filename: ')\n",
    "\n",
    "    with open(os.path.join(filepath, filename), 'r') as fp:\n",
    "        corrections_dict = json.load(fp)\n",
    "        \n",
    "    with open(os.path.join(filepath, \"seen_tokens.json\"), 'r') as fp:\n",
    "        seen_tokens = {key for key in json.load(fp)}\n",
    "\n",
    "else:\n",
    "    corrections_dict = dict()\n",
    "    seen_tokens = set()\n",
    "    \n",
    "### !!! The bin laden object here needs to be changed.\n",
    "\n",
    "for i, doc in enumerate(binladen):\n",
    "\n",
    "    for token in binladen.speeches_nlp[i].text_nlp:\n",
    "        entries_dict = dict()\n",
    "\n",
    "        if token.ent_type_ and \\\n",
    "        token.ent_type_ not in ['ORATOR', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'] and \\\n",
    "        token.text not in seen_tokens:\n",
    "\n",
    "            seen_tokens.add(token.text)\n",
    "            \n",
    "            with open(os.path.join(filepath, \"seen_tokens.json\"), \"wb\") as f:\n",
    "                    f.write(json.dumps(dict.fromkeys(seen_tokens)).encode(\"utf-8\"))\n",
    "\n",
    "            wikientry = get_wikisummary(token.text)\n",
    "            entries_dict[token.text] = [token.ent_type_, wikientry[0], wikientry[1]]\n",
    "            entries_dict['sentence'] = ['', '', token.sent]\n",
    "            displacy.render(token.sent, style = 'ent')\n",
    "            pd.set_option('display.max_colwidth', -1)\n",
    "            \n",
    "            display(pd.DataFrame.from_dict(entries_dict, orient='index', columns = ['ent_type_', 'wiki_title', 'summary'])\n",
    "                .style.set_properties(**{'text-align': 'left'})\n",
    "                .set_table_styles([dict(selector='th', props=[('text-align', 'left')])]))\n",
    "\n",
    "            if input('correct y/n ').lower() == 'n':\n",
    "                corrections_dict[token.text] = {\n",
    "                    'original ent_type_' : token.ent_type_, \n",
    "                    'wiki_title': wikientry[0], \n",
    "                    'wiki_summary' : wikientry[1],\n",
    "                    'correction' : input('correct type')\n",
    "                }\n",
    "\n",
    "                ### check wiki entry and correct with manual entry if required\n",
    "                \n",
    "                answer = 'n'\n",
    "                while answer == 'n':\n",
    "                    display(pd.DataFrame.from_dict(corrections_dict[token.text], orient = \"index\"))\n",
    "                    \n",
    "                    answer = input('correct wiki entry? (y/n)').lower()\n",
    "                    \n",
    "                    if answer != 'n':\n",
    "                        break\n",
    "                                \n",
    "                    corrections_dict[token.text] = {\n",
    "                        'original ent_type_' : token.ent_type_, \n",
    "                        'wiki_title': input(\"wiki_title: \"), \n",
    "                        'wiki_summary' : input(\"wiki_summary: \"),\n",
    "                        'correction' : input(\"correct type: \")\n",
    "                    }\n",
    "                    \n",
    "                with open(os.path.join(filapth, \"binladen_entitycorrections.json\"), \"wb\") as f:\n",
    "                    f.write(json.dumps(corrections_dict).encode(\"utf-8\"))\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PDF Report for Each Orator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from weasyprint import HTML\n",
    "\n",
    "filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Experiment 2 - Testing Named Entity Recognition in the spaCy models/\"\n",
    "\n",
    "with open(os.path.join(filepath, \"binladen_entitycorrections.json\"), 'r') as fp:\n",
    "    questions = json.load(fp)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader(searchpath=filepath))\n",
    "template = env.get_template('myreport.html')\n",
    "  \n",
    "table = pd.DataFrame.from_dict(questions).T\n",
    "\n",
    "template_vars = {\"title\" : \"bin Laden Entity Corrections\",\n",
    "                 \"islamic_terms\": table.to_html()}\n",
    "    \n",
    "html_out = template.render(template_vars)\n",
    "HTML(string=html_out).write_pdf(os.path.join(filepath, \"binladen_entitycorrections.pdf\"), stylesheets=[os.path.join(filepath, \"style.css\")])    \n",
    "    \n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option(\"display.max_columns\", 999)\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "    \n",
    "display(pd.DataFrame.from_dict(questions).T\n",
    "        .style.set_properties(**{'text-align': 'left'})\n",
    "        .set_table_styles([dict(selector='th', props=[('text-align', 'left')])]))\n",
    "\n",
    "print(f'completed at {str(datetime.datetime.now())}') #1220"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
