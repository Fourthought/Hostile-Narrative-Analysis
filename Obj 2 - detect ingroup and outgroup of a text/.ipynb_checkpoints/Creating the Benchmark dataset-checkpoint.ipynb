{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Benchmark Data\n",
    "---\n",
    " \n",
    "In this notebook the benchmark data is creating for testing the NLP technologies in each experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InGroup and Outgroup For Each Orator\n",
    "\n",
    "In this cell a JSON object is created containing the ingroups and outgroups of each orator. These groups are noun phrases identifying the groups and are taken from the speech in which each orator identified their outgroup. For bin Laden, this was his first speech published on 23/08/1996; for Bush he first identified hi outgroup in his State of the Union address on 20/09/2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete at:  19/06/2020 - 15:22:57\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "groups_benchmark = {\n",
    "    \n",
    "    \"bush\" : {\n",
    "        \"ingroup\" : [\"america\", \"american people\", \"americans\", \"united states\", \"united states of America\", \"my fellow americans\", \"fellow americans\"],\n",
    "        \"outgroup\" : [\"al qaeda\", \"taliban regime\", \"taliban\", \"egyptian islamic jihad\", \"islamic movement of uzbekistan\"]\n",
    "    },\n",
    "    \n",
    "    \"binladen\" : {\n",
    "        \"ingroup\" : [\"people of islam\", \"islamic world\", \"ummah of islam\", \"muslims\", \"muslim people\", \"muslim nation\"],\n",
    "        \"outgroup\" : [\"zionist-crusaders alliance\", \"american crusaders\", \"american zionist alliance\", \"american-israeli alliance\", \\\n",
    "                      \"Jewish-crusade alliance\", \"saudi regime\", \"american enemy\", \"zionist-crusaders\", \"Christian armies of the Americans\", \n",
    "                     \"american people\", \"american army\", \"the bush administration\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Data/\"\n",
    "\n",
    "with open(os.path.join(filepath, \"groups_benchmark.json\"), \"wb\") as f:\n",
    "    f.write(json.dumps(groups_benchmark).encode(\"utf-8\"))\n",
    "    \n",
    "print(\"complete at: \", datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bush', 'binladen']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ingroup</th>\n",
       "      <th>outgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">bush</th>\n",
       "      <th>0</th>\n",
       "      <td>america</td>\n",
       "      <td>al qaeda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american people</td>\n",
       "      <td>taliban regime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>americans</td>\n",
       "      <td>taliban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>united states</td>\n",
       "      <td>egyptian islamic jihad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>united states of America</td>\n",
       "      <td>islamic movement of uzbekistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my fellow americans</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fellow americans</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">binladen</th>\n",
       "      <th>0</th>\n",
       "      <td>people of islam</td>\n",
       "      <td>zionist-crusaders alliance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>islamic world</td>\n",
       "      <td>american crusaders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ummah of islam</td>\n",
       "      <td>american zionist alliance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>muslims</td>\n",
       "      <td>american-israeli alliance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muslim people</td>\n",
       "      <td>Jewish-crusade alliance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim nation</td>\n",
       "      <td>saudi regime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>american enemy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>zionist-crusaders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>Christian armies of the Americans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>american people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>american army</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>the bush administration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ingroup                           outgroup\n",
       "bush     0                    america                           al qaeda\n",
       "         1            american people                     taliban regime\n",
       "         2                  americans                            taliban\n",
       "         3              united states             egyptian islamic jihad\n",
       "         4   united states of America     islamic movement of uzbekistan\n",
       "         5        my fellow americans                                   \n",
       "         6           fellow americans                                   \n",
       "binladen 0            people of islam         zionist-crusaders alliance\n",
       "         1              islamic world                 american crusaders\n",
       "         2             ummah of islam          american zionist alliance\n",
       "         3                    muslims          american-israeli alliance\n",
       "         4              muslim people            Jewish-crusade alliance\n",
       "         5              muslim nation                       saudi regime\n",
       "         6                                                american enemy\n",
       "         7                                             zionist-crusaders\n",
       "         8                             Christian armies of the Americans\n",
       "         9                                               american people\n",
       "         10                                                american army\n",
       "         11                                      the bush administration"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/19736080/creating-dataframe-from-a-dictionary-where-entries-have-different-lengths\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "keys = list(groups_benchmark.keys())\n",
    "print(keys)\n",
    "\n",
    "frames = []\n",
    "for value in groups_benchmark.values():\n",
    "    frames.append(pd.DataFrame(dict([ (k, pd.Series(v)) for k, v in value.items() ]), index = None).fillna(\"\"))\n",
    "\n",
    "display(pd.concat(frames , keys = keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core_web_md\n",
      "['tagger', 'parser', 'ner', 'Named Entity Matcher', 'merge_entities', 'Concept Matcher']\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "import pipeline\n",
    "importlib.reload(pipeline)\n",
    "cnd = pipeline.CND(extended = False)\n",
    "\n",
    "print(cnd.nlp.meta['name'])\n",
    "print([pipe for pipe in cnd.nlp.pipe_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting relevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "filepath = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CNDPipeline\\dataset\\Osama bin Laden\\fulltext.txt\"\n",
    "\n",
    "with open(filepath, \"r\") as f:\n",
    "    fulltext = f.read()\n",
    "    \n",
    "doc = cnd(fulltext)\n",
    "\n",
    "sents_dict = dict()\n",
    "\n",
    "for sent in doc.sents:\n",
    "    if doc[sent.end -1].text == '\\n':\n",
    "        sents_dict[len(sents_dict)] = str(sent)\n",
    "    else:\n",
    "        sents_dict[len(sents_dict)] = str(sent)\n",
    "        \n",
    "print(len(sents_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing Sentences Relating to Ingroup and Outgroup\n",
    "\n",
    "In this notebook we iterate over all the sentence in the speech if appropriate manually classify each sentence as either ingroup elevation or outgroup othering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete at:  19/06/2020 - 15:23:36\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "from spacy import displacy\n",
    "from visuals import sent_frame\n",
    "\n",
    "sents_dict = dict()\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "ingroup = dict()\n",
    "outgroup = dict()\n",
    "index = dict()\n",
    "ingroup_file = \"ingroup_sents.json\"\n",
    "ingroup_filepath = os.path.join(dirpath, ingroup_file)\n",
    "outgroup_file = \"outgroup_sents.json\"\n",
    "outgroup_filepath = os.path.join(dirpath, outgroup_file)\n",
    "index_filepath = os.path.join(dirpath, \"index.json\")\n",
    "\n",
    "# open previous file and progress index\n",
    "\n",
    "try:\n",
    "    with open(ingroup_filepath, 'r') as fp:\n",
    "        ingroup = json.load(fp)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    with open(outgroup_filepath, 'r') as fp:\n",
    "        outgroup = json.load(fp)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    with open(index_filepath, 'r') as fp:\n",
    "        index = json.load(fp)\n",
    "except:\n",
    "    index = 0\n",
    "\n",
    "#iterate over each sentence dictionary for classification of ingroup or outgroup\n",
    "while index < len(sents_dict):\n",
    "\n",
    "    # record progress  through dictionary object\n",
    "    with open(index_filepath, \"wb\") as f:\n",
    "            f.write(json.dumps(index).encode(\"utf-8\"))\n",
    "\n",
    "    # clear screen\n",
    "    clear_output(wait=True)# get text\n",
    "    \n",
    "    # show progress through input_dict\n",
    "    print(f'{index} / {len(sents_dict)}')\n",
    "    \n",
    "    # get sentence text\n",
    "    text = sents_dict[index]\n",
    "\n",
    "    # parse text\n",
    "    doc = cnd(text)\n",
    "\n",
    "    # if the option to show the dependency parse is passed display it\n",
    "#     displacy.render(doc, style=\"dep\")\n",
    "\n",
    "    # display the sentence frame in compact form\n",
    "    display(sent_frame(doc))\n",
    "\n",
    "    entry = input('ingroup(i) / outgroup(o) / delete (d) / back(b)').lower()\n",
    "    \n",
    "    # ask if sentence is refering to an ingroup or outgroup\n",
    "    if entry in ['i', 'o']:        \n",
    "        if entry == 'i': # add sentence to ingroup dictionary if user selects ingroup\n",
    "            print(len(ingroup), ' => ingroup add: ', text)\n",
    "            ingroup[len(ingroup)] = text\n",
    "            \n",
    "            # write dictionary to file\n",
    "            with open(ingroup_filepath, \"wb\") as f:\n",
    "                f.write(json.dumps(ingroup).encode(\"utf-8\"))\n",
    "            \n",
    "        else: # else add sentence to outgroup dictionary\n",
    "            print(len(outgroup), ' => outgroup add: ', text)\n",
    "            outgroup[len(outgroup)] = text\n",
    "            \n",
    "            # write dictionary to file\n",
    "            with open(outgroup_filepath, \"wb\") as f:\n",
    "                f.write(json.dumps(outgroup).encode(\"utf-8\"))\n",
    "                \n",
    "        # increase index by 1\n",
    "        index += 1\n",
    "    \n",
    "    # if user enters 'd' then go back by 1 in the dictionary and delete\n",
    "    elif entry == 'd': \n",
    "        if index != 0:\n",
    "            \n",
    "            # test whether the previous sentence was ingroup or outgroup and delete from respective dictionary\n",
    "            \n",
    "            if index >= 0 and len(ingroup) - 1 >= 0 and sents_dict[index-1] == ingroup[len(ingroup) - 1]:\n",
    "                print('deleting from ingroup: ', ingroup.pop())\n",
    "\n",
    "                with open(ingroup_filepath, \"wb\") as f:\n",
    "                    f.write(json.dumps(ingroup).encode(\"utf-8\"))\n",
    "\n",
    "            elif index >= 0 and len(outgroup) - 1 >= 0 and sents_dict[index-1] == outgroup[len(outgroup) - 1]:\n",
    "                print('deleting from outgroup: ', outgroup.pop())\n",
    "\n",
    "                with open(outgroup_filepath, \"wb\") as f:\n",
    "                    f.write(json.dumps(outgroup).encode(\"utf-8\"))\n",
    "\n",
    "            index -= 1\n",
    "        \n",
    "        else:\n",
    "            print('iterating backwards by one sentence')\n",
    "            pass\n",
    "        \n",
    "    # quit    \n",
    "    elif entry == 'q':\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        index += 1\n",
    "\n",
    "print(\"complete at: \", datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"))  #1220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Gold Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from spacy import displacy\n",
    "import importlib\n",
    "import visuals\n",
    "importlib.reload(visuals)\n",
    "\n",
    "filenames = [\"bush_ingroup_sents.jsonl\",\n",
    "             \"bush_outgroup_sents.jsonl\",\n",
    "             \"laden_ingroup_sents.jsonl\",\n",
    "             \"laden_outgroup_sents.jsonl\"]\n",
    "\n",
    "path = os.getcwd()\n",
    "index_filename = \"index.json\"\n",
    "index_filepath = os.path.join(path, index_filename)\n",
    "\n",
    "gold_ents_filename = \"gold_ents.jsonl\"\n",
    "gold_ents_filepath = os.path.join(path, gold_ents_filename)\n",
    "\n",
    "gold_ents = []\n",
    "\n",
    "try:  \n",
    "    with jsonlines.open(gold_ents_filepath) as f:\n",
    "        gold_ents = list(f.iter())\n",
    "    if len(test_chunks) == 0:\n",
    "        for filename in filenames:\n",
    "            with jsonlines.open(os.path.join(path, filename)) as f:\n",
    "                lines = list(f.iter())\n",
    "                for i, line in enumerate(lines): \n",
    "                    gold_ents.append({len(gold_ents) : line[str(i)]})\n",
    "except:\n",
    "    pass\n",
    "            \n",
    "try:\n",
    "    with open(index_filepath, \"r\") as index_json:\n",
    "        ref = json.load(index_json)\n",
    "        \n",
    "except:\n",
    "    ref = 0\n",
    "    \n",
    "print(ref)\n",
    "\n",
    "while ref < len(gold_ents):\n",
    "    line = gold_ents[ref]\n",
    "    \n",
    "    with open(index_filepath, \"wb\") as f:\n",
    "        f.write(json.dumps(ref).encode(\"utf-8\"))\n",
    "    \n",
    "    while True:\n",
    "        doc = cnd(line[str(ref)])\n",
    "        line[\"gold_chunks\"] = visuals.chunk_custom_attrs(list(doc._.custom_chunks), json = True)\n",
    "        index = 0\n",
    "\n",
    "        # iterate through each noun chunk to record whether ingroup/outgroup\n",
    "        while index < len(line[\"gold_chunks\"]):\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            options = {\"compact\": True}\n",
    "            displacy.render(doc, style = \"dep\", options = options)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            display(pd.DataFrame([line[str(ref)]]))\n",
    "            df = [[\"spans:\"] + [ent[\"text\"] for ent in line[\"gold_chunks\"]], \\\n",
    "                  [\"entity:\"] + [ent[\"entity\"] for ent in line[\"gold_chunks\"]], \\\n",
    "                  [\"modifier:\"] + [ent[\"modifier\"] for ent in line [\"gold_chunks\"]], \\\n",
    "                  [\"span_type:\"] + [ent[\"span_type\"] for ent in line[\"gold_chunks\"]], \\\n",
    "                  [\"ATTRIBUTE:\"] + [ent[\"ATTRIBUTE\"] for ent in line[\"gold_chunks\"]]]\n",
    "            display(pd.DataFrame(df))\n",
    "\n",
    "            \n",
    "            ent = line[\"gold_chunks\"][index]\n",
    "            print(ref, '/', len(gold_ents))\n",
    "            print(f'Named Entity ({ent[\"span_type\"]}) ({ent[\"ATTRIBUTE\"]}): {ent[\"text\"]}')\n",
    "\n",
    "            group = None\n",
    "            answers = {\"i\" : \"ingroup\", \"o\" : \"outgroup\", \"y\" : True, \"n\" : False} \n",
    "            \n",
    "            while group not in [\"i\", \"o\", \"q\", \"\", \"b\"]:\n",
    "                group = input(\"grouping? (i/o/q/b/q)\")\n",
    "            if group == \"\":\n",
    "                ent[\"grouping\"] = \"\"\n",
    "                ent[\"detectable\"] = \"\"\n",
    "                index += 1\n",
    "                continue\n",
    "            if group == \"q\":\n",
    "                raise SystemExit(\"Stop right there\")\n",
    "            if group == \"b\":\n",
    "                if index != 0:\n",
    "                    index -= 1\n",
    "                continue\n",
    "\n",
    "            detectable = None\n",
    "            while detectable not in [\"y\", \"n\"]:\n",
    "                detectable = input(\"detectable (y/n)\")\n",
    "\n",
    "            ent[\"grouping\"] = answers[group]\n",
    "            ent[\"detectable\"] = answers[detectable]\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        # check results\n",
    "        attrs = [\"text\", \"entity\", \"span_type\", \"ATTRIBUTE\", \"grouping\", \"detectable\"]\n",
    "        results = [[result[attr] for attr in attrs] for result in line[\"gold_chunks\"]]\n",
    "        display(pd.DataFrame(results, columns = attrs))\n",
    "        \n",
    "        satisfied = None\n",
    "        while satisfied not in [\"y\", \"n\", \"q\"]:\n",
    "            satisfied = input(\"safisfied? (y/n/q)\")\n",
    "        if satisfied == \"q\":\n",
    "            raise SystemExit(\"Stop right there\")\n",
    "        if satisfied == \"y\":\n",
    "            with jsonlines.open(os.path.join(path, gold_ents_filepath), 'w') as writer:\n",
    "                writer.write_all(gold_ents)\n",
    "            ref += 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for making corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "\n",
    "path = os.getcwd()\n",
    "gold_ents_filename = \"\"\n",
    "gold_ents_filepath = os.path.join(path, gold_ents_filename)\n",
    "\n",
    "gold_ents = []\n",
    "new_gold_ents = []\n",
    "\n",
    "with jsonlines.open(gold_ents_filepath) as f:\n",
    "    gold_ents = list(f.iter())\n",
    "\n",
    "attrs = [\"text\", \"grouping\", \"detectable\"]\n",
    "\n",
    "for i, gold_ent in enumerate(gold_ents):\n",
    "    for ent in gold_ent[\"gold_chunks\"]:\n",
    "        if ent[\"text\"].lower() == \"palestine\":\n",
    "            print(f'text: {gold_ent[str(i)]}')\n",
    "            for attr in attrs:\n",
    "                print(f'{attr}: {ent[attr]}')\n",
    "            ent[\"grouping\"] = \"ingroup\"\n",
    "            ent[\"detectable\"] = \"False\"\n",
    "            for attr in attrs:\n",
    "                print(f'{attr}: {ent[attr]}')\n",
    "            print('----')\n",
    "            \n",
    "# with jsonlines.open(os.path.join(path, gold_ents_filepath), 'w') as writer:\n",
    "#     writer.write_all(gold_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of named entities: 529\n",
      "total number of detectable named entities: 97\n",
      "total number of non-detectable named entities: 359\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "\n",
    "path = os.getcwd()\n",
    "gold_ents_filename = \"gold_ents.jsonl\"\n",
    "gold_ents_filepath = os.path.join(path, gold_ents_filename)\n",
    "\n",
    "gold_ents = []\n",
    "new_gold_ents = []\n",
    "\n",
    "with jsonlines.open(gold_ents_filepath) as f:\n",
    "    gold_ents = list(f.iter())\n",
    "    \n",
    "ents_total = 0\n",
    "detectable_count = 0\n",
    "non_detectable_count = 0\n",
    "\n",
    "for gold_ent in gold_ents:\n",
    "    for ent in gold_ent[\"gold_chunks\"]:\n",
    "        if ent[\"grouping\"]: \n",
    "            ents_total += 1\n",
    "        if ent[\"detectable\"] == True:\n",
    "            detectable_count += 1\n",
    "        if ent[\"detectable\"] == False:\n",
    "            non_detectable_count += 1\n",
    "            \n",
    "print(\"total number of named entities:\", ents_total)\n",
    "print(\"total number of detectable named entities:\", detectable_count)\n",
    "print(\"total number of non-detectable named entities:\", non_detectable_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Notes\n",
    "\n",
    "In this sentence \"America\" refers to Territory, whereas in others \"America\" will refer to the ingroup. Should GPE refer to a group asset rather than group?\n",
    "- \"In the past week, we have seen the American people at their very best everywhere in America.\"\n",
    "\n",
    "\"Muslims in nations\" should be a noun chunk, and group.\n",
    "- \"Both Americans and Muslim friends and citizens, tax-paying citizens, and Muslims in nations were just appalled and could not believe what -- what we saw on our TV screens.\"\n",
    "\n",
    "\"behalf of the American people\", should this be chunked? add to noun chunk gold list.\n",
    "- \"\tAnd on behalf of the American people, I thank they world for its outpouring of support.\"\n",
    "\n",
    "Need to make a decision about whether to create span_chunks in the ner component.\n",
    "\n",
    "Should this become a hypernymic phrase\n",
    "- \"America has no truer friend than Great Britain\"\n",
    "\n",
    "Chunk as \"many millions of Americans\" - add to noun chunk gold list.\n",
    "- \"It's practiced freely by many millions of Americans, and by millions more in countries that America counts as friends.\"\n",
    "\n",
    "\"GPE\" elements look like they become assets rather than a group.\n",
    "for now, mark assets as ingroup and figure out how to alter later.\n",
    "- \"And what is at stake is not just America's freedom.\"\n",
    "\n",
    "appositional modifier (appos) phrases\n",
    "- \"We are joined in this operation by our staunch friend, Great Britain.\"\n",
    "- \"Our staunch friends, Great Britain, our neighbors Canada and Mexico, our NATO allies, our allies in Asia, Russia and nations from every continent on the Earth have offered help of one kind or another -- from military assistance to intelligence information, to crack down on terrorists' financial networks.\"\n",
    "- \"At the same time, we are showing the compassion of America by delivering food and medicine to the Afghan people who are, themselves, the victims of a repressive regime.\"\n",
    "- \"I even had nice things to say about my friend, Ted Kennedy.\"\n",
    "- \"O protectors of monotheism and guardians of the faith; O successors of those who spread the light of guidance in the world; O grandsons of Sa'd Bin-Abi-Waqqas, al-Muthanna Bin-Harithah al-Shibani, al-Qa'qa' Bin-'Amr al-Tamimi, and the companions who fought alongside them: You rushed to join the Army and the Guard merely to join the jihad for the cause of God in order to spread the word of God and to defend Islam and the land of the two holy mosques against invaders and occupiers, which is the highest degree of belief in religion.\"\n",
    "- \"That was the only door left open to the public for ending injustice and upholding right and justice, and in whose interests do Prince Sultan and Prince Nayif plunge the country and the people into an internal war that would destroy everything, enlisting the aid and advice of those who fomented internal sedition in their country and using the people's police force to put down the reform movement there and pit members of the public one against the other—leaving the main enemy in the region, namely the Jewish-American alliance, safe and secure, having found such traitors to implement its policies aimed at exhausting the nation's human and financial resources internally.\"\n",
    "- \"But, thank God, the vast majority of the people, civilians and military, are aware of that sinister plan and will not allow themselves to be an instrument for strikes against one another in implementation of the policy of the main enemy, namely the Israeli-American alliance, through the Saudi regime, its agent in the country.\"\n",
    "\n",
    "pronoun modifer denoting group\n",
    "- \"Our Islamic nation has been tasting the same for more than 80 years of humiliation and disgrace, its sons killed and their blood spilled, its sanctities desecrated.\"\n",
    "\n",
    "interesting phrase, the Afghan people is only detectable as the ingroup if USA is also marked as ingroup.\n",
    "an extended \"billion Afghan people\" can be detected as ingroup from \"we are the friends of\"\n",
    "- \"The United States of America is a friend to the Afghan people, and we are the friends of almost a billion worldwide who practice the Islamic faith.\"\n",
    "\n",
    "the conjunctions from the head, \"diligent and determined work\" are split between dependency trees.\n",
    "- \"We may never know what horrors our country was spared by the diligent and determined work of our police forces, the FBI, ATF agents, federal marshals, Custom officers, Secret Service, intelligence professionals and local law enforcement officials, under the most trying conditions.\"\n",
    "\n",
    "the named entities \"Africa\" and \"Latin America\" are split from the conjunction with the head \"friends and allies\"\n",
    "- \"Together with friends and allies from Europe to Asia, and Africa to Latin America, we will demonstrate that the forces of terror cannot stop the momentum of freedom.\"\n",
    "\n",
    "should expand to \"brave men of the United States military\" and \"brave women of the United States military\"\n",
    "- \"And we have one more great asset in this cause: The brave men and women of the United States military.\"\n",
    "\n",
    "should expand so that \"Representative\" applies to each surname of the conjunction\n",
    "- \"I also want to thank Representative Porter Goss, LaFalce, Oxley, and Sensenbrenner for their hard work.\"\n",
    "\n",
    "should expand to \"America is ally against terror\" and \"Afghanistan is ally against terrorism\"\n",
    "- \"America and Afghanistan are now allies against terror.\"\n",
    "\n",
    "\"terror\" is annotated as a verb when it should be a noun\n",
    "- \"Al Qaeda is to terror what the mafia is to crime.\"\n",
    "\n",
    "\"we\" is a hypernym of the hyponym \"largest source of humanitarian aid\"\n",
    "- \"After all, we are currently its largest source of humanitarian aid; but we condemn the Taliban regime.\"\n",
    "\n",
    "how to mark \"military capability of the Taliban regime.\" as an outgroup asset when the root and modifier don't refer to an outgroup term.\n",
    "- \"These carefully targeted actions are designed to disrupt the use of Afghanistan as a terrorist base of operations and to attack the military capability of the Taliban regime.\"\n",
    "\n",
    "the verb \"close\" is marked as an ADJ (as in reference to distance) rather then a VERB\n",
    "- \"More than two weeks ago, I gave Taliban leaders a series of clear and specific demands: Close terrorist training camps; hand over leaders of the Al Qaeda network; and return all foreign nationals, including American citizens, unjustly detained in your country.\"\n",
    "\n",
    "adpositional phrase \"indifference of governments\" is split across dependency tree\n",
    "- add to custom chunks test data set\n",
    "- \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\"\n",
    "\n",
    "The phrase \"the suffering the Taliban have brought upon Afghanistan\" is a hyponym of the hyernym, \"the terrible burden of war.\"\n",
    "(interesting story phrase, Bush shares the responsibility for the \"burden of war\")\n",
    "- \"And my country grieves for all the suffering the Taliban have brought upon Afghanistan, including the terrible burden of war.\"\n",
    "\n",
    "noun chunk should be \"Abandoned al Qaeda houses in Kabul\"\n",
    "- add to custom chunks test data set\n",
    "- how to markup the noun phrase?\n",
    "- \"Abandoned al Qaeda houses in Kabul contained diagrams for crude weapons of mass destruction.\"\n",
    "\n",
    "split dependency for \"weapons of mass destruction\"\n",
    "- add to custom chunks test data set\n",
    "- \"North Korea is a regime arming with missiles and weapons of mass destruction, while starving its citizens.\"\n",
    "\n",
    "split dependency for \"people in industry\"\n",
    "- add to custom chunks test data set\n",
    "- good test phrase for adpositional phrases\n",
    "- \"The same thing has befallen the people in industry and in agriculture, the cities and villages, and the people in the desert and the rural areas.\"\n",
    "\n",
    "noun chunk should be \"brothers in Palestine\"\n",
    "- add to custom chunks test data set\n",
    "- \"The money paid for US goods is turning into bullets [fired at] the chests of our brothers in Palestine, and tomorrow the chests of the sons of the country of the two holy mosques; by buying their goods we are strengthening their economy while we continue to become poorer.\"\n",
    "\n",
    "noun chunks should be \"Russians in Afghanistan\" and \"Serbs in Bosnia-Herzegovina\"\n",
    "- add to custom chunks test data set\n",
    "- \"I say: If the sons of the country of the two holy mosques—who went to fight the Russians in Afghanistan, the Serbs in Bosnia-Herzegovina, and who are now fighting in Chechnya, and God has granted them victory over the Russians, who are allying with you—and they are also fighting in Tajikistan—believe in the need to fight against atheism everywhere, they have the strength and enthusiasm in the land in which they were born to defend their greatest holy sites, the holy Ka'bah, the qiblah of all Muslims.\"\n",
    "\n",
    "split dependency for \"[that enemy], namely the Israeli-American alliance\"\n",
    "- \"There is no greater duty after faith than warding [daf'] off [that enemy], namely the Israeli-American alliance occupying the land of the two holy mosques and the land of the ascension of the Prophet, may God's prayers and blessings be upon him.\"\n",
    "\n",
    "should read \"sons of Islam\" and \"daughters of Islam\"\n",
    "- \"Sons and daughters of Islam!\"\n",
    "\n",
    "should read, \"leaders of atheism in the United States\"?\n",
    "- add to noun chunk gold data.\n",
    "- \"Besides, this claim is no longer valid following the statements made by the leaders of atheism in the United States, the most recent being that of US Defense Secretary William Perry after the al-Khubar blast targeting US troops.\"\n",
    "\n",
    "merged dependency, \"Armed Forces, Guard\" should be split as a conjunction.\n",
    "- \"We alert you to the fact that the regime might carry out operations against members of the Armed Forces, Guard, or security forces and try to attribute them to the mujahidin with a view to driving a wedge between them and you.\"\n",
    "\n",
    "merged dependency, should read, \"our holy sites\" of \"the Jews\" and \"Christians\"\n",
    "- \"By doing that we will have contributed to ridding our holy sites of the Jews and Christians and forced them to leave our land, defeated, God willing.\"\n",
    "\n",
    "merged conjunction, should read, \"roles of Pharoah\", \"roles of Ceasar\" and \"roles of Chosroes\".\n",
    "- \"Today, the roles of Pharaoh, Caesar, and Chosroes have been taken up by Israel and United States, who first occupied our Aqsa Mosque, in the direction of which our Holy Prophet performed his prayers.\"\n",
    "\n",
    "missing entity, \"Israeli Tanks\"\n",
    "- In these days, Israeli tanks rampage across Palestine, in Ramallah, Rafah and Beit Jala and many other parts of the land of Islam\n",
    "\n",
    "very good hypernym phrase\n",
    "- \"The creation of Israel is a crime which must be erased.\"\n",
    "\n",
    "split apositional phrase, \"usurpation of their land\"\n",
    "- \"Thus the American people have chosen, consented to, and affirmed their support for the Israeli oppression of the Palestinians, the occupation and usurpation of their land, and its continuous killing, torture, punishment and expulsion of the Palestinians.\"\n",
    "\n",
    "strong Anti-semetic statement\n",
    "- \"As a result of this, in all its different forms and guises, the Jews have taken control of your economy, through which they have then taken control of your media, and now control all aspects of your life making you their servants and achieving their aims at your expense; precisely what Benjamin Franklin warned you against.\"\n",
    "\n",
    "missing entity, \"American Friends\"\n",
    "- \"The freedom and democracy that you call to is for yourselves and for white race only; as for the rest of the world, you impose upon them your monstrous, destructive policies and Governments, which you call the 'American friends'.\"\n",
    "\n",
    "noun chunk should read, \"Indians in Kashmir\" and \"Muslims in Southern Philippines.\"\n",
    "- add to chunker gold data\n",
    "- \"(4) We also advise you to stop supporting Israel, and to end your support of the Indians in Kashmir, the Russians against the Chechens and to also cease supporting the Manila Government against the Muslims in Southern Philippines.\"\n",
    "\n",
    "\n",
    "merged conjunction, should read, \"policies of sub dual\", \"theft\", \"occupation\" and \"policy of supporting the Jews\"\n",
    "- add to noun chunk gold data\n",
    "- \"(7) We also call you to deal with us and interact with us on the basis of mutual interests and benefits, rather than the policies of sub dual, theft and occupation, and not to continue your policy of supporting the Jews because this will result in more disasters for you.\"\n",
    "\n",
    "should read, \"people in Palestine\" and \"people in Lebanon\"\n",
    "- \"But after it became unbearable and we witnessed the oppression and tyranny of the American/Israeli coalition against our people in Palestine and Lebanon, it came to my mind.\"\n",
    "\n",
    "good hypernym phrase\n",
    "- \"And that day, it was confirmed to me that oppression and the intentional killing of innocent women and children is a deliberate American policy.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
