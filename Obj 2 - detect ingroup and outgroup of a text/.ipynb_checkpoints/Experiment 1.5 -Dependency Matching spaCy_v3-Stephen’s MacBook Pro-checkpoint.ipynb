{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing to Identify the Ingroup and Outgroup\n",
    "---\n",
    "\n",
    "In this notebook we are experimenting with dependency parsing to identify the ingroup and outgroup of a text.\n",
    "\n",
    "For this tasks we use spaCy's dependency matcher to identify the ingroup and outgroup using hypernymy.\n",
    "\n",
    "Hyponymic relationships are expressed in narrative clauses, therefore, we use the dependency matcher to detect hyponymic narrative clauses from which the ingroup and outgroup can be identified.\n",
    "\n",
    "We refractor a set of regex defined Hearst Patterns into dependency patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "CPU times: user 2.66 s, sys: 1.22 s, total: 3.88 s\n",
      "Wall time: 2.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from spacy.tokens import Token, Span, Doc\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Social</th>\n",
       "      <th>Religious</th>\n",
       "      <th>Commercial</th>\n",
       "      <th>Health</th>\n",
       "      <th>Security</th>\n",
       "      <th>Political</th>\n",
       "      <th>Military</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Neutral</th>\n",
       "      <th>0</th>\n",
       "      <td>Person</td>\n",
       "      <td>ReligiousGroup</td>\n",
       "      <td>Econ</td>\n",
       "      <td>MedicalPerson</td>\n",
       "      <td>SecurityGroup</td>\n",
       "      <td>Institution</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Community</td>\n",
       "      <td>ReligiousPerson</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Ingroup</th>\n",
       "      <th>0</th>\n",
       "      <td>Family</td>\n",
       "      <td>Believer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ally</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outgroup</th>\n",
       "      <th>0</th>\n",
       "      <td>Outcast</td>\n",
       "      <td>Aposate</td>\n",
       "      <td>Competitor</td>\n",
       "      <td>Vermin</td>\n",
       "      <td>Criminal</td>\n",
       "      <td></td>\n",
       "      <td>Adversary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Entity</th>\n",
       "      <th>0</th>\n",
       "      <td>Area</td>\n",
       "      <td>Area</td>\n",
       "      <td>Area</td>\n",
       "      <td>Area</td>\n",
       "      <td>Area</td>\n",
       "      <td>Area</td>\n",
       "      <td>Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Structure</td>\n",
       "      <td>Structure</td>\n",
       "      <td>Structure</td>\n",
       "      <td>Structure</td>\n",
       "      <td>Structure</td>\n",
       "      <td>Structure</td>\n",
       "      <td>Structure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Capability</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Capability</td>\n",
       "      <td>Capability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Event</td>\n",
       "      <td>Event</td>\n",
       "      <td>Event</td>\n",
       "      <td>Event</td>\n",
       "      <td>Event</td>\n",
       "      <td>Event</td>\n",
       "      <td>Event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elevation</th>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Othering</th>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Social        Religious  Commercial         Health  \\\n",
       "Neutral   0      Person   ReligiousGroup        Econ  MedicalPerson   \n",
       "          1   Community  ReligiousPerson                              \n",
       "Ingroup   0      Family         Believer                              \n",
       "          1        Ally                                               \n",
       "Outgroup  0     Outcast          Aposate  Competitor         Vermin   \n",
       "Entity    0        Area             Area        Area           Area   \n",
       "          1   Structure        Structure   Structure      Structure   \n",
       "          2  Capability       Capability  Capability       Medicine   \n",
       "          3       Event            Event       Event          Event   \n",
       "Elevation 0    Positive                                               \n",
       "Othering  0    Negative                                               \n",
       "\n",
       "                  Security    Political    Military  \n",
       "Neutral   0  SecurityGroup  Institution              \n",
       "          1                                          \n",
       "Ingroup   0                                          \n",
       "          1                                          \n",
       "Outgroup  0       Criminal                Adversary  \n",
       "Entity    0           Area         Area        Area  \n",
       "          1      Structure    Structure   Structure  \n",
       "          2     Capability   Capability  Capability  \n",
       "          3          Event        Event       Event  \n",
       "Elevation 0                                          \n",
       "Othering  0                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 190 ms, sys: 51.5 ms, total: 241 ms\n",
      "Wall time: 576 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "group_schema = {\n",
    "    \"Social\" : {\"Neutral\": {\"Person\": [\"man\"],\n",
    "                            \"Community\": [\"organization\", \"nomad\"]},\n",
    "                \"Ingroup\": {\"Family\": [\"brothers\"],\n",
    "                            \"Ally\": [\"friend\"]},\n",
    "                \"Outgroup\": {\"Outcast\": \"\"},\n",
    "                \"Entity\": {\"Area\": \"\",\n",
    "                          \"Structure\": [\"\"],\n",
    "                          \"Capability\": [\"thing\"],\n",
    "                          \"Event\": [\"\"]},\n",
    "                \"Elevation\": {\"Positive\": [\"truthful\", \"exceptional\", \"good\"]},\n",
    "                \"Othering\": {\"Negative\": [\"unfair\", \"hoax\"]}},\n",
    "    \"Religious\": {\"Neutral\": {\"ReligiousGroup\" : [\"ulema\"],\n",
    "                              \"ReligiousPerson\": [\"\"]},\n",
    "                  \"Ingroup\": {\"Believer\": [\"\"]},\n",
    "                  \"Outgroup\": {\"Aposate\": [\"\"]},\n",
    "                  \"Entity\": {\"Area\": [\"\"],\n",
    "                             \"Structure\": [\"\"],\n",
    "                             \"Capability\": [\"\"],\n",
    "                             \"Event\": [\"\"]},\n",
    "                  \"Elevation\": {},\n",
    "                  \"Othering\": {}},\n",
    "    \"Commercial\": {\"Neutral\": {\"Econ\": [\"passenger\", \"customer\"]},\n",
    "                   \"Ingroup\": {},\n",
    "                   \"Outgroup\": {\"Competitor\": [\"\"]},\n",
    "                   \"Entity\": {\"Area\": \"\",\n",
    "                              \"Structure\": [\"\"],\n",
    "                              \"Capability\": [\"\"],\n",
    "                              \"Event\": [\"\"]},\n",
    "                   \"Elevation\": {},\n",
    "                   \"Othering\": {}},\n",
    "    \"Health\" : {\"Neutral\": {\"MedicalPerson\": [\"\"]},\n",
    "                \"Ingroup\": {},\n",
    "                \"Outgroup\" : {\"Vermin\": [\"parasite\"]},\n",
    "                \"Entity\": {\"Area\": \"\",\n",
    "                           \"Structure\": [\"\"],\n",
    "                           \"Medicine\": [\"vaccine\"],\n",
    "                           \"Event\": [\"\"]},\n",
    "                \"Elevation\": {},\n",
    "                \"Othering\": {}},\n",
    "    \"Security\": {\"Neutral\": {\"SecurityGroup\": [\"\"]},\n",
    "                 \"Ingroup\": {},\n",
    "                 \"Outgroup\": {\"Criminal\": [\"terrorist\"]},\n",
    "                 \"Entity\": {\"Area\": \"\",\n",
    "                            \"Structure\": [\"\"],\n",
    "                            \"Capability\": [\"\"],\n",
    "                            \"Event\": [\"\"]},\n",
    "                 \"Elevation\": {},\n",
    "                 \"Othering\": {}},\n",
    "    \"Political\": {\"Neutral\": {\"Institution\": [\"authorities\", \"alliance\"]},\n",
    "                  \"Ingroup\": {},\n",
    "                  \"Outgroup\": {},\n",
    "                  \"Entity\": {\"Area\": \"\",\n",
    "                             \"Structure\": [\"\"],\n",
    "                             \"Capability\": [\"\"],\n",
    "                             \"Event\": [\"\"]},\n",
    "                  \"Elevation\": {},\n",
    "                  \"Othering\": {}},\n",
    "    \"Military\": {\"Neutral\": {},\n",
    "                 \"Ingroup\": {},\n",
    "                 \"Outgroup\": {\"Adversary\": [\"enemy\"]},\n",
    "                 \"Entity\": {\"Area\": \"\",\n",
    "                            \"Structure\": [\"\"],\n",
    "                            \"Capability\": [\"\"],\n",
    "                            \"Event\": [\"\"]},\n",
    "                 \"Elevation\": {},\n",
    "                 \"Othering\": {}}\n",
    "}\n",
    "\n",
    "labels = []\n",
    "typology = dict()\n",
    "typology_chart = dict()\n",
    "\n",
    "## create a list of keys\n",
    "schema = {ideology: {subcat: ', '.join(list(terms.keys())) for (subcat, terms) in value.items()} \n",
    "             for (ideology, value) in group_schema.items()}\n",
    "\n",
    "keys = [list(cat.keys()) for cat in list(schema.values())][0]\n",
    "\n",
    "## Create frames for table\n",
    "frames = []\n",
    "schema = {ideology: {subcat: list(terms.keys()) for (subcat, terms) in value.items()} \n",
    "             for (ideology, value) in group_schema.items()}\n",
    "\n",
    "\n",
    "for frame in [list(cat.keys()) for cat in list(schema.values())][0]:\n",
    "    frames.append(pd.DataFrame.from_dict({k : v[frame] for k, v in list(schema.items())}, orient = 'index').fillna(\"\").T)\n",
    "\n",
    "# display table\n",
    "display(pd.concat(frames, keys = keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 2.86 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def doc_dep_graph(doc, strip = False):\n",
    "    \n",
    "    ''' \n",
    "    Put the graph with entity labels present (see 'tag' and 'label')\n",
    "    '''\n",
    "    \n",
    "    words = [] # the words of a dependency graph\n",
    "    arcs = [] # the arcs of a dependency graph\n",
    "    \n",
    "    for tok in doc:\n",
    "        \n",
    "        labels = dict()\n",
    "        labels[\"text\"] = tok.text\n",
    "        if tok._.concept:\n",
    "            labels[\"tag\"] = tok._.concept\n",
    "        else:\n",
    "            labels[\"tag\"] = tok.ent_type_\n",
    "        words.append(labels)\n",
    "        \n",
    "#     words = [{\"text\" : tok.text, \"tag\": tok.pos_} for tok in doc]\n",
    "#     print(\"hasModifier: \", [(tok, tok._.hasModifier) for tok in doc])\n",
    "#     print(\"isModifying: \", [(tok, tok._.isModifying) for tok in doc])\n",
    "    \n",
    "    # iterate through each token in a doc\n",
    "    for tok in doc:\n",
    "        \n",
    "        # ignore punctuation\n",
    "        if tok.dep_ in ['punct']:\n",
    "            continue\n",
    "    \n",
    "        if strip and not len(tok._.hasModifier):\n",
    "            continue\n",
    "\n",
    "        # if the token head has a method\n",
    "        # the label becomes the method label\n",
    "        \n",
    "        if len(tok._.hasModifier) > 0:\n",
    "            for mod in tok._.hasModifier:\n",
    "#                 print(tok, '=>', mod[0], '=>', mod[0][1], '=>', tok.i)\n",
    "                if mod[0] != \"isHyponym\":\n",
    "                    label = mod[0]\n",
    "                    head = mod[1].i\n",
    "\n",
    "                    if tok.i < head:\n",
    "                        arcs.append({\n",
    "                            \"start\": tok.i,\n",
    "                            \"end\": head,\n",
    "                            \"label\": label,\n",
    "                            \"dir\": \"right\"\n",
    "                        })\n",
    "\n",
    "                    # if the token index is greater than its head, the token head is the head\n",
    "                    elif tok.i > head:\n",
    "                        arcs.append({\n",
    "                            \"start\": head,\n",
    "                            \"end\": tok.i,\n",
    "                            \"label\": label,\n",
    "                            \"dir\": \"left\"\n",
    "                        })\n",
    "        else:\n",
    "            if len(tok._.isModifying):\n",
    "                label = \"\"\n",
    "            else:\n",
    "                label = tok.dep_\n",
    "            if tok.i < tok.head.i:\n",
    "                arcs.append({\n",
    "                    \"start\": tok.i,\n",
    "                    \"end\": tok.head.i,\n",
    "                    \"label\": label,\n",
    "                    \"dir\": \"left\"\n",
    "                })\n",
    "\n",
    "            # if the token index is greater than its head, the token head is the head\n",
    "            elif tok.i > tok.head.i:\n",
    "                arcs.append({\n",
    "                    \"start\": tok.head.i,\n",
    "                    \"end\": tok.i,\n",
    "                    \"label\": label,\n",
    "                    \"dir\": \"right\"\n",
    "                })\n",
    "    return {\"words\": words, \"arcs\": arcs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup The Named Concept Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "2:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "3 ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'NCR']\n",
      "CPU times: user 968 µs, sys: 48 µs, total: 1.02 ms\n",
      "Wall time: 1.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "def custom_ents(doc):\n",
    "    \n",
    "    \"\"\"\n",
    "    returns entities with their modifier as an entity phrase\n",
    "    \"\"\"\n",
    "    ents = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.root.dep_ in [\"amod\", \"compound\"]:\n",
    "            new_ent = Span(doc, ent.start, ent.root.head.i + 1, label = ent.root.ent_type_)\n",
    "            ents.append(new_ent)\n",
    "        else:\n",
    "            ents.append(ent)\n",
    "    return ents\n",
    "\n",
    "print(\"1: \", nlp.pipe_names)\n",
    "\n",
    "@Language.factory(\"NCR\", assigns = [], default_config = {})\n",
    "def create_Named_Concept_Recognition(nlp, name):\n",
    "    return Named_Concept_Recognition(nlp, name)\n",
    "\n",
    "class Named_Concept_Recognition:\n",
    "    \n",
    "    \"\"\"\n",
    "    Pipeline Component for labelling concepts by schema types\n",
    "    and as an named entity for nouns with a named entity modifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nlp, name):\n",
    "        \n",
    "        # snippet of the group schema\n",
    "        self.group_labels = {\n",
    "            \"Social\" : {\n",
    "                \"neutral\": {\"Person\": [\"man\"],\n",
    "                            \"Community\": [\"organization\", \"nomad\"],\n",
    "                           \"entity\": [\"thing\"]},\n",
    "                \"ingroup\": {\"Family\": [\"brothers\"],\n",
    "                            \"Ally\": [\"friend\"]},\n",
    "                \"elevation\": {\"Positive\": [\"truthful\", \"exceptional\", \"good\"]},\n",
    "                \"othering\": {\"Negative\": [\"unfair\", \"hoax\"]}},\n",
    "            \"Religious\": {\"neutral\": {\"ReligiousGroup\" : [\"ulema\"]}},\n",
    "            \"Commercial\": {\"neutral\": {\"Customer\": [\"passenger\"]}},\n",
    "            \"Health\" : {\"neutral\": {\"entity\": [\"vaccine\"]},\n",
    "                        \"outgroup\" : {\"Vermin\": [\"parasite\"]}},\n",
    "            \"Security\": {\"outgroup\": {\"Criminal\": [\"terrorist\"]}},\n",
    "            \"Political\": {\"neutral\": {\"Group\": [\"authorities\", \"alliance\"]}},\n",
    "            \"Military\": {\"outgroup\": {\"Adversary\": [\"enemy\"]}}\n",
    "        }\n",
    "\n",
    "        # set the custom labels\n",
    "        Token.set_extension(\"concept\", default = \"\", force = True)\n",
    "        Token.set_extension(\"attribute\", default = \"\", force = True)\n",
    "        Token.set_extension(\"context\", default = \"\", force = True)\n",
    "        Token.set_extension(\"modifyingTerm\", default = \"\", force = True)\n",
    "        Token.set_extension(\"hasModifier\", default = [], force = True)\n",
    "        Token.set_extension(\"isModifying\", default = [], force = True)\n",
    "        Token.set_extension(\"get_noun_span\", getter = self.get_noun_span, force = True)\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        for token in doc:\n",
    "            token._.context, token._.attribute, token._.concept = self.get_concept(token)\n",
    "            token._.modifyingTerm = token\n",
    "            \n",
    "        return doc\n",
    "    \n",
    "    def get_concept(self, token):\n",
    "        \n",
    "        \"\"\"\n",
    "        token extension for getting labels from group schema\n",
    "        \"\"\"\n",
    "        for context, attributes in self.group_labels.items():\n",
    "            for attribute, concepts in attributes.items():\n",
    "                for concept, terms in concepts.items():\n",
    "                    if token.lemma_.lower() in terms:\n",
    "                        return context, attribute, concept\n",
    "        return None, None, None\n",
    "    \n",
    "    def get_noun_span(self, token): # Named Concept Recognition\n",
    "        \n",
    "        nominal_deps = [\"appos\", \"acl\", \"relcl\", \"det\", \"predet\", \"nummod\", \"amod\", \"poss\", \"nmod\"]\n",
    "        adverbial_deps = [\"advmod\", \"advcl\", \"neg\", \"npmod\"]\n",
    "        compound_deps = [\"compound\", \"prt\", \"case\", \"mark\"]\n",
    "        \n",
    "        for chunk in token.subtree:\n",
    "            if chunk.i == token.i:\n",
    "                return token\n",
    "            if chunk.dep_ in nominal_deps + adverbial_deps + compound_deps:\n",
    "                return doc[chunk.i : token.i+1]\n",
    "                \n",
    "def explain(doc):\n",
    "\n",
    "    \"\"\"\n",
    "    doc extension for retrieving grouping category\n",
    "    \"\"\"\n",
    "    modification = {\"elevation\": \"elevated\",\n",
    "                   \"othering\": \"othered\",\n",
    "                   \"outgroup\": \"othered\",\n",
    "                   \"ingroup\": \"elevated\",\n",
    "                   \"neutral\": \"neutral\"}\n",
    "\n",
    "    for ent in doc.noun_chunks:\n",
    "        \n",
    "        # get the ent span to include compound statements\n",
    "        ent_span = ent.root\n",
    "        for left in ent_span.lefts:\n",
    "            if left.dep_ == [\"compound\", \"amod\"] or left.ent_type_ or left._.concept:\n",
    "                ent_span = doc[left.i : ent_span.i + 1]\n",
    "                break\n",
    "        \n",
    "        # get whether the ent has a modifying term\n",
    "        modifier = ent.root._.modifyingTerm\n",
    "        \n",
    "        if ent.root._.concept:\n",
    "            print(f\"'{ent_span}' has a '{modifier._.attribute}' classification where '{modifier}' is an '{modifier._.concept}' phrase from the '{modifier._.context}' context.\")\n",
    "        \n",
    "        if ent.root._.hasModifier:\n",
    "        \n",
    "            for method in ent.root._.hasModifier:\n",
    "                \n",
    "                # get the modifier noun span\n",
    "                noun_span = ''\n",
    "                noun_span = method[1]._.get_noun_span\n",
    "                modifier = method[1]._.modifyingTerm\n",
    "\n",
    "                # if the dependency label is negated\n",
    "                if method[0] in [\"hasNegation\"]:\n",
    "                    print(f\"'{ent_span}' is disassociated from '{noun_span}', which is a '{modifier._.attribute}' term from the '{modifier._.context}' context.\")\n",
    "                    continue\n",
    "                    \n",
    "                if method[0] in [\"hasModifier\"] and ent.root.ent_type_:\n",
    "                    print(f\"'{ent_span}' is {modification[method[1]._.attribute]} by the '{spacy.explain(method[1].dep_)} ({method[1].dep_})' term '{method[1]}' from the '{method[1]._.context}' context.'\")\n",
    "                    \n",
    "                elif method[0] not in [\"isHyponym\"] and not ent.root._.concept:\n",
    "                    modifier = method[1]._.modifyingTerm\n",
    "                    print(f\"'{ent_span}' is referred to as '{noun_span}', which is a '{modifier._.attribute}' term from the '{modifier._.context}' context.\")\n",
    "                \n",
    "                # warning for particular using of terms\n",
    "                if modifier._.concept == \"Vermin\":\n",
    "                    print(f\"WARNING: the term '{modifier}' referring to '{ent_span}' is from the '{modifier._.concept}' category and is often used in genocidal language.\")\n",
    "                    \n",
    "print(\"2: \", nlp.pipe_names)\n",
    "component = \"NCR\"\n",
    "if component in nlp.pipe_names:\n",
    "    nlp.remove_pipe(component)\n",
    "\n",
    "nlp.add_pipe(\"NCR\")\n",
    "Doc.set_extension(\"explain\", method = explain, force = True)\n",
    "Doc.set_extension(\"custom_ents\", getter = custom_ents, force = True)\n",
    "print(\"3\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attribute_ruler', 'tok2vec', 'merge_noun_chunks', 'merge_entities', 'merge_subtokens', 'token_splitter', 'doc_cleaner', 'parser', 'beam_parser', 'lemmatizer', 'trainable_lemmatizer', 'entity_linker', 'ner', 'beam_ner', 'entity_ruler', 'tagger', 'morphologizer', 'senter', 'sentencizer', 'textcat', 'spancat', 'future_entity_ruler', 'span_ruler', 'textcat_multilabel', 'en.lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.factory_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up The Clause Labelling Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 µs, sys: 0 ns, total: 164 µs\n",
      "Wall time: 167 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "class Clause_Labelling:\n",
    "    \n",
    "    def __init__(self, vocab, patterns, pattern_names=None):\n",
    "         \n",
    "    # Add patterns with pattern_names to the dependency matcher \n",
    "        \n",
    "        if pattern_names is None:\n",
    "            pattern_names = [\"pattern\" + str(pi) for pi in range(len(patterns))]\n",
    "        else:\n",
    "            pattern_names = [x for x in pattern_names]\n",
    "        self.matcher = DependencyMatcher(vocab)\n",
    "        for pi, pattern in enumerate(patterns):\n",
    "            print(\"pattern names: \", pattern_names[pi], pattern)\n",
    "            self.matcher.add(pattern_names[pi], pattern)\n",
    "        \n",
    "    def __call__(self, doc): # Clause Labelling\n",
    "    # Match the patterns to a doc, returns dep graph with edges that match \n",
    "    \n",
    "        matches = []\n",
    "        # iterate through the matches\n",
    "        for match in self.matcher(doc):\n",
    "\n",
    "            # capture the match label\n",
    "            match_id = nlp.vocab.strings[match[0]]\n",
    "#             print(f\"{match_id}: {[doc[i] for i in match[1][0]]}\")\n",
    "            \n",
    "            methods = {\n",
    "                \"hasMod\": \"hasModifier\",\n",
    "                \"hasModifier\": \"isModifierOf\",\n",
    "                \n",
    "                \"isHyponym\": \"hasHypernym\",\n",
    "                \"hasHypernym\": \"isHyponym\",\n",
    "                \"isHyp\" : \"isHyponym\",\n",
    "                \"verbprepPredicate\" : \"isHyponym\",\n",
    "                \"verbPredicate\" : \"isHyponym\",\n",
    "                \n",
    "                \"hasNameOf\": \"isCalled\",\n",
    "                \"hasNamely\": \"isCalled\",\n",
    "                \"isCalled\": \"isKnownAs\"\n",
    "            }\n",
    "\n",
    "            for subtree in match[1]:\n",
    "                negated = \"\"\n",
    "                for i, idx in zip(range(len(subtree)), subtree):\n",
    "                    pattern = patterns[match_id][i][\"SPEC\"][\"NODE_NAME\"]\n",
    "                    if pattern == \"SUBJECT\":\n",
    "                        clause_subject = doc[idx]\n",
    "                    if pattern == \"OBJECT\":\n",
    "                        clause_object = doc[idx]\n",
    "                    if pattern == \"NEGATED\":\n",
    "                        negated = doc[idx]\n",
    "\n",
    "                if clause_object._.concept:\n",
    "                    clause_subject._.hasModifier.append((methods[match_id], clause_object))\n",
    "                    clause_object._.isModifying.append((methods[clause_subject._.hasModifier[-1][0]], clause_subject))\n",
    "                    \n",
    "                if match_id in [\"hasMod\"] and clause_object._.concept:\n",
    "                    clause_subject._.modifyingTerm = clause_object\n",
    "                    \n",
    "                elif match_id not in [\"hasMod\"]:\n",
    "                    clause_subject._.isModifying.append((methods[match_id], clause_object))\n",
    "                    clause_object._.hasModifier.append((methods[clause_subject._.isModifying[-1][0]], clause_subject))\n",
    "#                     clause_object._.modifyingTerm = clause_subject\n",
    "                \n",
    "                if negated:\n",
    "                    negated._.isModifying.append((\"isNegated\", clause_object))\n",
    "                    clause_object._.hasModifier.append((\"hasNegation\", negated))\n",
    "#                     clause_object._.modifyingTerm = clause_subject\n",
    "        return doc\n",
    "\n",
    "# if not Language.has_factory(\"Clause_Labelling\"):\n",
    "#     @Language.factory(\"Clause_Labelling\",\n",
    "#                       assigns=[],\n",
    "#                       default_config={'patterns': None, 'pattern_names': None})\n",
    "#     def makeClauseLabelling(nlp, name, patterns, pattern_names):\n",
    "#         \"\"\"\n",
    "#         Utility function for creating spaCy pipeline component.\n",
    "#         \"\"\"\n",
    "\n",
    "#         return Clause_Labelling(nlp.vocab, patterns=patterns)\n",
    "    \n",
    "# if not Language.has_factory(\"Clause_Labelling\"):\n",
    "\n",
    "@Language.factory(\"Clause_Labelling\", default_config={'patterns': None, 'pattern_names': None})\n",
    "def makeClauseLabelling(nlp, name, patterns, pattern_names):\n",
    "    \"\"\"\n",
    "    Utility function for creating spaCy pipeline component.\n",
    "    \"\"\"\n",
    "\n",
    "    return Clause_Labelling(nlp.vocab, patterns=patterns, pattern_names=pattern_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Language.has_factory(\"Clause_Labelling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Patterns for the Parser\n",
    "\n",
    "Developing general patterns for clauses using the labelling schema reveals a primary syntactic model upon whichlayers of meaning are applied according to the predicate relationship. In reference to UD framework categories, one of six subject dependency labels generally refer to a clause’s subject, and one of four object labels refer to the object,while one of two prepositional or one of three complement labels can refer to either. For Labov and Waletzky, predicatesare a clause’s head, which indicates its syntactic function and sequence in a narrative. For detecting a predicate’s syntactic function, the UD framework is also used for verb labelling, while VerbNet provides a lexical resource for interpreting its the semantic meaning. As will be shown, how the subject and object are linked by the predicate gives different meaning to each clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "naming_predicate_list = {\"LEMMA\": {\"IN\": [\"know\", \"name\", \"namely\", \"baptize\", \"call\", \"christen\",\n",
    "                         \"dub\", \"entitle\", \"nickname\", \"rename\"]}}\n",
    "verb_tag_list = {\"TAG\": {\"IN\": [\"VBN\", \"VBD\", \"VBP\", \"VBG\", \"VBZ\"]}}\n",
    "_object = {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}}\n",
    "_subject = {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\", \"PRON\"]}}\n",
    "subject_deps = {\"DEP\": {\"IN\": [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]}}\n",
    "object_deps = {\"DEP\": {\"IN\": [\"dobj\", \"dative\", \"attr\", \"oprd\"]}}\n",
    "complements_deps = {\"DEP\": {\"IN\": [\"ccomp\", \"xcomp\", \"acomp\"]}}\n",
    "nominal_deps = {\"DEP\": {\"IN\": [\"appos\", \"acl\", \"relcl\", \"det\", \"predet\", \"nummod\", \"amod\", \"poss\", \"nmod\"]}}\n",
    "adverbial_deps = {\"DEP\": {\"IN\": [\"advmod\", \"advcl\", \"neg\", \"npmod\"]}}\n",
    "preposition_deps = {\"DEP\": {\"IN\": [\"pobj\", \"pcomp\"]}}\n",
    "coordination_deps = {\"DEP\": {\"IN\": [\"conj\", \"cc\", \"preconj\", \"prep\"]}}\n",
    "auxiliary_deps = {\"DEP\": {\"IN\": [\"aux\", \"auxpass\"]}}\n",
    "compound_deps = {\"DEP\": {\"IN\": [\"compound\", \"prt\", \"case\", \"mark\"]}}\n",
    "ROOT = {\"DEP\": {\"IN\": [\"ROOT\"]}}\n",
    "_object = {**_object, **object_deps}\n",
    "_subject = {**_subject, **subject_deps}\n",
    "\n",
    "def join_objs(d1, lst):\n",
    "    DEPS = []\n",
    "    for entry in lst:\n",
    "        DEPS += entry[\"DEP\"][\"IN\"]\n",
    "\n",
    "    return dict([(\"POS\", d1[\"POS\"]), (\"DEP\", {\"IN\": d1[\"DEP\"][\"IN\"] + DEPS})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Detects Nouns or Proper Nouns which are modified by a Named Concept\n",
    "\n",
    "The modification method detects modifier relationships in the dependency parse for both objectives one and two of the methodology. \n",
    "\n",
    "To explain this method, we take four wrongly classified named entities from the first Sentiment Analysis experiment\n",
    "- Muslim friends (Bush (-0.83)), \n",
    "- the United States Authorities (Bush (-0.64))\n",
    "- Mujahidin Brothers (bin Laden (-0.58))\n",
    "- US Enemy (bin Laden (+0.42)).\n",
    "\n",
    "The code below shows how using the group labelling schema, each concept is labelled according to its group concept using the custom pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern names:  hasMod [{'PATTERN': {'POS': {'IN': ['NOUN', 'PROPN']}}, 'SPEC': {'NODE_NAME': 'SUBJECT'}}, {'PATTERN': {'DEP': {'IN': ['amod', 'compound']}}, 'SPEC': {'NBOR_NAME': 'SUBJECT', 'NBOR_RELOP': '>', 'NODE_NAME': 'OBJECT'}}]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1008] Invalid pattern: each pattern should be a list of dicts. Check that you are providing a list of patterns as `List[List[dict]]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_grained\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_lemma\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m110\u001b[39m,\n\u001b[1;32m     22\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_spacing\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m     23\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollapse_punct\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# nlp.add_pipe('ClauseLabelling')\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[43mClause_Labelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# nlp.add_pipe(\"Clause_Labelling\", config={\"patterns\": patterns})\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n",
      "File \u001b[0;32m<timed exec>:16\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, vocab, patterns, pattern_names)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spaCy_v3/lib/python3.10/site-packages/spacy/matcher/dependencymatcher.pyx:174\u001b[0m, in \u001b[0;36mspacy.matcher.dependencymatcher.DependencyMatcher.add\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spaCy_v3/lib/python3.10/site-packages/spacy/matcher/dependencymatcher.pyx:116\u001b[0m, in \u001b[0;36mspacy.matcher.dependencymatcher.DependencyMatcher._validate_input\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E1008] Invalid pattern: each pattern should be a list of dicts. Check that you are providing a list of patterns as `List[List[dict]]`."
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"our many Muslim friends. the United States Authorities.\",\n",
    "    \"your Mujahidin Brothers. the US Enemy.\",\n",
    "    \"the unfair United Nations. the truthful Ulema.\"\n",
    "        ]\n",
    "\n",
    "patterns = {}\n",
    "\n",
    "patterns.update({\"hasMod\": [\n",
    "    \n",
    "    {\"PATTERN\": {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}},\n",
    "     \"SPEC\": {\"NODE_NAME\": \"SUBJECT\"}},\n",
    "    \n",
    "    {\"PATTERN\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"SUBJECT\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"OBJECT\"}}\n",
    "    \n",
    "]})\n",
    "        \n",
    "options = {\"fine_grained\" : True,\n",
    "          \"add_lemma\" : True,\n",
    "          \"distance\" : 110,\n",
    "          \"word_spacing\" : 40,\n",
    "          \"collapse_punct\": True}\n",
    "\n",
    "# nlp.add_pipe('ClauseLabelling')\n",
    "nlp.add_pipe(Clause_Labelling(nlp.vocab, patterns.values(), patterns.keys()))\n",
    "# nlp.add_pipe(\"Clause_Labelling\", config={\"patterns\": patterns})\n",
    "\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "#     print([(tok, tok._.isModifying) for tok in doc])\n",
    "    displacy.render(doc_dep_graph(doc, strip = False), style = \"dep\", manual = True, options = options)\n",
    "    doc._.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from above}, these labels enable the generation of an rationale to explain group classification from a compound or adjectival modifier (amod) relationship. \n",
    "\n",
    "As also shown above, this method can also be used to explain how an named entity is either elevated or othered by a modifier term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Naming Method\n",
    "\n",
    "While the modification method draws upon a simple modifier relationship, the naming method is the first of the three to draw upon language clauses. \n",
    "\n",
    "The pattern for this method is based on the clause head linking a a subject and object by a naming term. \n",
    "\n",
    "The predicate terms from the data set for this method are 'known' and 'named', and using VerbNet, 20 similarily functional terms are added to the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {}\n",
    "\n",
    "patterns.update({\"hasMod\": [\n",
    "    \n",
    "    {\"PATTERN\": {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}},\n",
    "     \"SPEC\": {\"NODE_NAME\": \"SUBJECT\"}},\n",
    "    \n",
    "    {\"PATTERN\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"SUBJECT\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"OBJECT\"}}\n",
    "    \n",
    "]})\n",
    "\n",
    "patterns.update({\"hasNameOf\": [  # object naming object\n",
    "\n",
    "    # OBJECT known as / named SUBJECT\n",
    "\n",
    "    {\"PATTERN\": join_objs(_subject, [preposition_deps]),\n",
    "     \"SPEC\": {\"NODE_NAME\": \"SUBJECT\"}},\n",
    "\n",
    "    {\"PATTERN\": naming_predicate_list,\n",
    "        \"SPEC\": {\"NBOR_NAME\": \"SUBJECT\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"PREDICATE\"}},\n",
    "\n",
    "    {\"PATTERN\": join_objs(_object, [_subject, preposition_deps]),\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"PREDICATE\", \"NBOR_RELOP\": \">>\", \"NODE_NAME\": \"OBJECT\"}}\n",
    "]})\n",
    "\n",
    "patterns.update({\"hasNamely\": [  # object naming object\n",
    "\n",
    "    # VERB the OBJECT, namely the SUBJECT\n",
    "\n",
    "    {\"PATTERN\": _object,\n",
    "     \"SPEC\": {\"NODE_NAME\": \"SUBJECT\"}},\n",
    "    \n",
    "    {\"PATTERN\": verb_tag_list,\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"SUBJECT\", \"NBOR_RELOP\": \"<\", \"NODE_NAME\": \"VERB_PREDICATE\"}},\n",
    "    \n",
    "    {\"PATTERN\": join_objs(_object, [ROOT]),\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"VERB_PREDICATE\", \"NBOR_RELOP\": \"<\", \"NODE_NAME\": \"OBJECT\"}},\n",
    "    \n",
    "    {\"PATTERN\": naming_predicate_list,\n",
    "        \"SPEC\": {\"NBOR_NAME\": \"OBJECT\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"PREDICATE\"}},\n",
    "]})\n",
    "\n",
    "texts = [\n",
    "    \"passengers like an exceptional man named Todd Beamer\",\n",
    "    \"a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "#     \"leaving the main enemy in the region, namely the Jewish-American alliance\"\n",
    "]\n",
    "\n",
    "components = [\"Clause_Labelling\"]\n",
    "for component in components:\n",
    "    if component in nlp.pipe_names:\n",
    "        nlp.remove_pipe(component)\n",
    "        \n",
    "nlp.add_pipe(Clause_Labelling(nlp, patterns.values(), patterns.keys()))\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    options = {\n",
    "#         \"fine_grained\" : True,\n",
    "        \"add_lemma\" : True,\n",
    "        \"distance\" : 900/len(doc),\n",
    "        \"word_spacing\" : 40,\n",
    "        \"collapse_punct\": True}\n",
    "\n",
    "    displacy.render(doc_dep_graph(doc, strip = True), style = \"dep\", manual = True, options = options)\n",
    "    doc._.explain()\n",
    "#     displacy.render(doc, options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels = {\n",
    "    \"Social\" : {\"neutral\": {\"Person\": [\"man\"],\n",
    "                            \"Community\": [\"organization\", \"nomad\"],\n",
    "                            \"Entity\": [\"thing\"]},\n",
    "                \"ingroup\": {\"Family\": [\"brothers\"],\n",
    "                            \"Ally\": [\"friend\"]},\n",
    "                \"elevation\": {\"Right\": [\"truthful\"],\n",
    "                              \"Good\": [\"exceptional\", \"good\"]},\n",
    "                \"othering\": {\"Wrong\": [\"unfair\", \"hoax\"]}},\n",
    "    \"Religious\": {\"neutral\": {\"ReligiousGroup\" : [\"ulema\"]}},\n",
    "    \"Commercial\": {\"neutral\": {\"Customer\": [\"passenger\"]}},\n",
    "    \"Health\" : {\"neutral\": {\"Entity\": [\"vaccine\"]}, \n",
    "                \"outgroup\" : {\"Vermin\": [\"parasite\"]}},\n",
    "    \"Security\": {\"outgroup\": {\"Criminal\": [\"terrorist\"]}},\n",
    "    \"Political\": {\"neutral\": {\"Group\": [\"authorities\", \"alliance\"]}},\n",
    "    \"Military\": {\"outgroup\": {\"Adversary\": [\"enemy\"]}}\n",
    "}\n",
    "\n",
    "def get_concept(token):\n",
    "\n",
    "    \"\"\"\n",
    "    token extension for getting labels from group schema\n",
    "    \"\"\"\n",
    "    for context, attributes in group_labels.items():\n",
    "        for attribute, concepts in attributes.items():\n",
    "            for concept, terms in concepts.items():\n",
    "                if token.lower() in terms:\n",
    "                    return context, attribute, concept\n",
    "    return \"fuckface\"\n",
    "\n",
    "print(get_concept(\"terrorist\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {}\n",
    "\n",
    "patterns.update({\"hasMod\": [\n",
    "    \n",
    "    {\"PATTERN\": {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}},\n",
    "     \"SPEC\": {\"NODE_NAME\": \"SUBJECT\"}},\n",
    "    \n",
    "    {\"PATTERN\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"SUBJECT\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"OBJECT\"}}\n",
    "    \n",
    "]})\n",
    "\n",
    "patterns.update({\"hasNameOf\": [  # object naming object\n",
    "\n",
    "    # OBJECT known as / named SUBJECT\n",
    "\n",
    "    {\"PATTERN\": join_objs(_subject, [preposition_deps]),\n",
    "     \"SPEC\": {\"NODE_NAME\": \"SUBJECT\"}},\n",
    "\n",
    "    {\"PATTERN\": naming_predicate_list,\n",
    "        \"SPEC\": {\"NBOR_NAME\": \"SUBJECT\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"PREDICATE\"}},\n",
    "\n",
    "    {\"PATTERN\": join_objs(_object, [_subject, preposition_deps]),\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"PREDICATE\", \"NBOR_RELOP\": \">>\", \"NODE_NAME\": \"OBJECT\"}}\n",
    "]})\n",
    "\n",
    "patterns.update({\"hasNamely\": [  # object naming object\n",
    "\n",
    "    # VERB the OBJECT, namely the SUBJECT\n",
    "\n",
    "    {\"PATTERN\": _object,\n",
    "     \"SPEC\": {\"NODE_NAME\": \"SUBJECT\"}},\n",
    "    \n",
    "    {\"PATTERN\": verb_tag_list,\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"SUBJECT\", \"NBOR_RELOP\": \"<\", \"NODE_NAME\": \"VERB_PREDICATE\"}},\n",
    "    \n",
    "    {\"PATTERN\": join_objs(_object, [ROOT]),\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"VERB_PREDICATE\", \"NBOR_RELOP\": \"<\", \"NODE_NAME\": \"OBJECT\"}},\n",
    "    \n",
    "    {\"PATTERN\": naming_predicate_list,\n",
    "        \"SPEC\": {\"NBOR_NAME\": \"OBJECT\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"PREDICATE\"}},\n",
    "]})\n",
    "\n",
    "hyponymy_list = {\"LEMMA\": {\"IN\": [\"like\", \"include\", \"except\", \"whether\", \"as\"]}}\n",
    "patterns.update({\"isHyp\": [  # Hypernym prep Hyponym\n",
    "    \n",
    "    {\"PATTERN\": join_objs(_subject, [ROOT]),\n",
    "     \"SPEC\": {\"NODE_NAME\": \"SUBJECT\"}},\n",
    "\n",
    "    {\"PATTERN\": hyponymy_list,\n",
    "        \"SPEC\": {\"NBOR_NAME\": \"SUBJECT\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"PREDICATE\"}},\n",
    "\n",
    "    {\"PATTERN\": join_objs(_object, [preposition_deps]),\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"PREDICATE\", \"NBOR_RELOP\": \">>\", \"NODE_NAME\": \"OBJECT\"}}\n",
    "]})\n",
    "\n",
    "patterns.update({\"verbPredicate\": [  # Hypernym verb Hyponym\n",
    "\n",
    "    # who attacked our country\n",
    "    {\"PATTERN\": _subject,\n",
    "     \"SPEC\": {\"NODE_NAME\": \"OBJECT\"}},\n",
    "\n",
    "    {\"PATTERN\": verb_tag_list,\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"OBJECT\", \"NBOR_RELOP\": \"<\", \"NODE_NAME\": \"PREDICATE\"}},\n",
    "\n",
    "    {\"PATTERN\": {\"DEP\": {\"IN\": [\"neg\"]}},\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"PREDICATE\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"NEGATION\"}},\n",
    "    \n",
    "    {\"PATTERN\": {\"DEP\": {\"IN\": [\"cc\"]}},\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"PREDICATE\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"CORDINATING_CONJUNCTION\"}},\n",
    "\n",
    "    {\"PATTERN\": join_objs(_object, [complements_deps]),\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"PREDICATE\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"NEGATED\"}},\n",
    "    \n",
    "    {\"PATTERN\": {\"DEP\": {\"IN\": [\"conj\"]}},\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"PREDICATE\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"SUBJECT\"}}\n",
    "]})\n",
    "\n",
    "patterns.update({\"verbprepPredicate\": [  # Hypernym verb (prep) Hyponym\n",
    "\n",
    "    # war begins with al Qaeda\n",
    "\n",
    "    {\"PATTERN\": _subject,\n",
    "     \"SPEC\": {\"NODE_NAME\": \"OBJECT\"}},\n",
    "\n",
    "    {\"PATTERN\": verb_tag_list,\n",
    "        \"SPEC\": {\"NBOR_NAME\": \"OBJECT\", \"NBOR_RELOP\": \"<\", \"NODE_NAME\": \"PREDICATE\"}},\n",
    "\n",
    "    {\"PATTERN\": {\n",
    "        \"DEP\": {\"IN\": [\"prep\"]}}, # TODO: this needs to be fixed to be an adjacent token\n",
    "        \"SPEC\": {\"NBOR_NAME\": \"PREDICATE\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"PREP\"}},\n",
    "\n",
    "    {\"PATTERN\": join_objs(_object, [complements_deps, preposition_deps]),\n",
    "     \"SPEC\": {\"NBOR_NAME\": \"PREP\", \"NBOR_RELOP\": \">\", \"NODE_NAME\": \"SUBJECT\"}}\n",
    "]})\n",
    "\n",
    "\n",
    "components = [\"Clause_Labelling\"]\n",
    "for component in components:\n",
    "    if component in nlp.pipe_names:\n",
    "        nlp.remove_pipe(component)\n",
    "        \n",
    "nlp.add_pipe(Clause_Labelling(nlp, patterns.values(), patterns.keys()))\n",
    "    \n",
    "texts = [\n",
    "    \"passengers like an exceptional man named Todd Beamer\",\n",
    "    \"The Aryan himself was probably at first a Nomad\",\n",
    "    \"The Jew has never been a Nomad, but always a parasite\",\n",
    "    \"Lockdowns are killing countries all over the world\",\n",
    "    \"vaccines have never been a good thing, but always a hoax\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    options = {\n",
    "#         \"fine_grained\" : True,\n",
    "        \"add_lemma\" : True,\n",
    "        \"distance\" : 1000/len(doc),\n",
    "        \"word_spacing\" : 40,\n",
    "        \"collapse_punct\": True}\n",
    "\n",
    "    displacy.render(doc_dep_graph(doc, strip = True), style = \"dep\", manual = True, options = options)\n",
    "    doc._.explain()\n",
    "#     displacy.render(doc, options = options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy_v3",
   "language": "python",
   "name": "spacy_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
