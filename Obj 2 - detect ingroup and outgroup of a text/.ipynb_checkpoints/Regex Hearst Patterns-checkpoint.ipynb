{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Hearst Patterns\n",
    "---\n",
    "In this experiment we test the utility of Hearst Patterns for detecting the ingroup and outgroup of a text.\n",
    "\n",
    "For this experiment regex is used with code taken from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/hearstPatterns.py\n",
    "\n",
    "Hypernym relations are semantic relationships between two concepts: C1 is a hypernym of C2 means that C1 categorizes C2 (e.g. “instrument” is a hypernym of “Piano”). For this research, the phrase, \"America has enemies, such as Al Qaeda and the Taliban\" would return the following '[('Al Qaeda', 'enemy'), ('the Taliban', 'enemy')]'. In this example, the categorising term 'enemy' is a hypernym of both 'Al Qaeda' and the 'Taliban'; conversely 'al Qaeda' and 'the Tabliban' are hyponyms of 'enemy'. Using this technique, hypernym terms could be classified as ingroup or outgroup and named entities identified as hyponym terms could be identified as either group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment has not produced any results from the bin Laden text, but has produced some promising results from the Bush text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_the_evidence -PRON- have gather NP_all_point to NP_a_collection of NP_loosely_affiliate_terrorist_organization know as NP_al_Qaeda .\n",
      "[('al Qaeda', 'loosely affiliate terrorist organization')]\n",
      "NP_terrorist_group like NP_al_Qaeda depend upon NP_the_aid or NP_indifference of NP_government .\n",
      "[('al Qaeda', 'terrorist group')]\n",
      "other NP_close_friend , include NP_Canada , NP_Australia , NP_Germany and NP_France , have pledge NP_force as NP_the_operation unfold .\n",
      "[('Canada', 'close friend'), ('Australia', 'close friend'), ('Germany', 'close friend'), ('France', 'close friend'), ('force', 'the operation')]\n"
     ]
    }
   ],
   "source": [
    "h = HearstPatterns(extended=True, merge = False)\n",
    "\n",
    "true_positives = [\n",
    "    \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "    \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\",\n",
    "    \"Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\",\n",
    "]\n",
    "             \n",
    "for sentence in true_positives:\n",
    "    print(h.find_hyponyms(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are some false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "NP_terrorist , include NP_e__mail , NP_the_internet , and NP_cell_phone \n",
      "[('e  mail', 'terrorist'), ('the internet', 'terrorist'), ('cell phone', 'terrorist')]\n",
      "-----\n",
      "NP_the_United_States as NP_a_hostile_regime\n",
      "[('the United States', 'a hostile regime')]\n"
     ]
    }
   ],
   "source": [
    "false_positives = [\n",
    "    \"This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\",\n",
    "    \"From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\"\n",
    "]\n",
    "\n",
    "for sentence in false_positives:\n",
    "    print(h.find_hyponyms(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "the following code is taken from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/test/test_hearstPatterns.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from spacy.pipeline import merge_noun_chunks\n",
    "from spacy.pipeline import merge_entities\n",
    "\n",
    "\n",
    "class HearstPatterns(object):\n",
    "\n",
    "    def __init__(self, extended=False, merge = False):\n",
    "\n",
    "        self.__adj_stopwords = [\n",
    "            'able', 'available', 'brief', 'certain',\n",
    "            'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',\n",
    "            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "            'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "            'same', 'several', 'significant', 'similar', 'such', 'sup', 'sure'\n",
    "        ]\n",
    "\n",
    "        # now define the Hearst patterns\n",
    "        # format is <hearst-pattern>, <general-term>\n",
    "        # so, what this means is that if you apply the first pattern,\n",
    "        # the first Noun Phrase (NP)\n",
    "        # is the general one, and the rest are specific NPs\n",
    "        self.__hearst_patterns = [\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?know as (NP_\\\\w+ ?(, )?(and |or )?)+)', # added for this experiment\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "                'last'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if extended:\n",
    "            self.__hearst_patterns.extend([\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?for example (, )?'\n",
    "                    '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+'\n",
    "                    '(\\\\. )?\\\\))',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "                    'for instance)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "                    '?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                )\n",
    "            ])\n",
    "\n",
    "        self.__spacy_nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        if merge:\n",
    "            self.__spacy_nlp.add_pipe(nlp.create_pipe(\"merge_entities\"), after = \"ner\")\n",
    "            #self.__spacy_nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"), last = True)\n",
    "            \n",
    "\n",
    "    def chunk(self, rawtext):\n",
    "        doc = self.__spacy_nlp(rawtext)\n",
    "        chunks = []\n",
    "        for sentence in doc.sents:\n",
    "            \n",
    "            sentence_text = sentence.lemma_   ## lemmatise sentence\n",
    "            \n",
    "            ## iterate through the sentence noun chunks\n",
    "            for chunk in sentence.noun_chunks: \n",
    "                \n",
    "                # if the chunk is 'for example' or 'example of' move to next chunk\n",
    "                if chunk.lemma_.lower() == \"example\":   \n",
    "                    start = chunk.start\n",
    "                    pre_token = sentence[start - 1].lemma_.lower()\n",
    "                    post_token = sentence[start + 1].lemma_.lower()\n",
    "                    if start > 0 and\\\n",
    "                            (pre_token == \"for\" or post_token == \"of\"):\n",
    "                        continue\n",
    "                \n",
    "                # if the chunk is 'type' move to the next chunk\n",
    "                if chunk.lemma_.lower() == \"type\": \n",
    "                    continue\n",
    "                \n",
    "                chunk_arr = []\n",
    "                replace_arr = []\n",
    "                # print(\"chunk:\", chunk)\n",
    "                \n",
    "                ## iterate through the tokens in the chunk\n",
    "                for token in chunk:\n",
    "                    \n",
    "                    ## if the token is a stopword or 'i.e.' or 'e.g.' move to next token\n",
    "                    if token.lemma_ in self.__adj_stopwords + [\"i.e.\", \"e.g.\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    # append token lemma to the chunk array\n",
    "                    chunk_arr.append(token.lemma_)\n",
    "                    \n",
    "                    # Remove punctuation and stopword adjectives\n",
    "                    # (generally quantifiers of plurals)\n",
    "                    if token.lemma_.isalnum():\n",
    "                        replace_arr.append(token.lemma_)\n",
    "                    else:\n",
    "                        replace_arr.append(''.join(\n",
    "                            char for char in token.lemma_ if char.isalnum()\n",
    "                        ))\n",
    "                if len(chunk_arr) == 0:\n",
    "                    chunk_arr.append(chunk[-1].lemma_)\n",
    "                chunk_lemma = ' '.join(chunk_arr)\n",
    "                # print(chunk_lemma)\n",
    "                replacement_value = 'NP_' + '_'.join(replace_arr)\n",
    "                if chunk_lemma:\n",
    "                    sentence_text = re.sub(r'\\b%s\\b' % re.escape(chunk_lemma),\n",
    "                                           r'%s' % replacement_value,\n",
    "                                           sentence_text)\n",
    "            print(sentence_text)\n",
    "            chunks.append(sentence_text)    \n",
    "        return chunks\n",
    "\n",
    "    \"\"\"\n",
    "        This is the main entry point for this code.\n",
    "        It takes as input the rawtext to process and returns a list\n",
    "        of tuples (specific-term, general-term)\n",
    "        where each tuple represents a hypernym pair.\n",
    "    \"\"\"\n",
    "    def find_hyponyms(self, rawtext):\n",
    "\n",
    "        hyponyms = []\n",
    "        np_tagged_sentences = self.chunk(rawtext)\n",
    "\n",
    "        for sentence in np_tagged_sentences:\n",
    "            # two or more NPs next to each other should be merged\n",
    "            # into a single NP, it's a chunk error\n",
    "\n",
    "            for (hearst_pattern, parser) in self.__hearst_patterns:\n",
    "                matches = re.search(hearst_pattern, sentence)\n",
    "                if matches:\n",
    "                    match_str = matches.group(0)\n",
    "\n",
    "                    nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "                    \n",
    "                    if parser == \"first\":\n",
    "                        general = nps[0]\n",
    "                        specifics = nps[1:]\n",
    "                    else:\n",
    "                        general = nps[-1]\n",
    "                        specifics = nps[:-1]\n",
    "\n",
    "                    for i in range(len(specifics)):\n",
    "                        pair = (\n",
    "                            self.clean_hyponym_term(specifics[i]),\n",
    "                            self.clean_hyponym_term(general)\n",
    "                        )\n",
    "                        # reduce duplicates\n",
    "                        if pair not in hyponyms:\n",
    "                            hyponyms.append(pair)\n",
    "\n",
    "        return hyponyms\n",
    "\n",
    "    def clean_hyponym_term(self, term):\n",
    "        # good point to do the stemming or lemmatization\n",
    "        return term.replace(\"NP_\", \"\").replace(\"_\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_forty__four_percent of NP_patient with NP_uveitis have one or more identifiable sign or NP_symptom , such as NP_red_eye , NP_ocular_pain , NP_visual_acuity , or NP_photophobia , in NP_order of NP_decrease_frequency .\n",
      "[('red eye', 'symptom'), ('ocular pain', 'symptom'), ('visual acuity', 'symptom'), ('photophobia', 'symptom')]\n"
     ]
    }
   ],
   "source": [
    "h = HearstPatterns()\n",
    "doc = \"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\"\n",
    "print(h.find_hyponyms(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bear witness that there is no God except Allah, no associates has He, and I bear witness that Muhammad is His slave and messenger.\n",
      "\n",
      "[('Allah', 'no God'), ('no associate', 'no God')]\n",
      "==========\n",
      "The Crusader forces became the main cause of our disastrous condition, particularly in the economical aspect of it, due to the unjustified heavy spending on these forces, and as a result of the policies imposed on the country, especially in the field of oil industry, where production is restricted or expanded and prices are fixed to suit the American economy, ignoring the economy of the country.\n",
      "[('these force', 'a result')]\n",
      "==========\n",
      "The intimidation and harassment suffered by the leaders of the society, the scholars, heads of tribes, merchants, academic teachers and other eminent individuals;\n",
      "2.\n",
      "[('tribe', 'eminent individual'), ('merchant', 'eminent individual'), ('academic teacher', 'eminent individual')]\n",
      "==========\n",
      "They refused to be played against each others and to be used by the regime as a tool to carry out the policy of the American/Israeli alliance through their agent in our country: the Saudi regime.\n",
      "[('the regime', 'a tool')]\n",
      "==========\n",
      "No other priority, except Belief, could be considered before it; the people of knowledge, as Ibn Taymiyyah said, stated: \"To fight in defence of religion and Belief is a collective duty; there is no other duty after Belief than fighting the enemy who is corrupting the life and the religion.\n",
      "[('knowledge', 'Ibn Taymiyyah')]\n",
      "==========\n",
      "Humanly fabricated laws have been put forward permitting what has been forbidden by Allah such as usury (Riba) and other matters.\n",
      "[('usury', 'Allah')]\n",
      "==========\n",
      "Especially when the civil and the military infrastructures of Iraq were savagely destroyed, showing the depth of the Zionist/Crusaders hatred to the Muslims and their children, and also since the rejection of the idea of replacing the Crusaders forces by an Islamic force composed of the sons of the country and other Muslim people.\n",
      "[('the country', 'muslim people')]\n",
      "==========\n",
      "We expect the women of the land of the two Holy Places and other countries to carry out their role in boycotting American goods.\n",
      "[('the two Holy Places', 'country')]\n",
      "==========\n",
      "But your most disgraceful case was in Somalia; where, after vigorous propaganda about the power of the USA and its post-cold war leadership of the new world order, you moved tens of thousands of international forces, including twenty-eight thousand American solders, into Somalia.\n",
      "[('twenty  eight thousand american solder', 'international force')]\n",
      "==========\n",
      "Its appropriate remedy ,however, is in the hands of the youth of Islam, as the poet said:\n",
      "\"I am willing to sacrifice self and wealth for knights who never disappointed me.\n",
      "\n",
      "[('Islam', 'the poet')]\n",
      "==========\n",
      "It is a legitimate right well known to all humans and other creatures.\n",
      "[('all human', 'creature')]\n",
      "==========\n",
      "More than 600,000 Iraqi children have died due to lack of food and medicine and as a result of the unjustifiable aggression (sanctions) imposed on Iraq and its people.\n",
      "\n",
      "[('medicine', 'a result')]\n",
      "==========\n",
      "Our youth know that the humiliation suffered by the Muslims as a result of the occupation of their sanctuaries cannot be obliterated and removed except by explosions and Jihad.\n",
      "[('the Muslims', 'a result')]\n",
      "==========\n",
      "\"Do not moan on any one except a lion in the woods, courageous in the burning wars.\n",
      "\n",
      "[('a lion', 'any one')]\n",
      "==========\n",
      "Our Lord, do not lay on us a burden as Thou didst lay on those before us; Our Lord, do not impose upon us that which we have no strength to bear; and pardon us and grant us protection and have mercy on us, Thou art our patron\n",
      "[('a burden', 'Thou didst')]\n",
      "==========\n",
      "What the United States tastes today is a very small thing compared to what we have tasted for tens of years.\n",
      "\n",
      "[('what', 'a very small thing')]\n",
      "==========\n",
      "Israeli tanks and tracked vehicles also enter to wreak havoc in Palestine, in Jenin, Ramallah, Rafah, Beit Jala, and other Islamic areas\n",
      "[('Jenin', 'islamic area'), ('Ramallah', 'islamic area'), ('Rafah', 'islamic area'), ('Beit Jala', 'islamic area')]\n",
      "==========\n",
      "When those have stood in defense of their weak children, their brothers and sisters in Palestine and other Muslim nations, the whole world went into an uproar, the infidels followed by the hypocrites.\n",
      "\n",
      "[('Palestine', 'muslim nation')]\n",
      "==========\n",
      "Those who believe, fight in the Cause of Allah, and those who disbelieve, fight in the cause of Taghut (anything worshipped other than Allah e.g. Satan).\n",
      "[('Satan', 'Allah')]\n",
      "==========\n",
      "Muslims believe in all of the Prophets, including Abraham, Moses, Jesus and Muhammad, peace and blessings of Allah be upon them all.\n",
      "[('Abraham', 'the prophet'), ('Moses', 'the prophet'), ('Jesus', 'the prophet'), ('Muhammad', 'the prophet'), ('peace', 'the prophet'), ('blessing', 'the prophet')]\n",
      "==========\n",
      "Allah has challenged anyone to bring a book like the Quran or even ten verses like it.\n",
      "\n",
      "[('the Quran', 'a book'), ('even ten verse', 'a book')]\n",
      "==========\n",
      "You are a nation that exploits women like consumer products or advertising tools calling upon customers to purchase them.\n",
      "[('consumer product', 'woman'), ('advertising tool', 'woman')]\n",
      "==========\n",
      "Giant corporations and establishments are established on this, under the name of art, entertainment, tourism and freedom, and other deceptive names you attribute to it.\n",
      "\n",
      "[('freedom', 'deceptive name')]\n",
      "==========\n",
      "And because of all this, you have been described in history as a nation that spreads diseases that were unknown to man in the past.\n",
      "[('history', 'a nation')]\n",
      "==========\n",
      "Go ahead and boast to the nations of man, that you brought them AIDS as a Satanic American Invention.\n",
      "\n",
      "[('AIDS', 'a Satanic american invention')]\n",
      "==========\n",
      "This is our message to the Americans, as an answer to theirs.\n",
      "[('the Americans', 'an answer')]\n",
      "==========\n",
      "This means the oppressing and embargoing to death of millions as Bush Sr did in Iraq in the greatest mass slaughter of children mankind has ever known, and it means the throwing of millions of pounds of bombs and explosives at millions of children - also in Iraq - as Bush Jr did, in order to remove an old agent and replace him with a new puppet to assist in the pilfering of Iraq's oil and other outrages.\n",
      "\n",
      "[('million', 'Bush Sr')]\n",
      "==========\n",
      "In addition, Bush sanctioned the installing of sons as state governors, and didn't forget to import expertise in election fraud from the region's presidents to Florida to be made use of in moments of difficulty.\n",
      "\n",
      "[('son', 'state governor')]\n",
      "==========\n",
      "All that we have to do is to send two mujahidin to the furthest point east to raise a piece of cloth on which is written al-Qaida, in order to make the generals race there to cause America to suffer human, economic, and political losses without their achieving for it anything of note other than some benefits for their private companies.\n",
      "\n",
      "[('some benefit', 'note')]\n",
      "==========\n",
      "It is true that this shows that al-Qaida has gained, but on the other hand, it shows that the Bush administration has also gained, something of which anyone who looks at the size of the contracts acquired by the shady Bush administration-linked mega-corporations, like Halliburton and its kind, will be convinced.\n",
      "[('Halliburton', 'the shady Bush administration  link mega  corporation')]\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentences = []\n",
    "\n",
    "h = HearstPatterns(extended=True, merge = False)\n",
    "\n",
    "\n",
    "# filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Experiment 7 - Dependency Matcher/\"\n",
    "# filename = \"bush_ingroup_sents.json\"\n",
    "\n",
    "# with open(os.path.join(filepath, filename), 'r') as fp:\n",
    "#     sentences = json.load(fp)\n",
    "\n",
    "# filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Speeches/George Bush/\"\n",
    "# filename = \"bush_complete.txt\"\n",
    "\n",
    "filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Speeches/Osama bin Laden/\"\n",
    "filename = \"binladen_complete.txt\"\n",
    "\n",
    "file = os.path.join(filepath, filename)\n",
    "\n",
    "_, file_extension = os.path.splitext(file)\n",
    "\n",
    "if file_extension == \".json\":\n",
    "    with open(file, 'r') as fp:\n",
    "        sentences = json.load(fp)\n",
    "        \n",
    "elif file_extension == \".txt\":\n",
    "    with open(file, 'r') as text:\n",
    "        sentences = {key:value.text for (key,value) in enumerate(nlp(text.read()).sents)}\n",
    "        \n",
    "else:\n",
    "    raise SystemExit(\"file not found!\")\n",
    "    \n",
    "    \n",
    "found = False\n",
    "    \n",
    "for sent in sentences.values():\n",
    "    hyponyms = h.find_hyponyms(sent)\n",
    "    if hyponyms:\n",
    "        print(sent)\n",
    "        print(hyponyms)\n",
    "        print('==========')\n",
    "        found = True\n",
    "\n",
    "if found == False:\n",
    "    print('none found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_hyponym_finder (__main__.TestHearstPatterns)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-e0484314abfb>\", line 40, in test_hyponym_finder\n",
      "    self.assertEqual(tuple(map(str.lower, hyps6[0])), (\"postharvest loss reduction\", \"benefit\"))\n",
      "AssertionError: Tuples differ: ('postharv loss reduction', 'benefit') != ('postharvest loss reduction', 'benefit')\n",
      "\n",
      "First differing element 0:\n",
      "'postharv loss reduction'\n",
      "'postharvest loss reduction'\n",
      "\n",
      "- ('postharv loss reduction', 'benefit')\n",
      "+ ('postharvest loss reduction', 'benefit')\n",
      "?           +++\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.943s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "class TestHearstPatterns(unittest.TestCase):\n",
    "\n",
    "    def test_hyponym_finder(self):\n",
    "        h = HearstPatterns(extended=True)\n",
    "\n",
    "        # H1\n",
    "        hyps1 = h.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")\n",
    "\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[0])), (\"red eye\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[1])), (\"ocular pain\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[2])), (\"visual acuity\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[3])), (\"photophobia\", \"symptom\"))\n",
    "\n",
    "        # H2\n",
    "        hyps2 = h.find_hyponyms(\"There are works by such authors as Herrick, Goldsmith, and Shakespeare.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[0])), (\"herrick\", \"author\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[1])), (\"goldsmith\", \"author\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[2])), (\"shakespeare\", \"author\"))\n",
    "\n",
    "        # H3\n",
    "        hyps3 = h.find_hyponyms(\"There were bruises, lacerations, or other injuries were not prevalent.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps3[0])), (\"bruise\", \"injury\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps3[1])), (\"laceration\", \"injury\"))\n",
    "\n",
    "        # H4\n",
    "        hyps4 = h.find_hyponyms(\"common law countries, including Canada, Australia, and England enjoy toast.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[0])), (\"canada\", \"common law country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[1])), (\"australia\", \"common law country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[2])), (\"england\", \"common law country\"))\n",
    "\n",
    "        # H5\n",
    "        hyps5 = h.find_hyponyms(\"Many countries, especially France, England and Spain also enjoy toast.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[0])), (\"france\", \"country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[1])), (\"england\", \"country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[2])), (\"spain\", \"country\"))\n",
    "\n",
    "        # H2\n",
    "        hyps6 = h.find_hyponyms(\"There are such benefits as postharvest losses reduction, food increase and soil fertility improvement.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[0])), (\"postharvest loss reduction\", \"benefit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[1])), (\"food increase\", \"benefit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[2])), (\"soil fertility improvement\", \"benefit\"))\n",
    "\n",
    "        # H'1\n",
    "        hyps7 = h.find_hyponyms(\"Fruits, i.e. , apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        hyps7 = h.find_hyponyms(\"Fruits, e.g. apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'2\n",
    "\n",
    "        hyps10 = h.find_hyponyms(\"Fruits (e.g. apples, bananas, oranges and peaches.)\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        hyps10 = h.find_hyponyms(\"Fruits (i.e. apples, bananas, oranges and peaches.)\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'3\n",
    "        hyps8 = h.find_hyponyms(\"Fruits, for example apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'4\n",
    "        hyps9 = h.find_hyponyms(\"Fruits, which may include apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from hearstPatterns.hearstPatterns import HearstPatterns\n",
    "\n",
    "class TestHearstPatterns(unittest.TestCase):\n",
    "\n",
    "    def test_hyponym_finder(self):\n",
    "        h = HearstPatterns()\n",
    "        hyps1 =  h.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")\n",
    "\n",
    "        self.assertEqual(hyps1[0], (\"red eye\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[1], (\"ocular pain\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[2], (\"visual acuity\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[3], (\"photophobia\", \"symptom\"))\n",
    "\n",
    "        hyps2 = h.find_hyponyms(\"There are works by such authors as Herrick, Goldsmith, and Shakespeare.\")\n",
    "        self.assertEqual(hyps2[0], (\"herrick\", \"author\"))\n",
    "        self.assertEqual(hyps2[1], (\"goldsmith\", \"author\"))\n",
    "        self.assertEqual(hyps2[2], (\"shakespeare\", \"author\"))\n",
    "\n",
    "        hyps3 = h.find_hyponyms(\"There were bruises, lacerations, or other injuries were not prevalent.\")\n",
    "        self.assertEqual(hyps3[0], (\"bruise\", \"injury\"))\n",
    "        self.assertEqual(hyps3[1], (\"laceration\", \"injury\"))\n",
    "\n",
    "        hyps4 =  h.find_hyponyms(\"common law countries, including Canada, Australia, and England enjoy toast.\")\n",
    "        self.assertEqual(hyps4[0], (\"canada\", \"common law country\"))\n",
    "        self.assertEqual(hyps4[1], (\"australia\", \"common law country\"))\n",
    "        self.assertEqual(hyps4[2], (\"england\", \"common law country\"))\n",
    "\n",
    "        hyps5 = h.find_hyponyms(\"Many countries, especially France, England and Spain also enjoy toast.\")\n",
    "        self.assertEqual(hyps5[0], (\"france\", \"country\"))\n",
    "        self.assertEqual(hyps5[1], (\"england\", \"country\"))\n",
    "        self.assertEqual(hyps5[2], (\"spain\", \"country\"))\n",
    "\n",
    "        hyps6 = h.find_hyponyms(\"There are such benefits as postharvest losses reduction, food increase and soil fertility improvement.\")\n",
    "        self.assertEqual(hyps6[0], (\"postharvest loss reduction\", \"benefit\"))\n",
    "        self.assertEqual(hyps6[1], (\"food increase\", \"benefit\"))\n",
    "        self.assertEqual(hyps6[2], (\"soil fertility improvement\", \"benefit\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
