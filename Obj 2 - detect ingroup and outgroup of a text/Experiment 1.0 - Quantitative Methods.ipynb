{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting the Ingroup and Outgroup of a Text\n",
    "---\n",
    "\n",
    "In this first experiment we use Bush (2001) and bin Laden’s (19XX) narratives as a dataset to detect the ingroup and outgroup of each. Shown in Figure 1, a manual annotation of two text reveals the ingroups and outgroups for each orator. Bush’s text is his “Address to Joint Session of Congress Following 9/11 Attacks ” made on the 20 September 2001; bin Laden’s text is his “Declaration of Jihad Against the Americans Occupying the Land of the Two Holy Places” published on 23 August 1996. These texts were chosen as they are the first of the dataset in which each declares war against each other. Bush identifies his ingroup as, ‘fellow Americans’, and after identifying al Qaeda as an terrorist organisation, he declares, “Our war on terror begins with al Qaeda”. Bin Laden similarly identifies his ingroup as “Muslim brethren” and declares Jihad against his outgroup of the Americans by stating, “driving back the American occupier enemy is the most essential duty after faith”. The ingroup and outgroup of each text are clearly defined by each orator, as is their intention to legitimise warfare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup NLP Pipeline and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from cndlib.visuals import display_side_by_side\n",
    "from cndlib.pipeline import add_hard_coded_entities, merge_compounds, custom_tokenizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "dirname = \"C:\\\\Users\\\\spa1e17\\\\OneDrive - University of Southampton\\\\Hostile-Narrative-Analysis\\\\dataset\"\n",
    "filename = \"named_entity_corrections.json\"\n",
    "filepath = os.path.join(dirname, filename)\n",
    "\n",
    "add_hard_coded_entities(nlp, filepath)\n",
    "\n",
    "merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(merge_ents, after = \"entity_ruler\")\n",
    "\n",
    "nlp.add_pipe(merge_compounds, last = True)\n",
    "\n",
    "print([pipe for pipe in nlp.pipe_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset\n",
    "\n",
    "The dataset is created by extracting the named entities relating to people or groups, which in turn are annotated in relation to each orator as either ingroup or outgroup. The annotations were made by words signifying group membership, such as \"terrorist organisation known as al Qaeda\", or by inference using annotator judgement. The annotations are saved in a .csv file and can be reviewed as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "docs = None\n",
    "docs = {\"bush\" : {\"name\" : \"George Bush\", \"text\" : dict()},\n",
    "       \"binladen\" : {\"name\" : \"Osama bin Laden\", \"text\" : dict()}}\n",
    "\n",
    "docs['bush']['dirpath'] = u\"C:\\\\Users\\\\spa1e17\\\\OneDrive - University of Southampton\\\\Hostile-Narrative-Analysis\\\\dataset\\\\George Bush\"\n",
    "docs['bush']['text']['filename'] = \"20010920-Address to Joint Session of Congress Following 911 Attacks.txt\"\n",
    "\n",
    "docs['binladen']['dirpath'] = u\"C:\\\\Users\\\\spa1e17\\\\OneDrive - University of Southampton\\\\Hostile-Narrative-Analysis\\\\dataset\\\\Osama bin Laden\"\n",
    "docs['binladen']['text']['filename'] = \"19960823-Declaration of Jihad Against the Americans Occupying the Land of the Two Holiest Sites.txt\"\n",
    "                                                \n",
    "# lambda function to capture the named entities of a text which are GEP, NORP, ORG or PERSON\n",
    "entity_list = lambda ents: [ent for ent in ents.noun_chunks \n",
    "                            if all((ent.root.pos_ != \"ADJ\",\n",
    "                                   ent.root.ent_type_ in [\"PERSON\", \"ORG\", \"NORP\",\"GPE\"]))\n",
    "                            ]\n",
    "\n",
    "# lambda function to process doc and extract entities\n",
    "get_entities = lambda orator : entity_list(nlp(orator[\"text\"][\"rawtext\"]))\n",
    "\n",
    "for orator in docs.values():\n",
    "    \n",
    "    with open(os.path.join(orator['dirpath'], orator['text']['filename']), 'r') as fp:\n",
    "    \n",
    "        # get bush entities\n",
    "        orator[\"text\"][\"rawtext\"] = fp.read()\n",
    "        orator[\"text\"][\"entities\"] = get_entities(orator)\n",
    "        orator['text']['analytics'] = dict()\n",
    "\n",
    "n = 20\n",
    "captions = [f\"First {n} of {len(orator['text']['entities'])} Entities for {orator['name']}\" for orator in docs.values()]\n",
    "\n",
    "display_side_by_side([pd.DataFrame([(ent.text, ent.root.text, ent.root.ent_type_, ent.root.pos_) \n",
    "                                    for ent in docs[orator][\"text\"][\"entities\"]]).head(20) \n",
    "                                   for orator in docs], captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Ingroup to Outgroup of a Text\n",
    "\n",
    "Export the entities to an output csv file for manual annotation.\n",
    "\n",
    "Annotation was based on three methods. Firstly, if the named entity had an associated seed term of either elevation or othering it was annotated as ingroup or outgroup respectively. For example, any seed term associated with the term “enemy” would be considered to be an outgroup, while any entity preceded by the phrase “my fellow” would be annotated as an ingroup. Secondly, any named entity whose grouping was identified elsewhere would be annotated as “linked”. For example, where the single instance of an entity had been annotated as an outgroup through the term “enemy”, all other instances of the same entity would be annotated with the same label. The final method is through inferred knowledge which draws upon real-world knowledge to make the annotation. For review, the annotated dataset is available online  and the first 12 annotation results are in Figure 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Field headers for the csv\n",
    "fields = [\"Orator\", \"Entity Type\", \"Part of Speech\", \"Entity Phrase\", \"Entity Root\", \"Grouping\", \"Seed Term\", \"Sentence\"]\n",
    "entities = []\n",
    "\n",
    "for orator in docs:\n",
    "    for entity in docs[orator][\"text\"][\"entities\"]:\n",
    "        entities.append([orator, entity.root.ent_type_, entity.root.pos_, entity.text, entity.root, '', '', str(entity.sent).replace('\\n', ' ').strip()])\n",
    "\n",
    "dirpath = os.getcwd()\n",
    "filename = \"entity_list.csv\"\n",
    "filepath = os.path.join(dirpath, filename)\n",
    "\n",
    "df = pd.DataFrame(entities, columns = fields)\n",
    "df.to_csv(filepath, sep=',',index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Annotations as a Tab Deliminated File\n",
    "\n",
    "The annotation file is saved as a .txt file and imported using tab delimiations to avoid any clashes with sentence commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from cndlib.visuals import display_side_by_side\n",
    "\n",
    "filename = \"entity_list_gold.txt\"\n",
    "filename = os.path.join(os.getcwd(), filename)\n",
    "\n",
    "for orator in docs:\n",
    "    docs[orator]['text']['groups'] = {\"ingroup\" : set(), \"outgroup\" : set()}\n",
    "\n",
    "with open(filename, newline = \"\") as fp:\n",
    "    data = csv.DictReader(fp, delimiter = '\\t')       \n",
    "        \n",
    "    for row in data:\n",
    "\n",
    "        if row[\"Grouping\"].lower().strip() == \"ingroup\":\n",
    "            docs[row[\"Orator\"]]['text']['groups'][\"ingroup\"].add(row[\"Entity Root\"].lower().strip())\n",
    "\n",
    "        if row[\"Grouping\"].lower().strip() == \"outgroup\":\n",
    "            docs[row[\"Orator\"]]['text']['groups'][\"outgroup\"].add(row[\"Entity Root\"].lower().strip())\n",
    "            \n",
    "dfs = []\n",
    "captions = []\n",
    "for orator in docs.values():\n",
    "    data = orator['text']['groups']\n",
    "    \n",
    "    for grouping, group_list in data.items():\n",
    "        \n",
    "        # convert set() to list()\n",
    "        data[grouping] = list(data[grouping])\n",
    "        \n",
    "        dfs.append(pd.DataFrame(group_list, columns = [f\"{orator['name']}'s {grouping.title()}s\"]))\n",
    "        captions.append(f\"{orator['name']}'s text has {len(group_list)} annotated {grouping} terms\")\n",
    "        \n",
    "display_side_by_side(dfs, captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Testing IBM Watson Sentiment Analysis\n",
    "\n",
    "IBM's sentiment analyser has a feature to \"analyse target phrases in context of the surrounding text for focused sentiment and emotion results\"  . For the first test, therefore, the named entities shown in figure 1 for each orator were passed to the API to get the sentiment scores for each. A positive score towards a named entity should infer ingroup membership, while a negative score should infer outgroup membership. The annotated entity phrases were passed to the API as target phrases and results were assessed against the annotations from figure 1. If positive sentiment scores correlated with an ingroup annotation or negative scores correlated with outgroup, the test was a pass; the test was a fail for positive score correlating with outgroup, or negative scores correlating with ingroup.\n",
    "\n",
    "API Documentation: https://cloud.ibm.com/docs/natural-language-understanding?topic=natural-language-understanding-getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Watson API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import EmotionOptions, Features, EntitiesOptions, KeywordsOptions, SentimentOptions\n",
    "\n",
    "apikey = 'D3ptPkoLkoQNJvIav-reiA5137cr3m8Y1f-mhX1bLile'\n",
    "url = 'https://api.eu-gb.natural-language-understanding.watson.cloud.ibm.com/instances/204e6ba7-952c-41ae-99e9-fe4e8208bfde'\n",
    "\n",
    "authenticator = IAMAuthenticator(apikey)\n",
    "service = NaturalLanguageUnderstandingV1(version='2019-07-12', authenticator=authenticator)\n",
    "service.set_service_url(url)\n",
    "\n",
    "# print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Responses from Watson API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "for orator in docs.values():\n",
    "    print(f\"getting results for {orator['name']}\")\n",
    "    \n",
    "    text = orator['text']['rawtext']\n",
    "    targets = orator['text']['groups']['ingroup'] + orator['text']['groups']['outgroup']\n",
    "    \n",
    "    orator['text']['analytics'].update(service.analyze(text=text, features = Features(\n",
    "                                                        emotion = EmotionOptions(targets = targets),\n",
    "                                                        entities = EntitiesOptions(sentiment = True, emotion = True),\n",
    "                                                        sentiment = SentimentOptions(targets = targets, document = True),\n",
    "                                                        keywords = KeywordsOptions(sentiment = True, emotion = True),             \n",
    "                                                      )).get_result())\n",
    "    \n",
    "    #empty the list of entity Spans to enable saving file as a json object\n",
    "    orator['text']['entities'] = None\n",
    "    \n",
    "    print(f\"{orator['name']} complete with {len(orator['text']['analytics']['sentiment']['targets'])} target entities scored for sentiment\")\n",
    "\n",
    "\n",
    "with open(os.path.join(os.getcwd(), f\"entity_sentiment_scores.json\"), \"wb\") as f:\n",
    "    f.write(json.dumps(docs).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Document Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "docs = None\n",
    "\n",
    "with open(os.path.join(os.getcwd(), f\"entity_sentiment_scores.json\"), \"r\") as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "for orator in docs.values():\n",
    "    response = orator['text']['analytics']\n",
    "    print(f\"document sentiment {orator['name']}: {response['sentiment']['document']['label']}\")\n",
    "    print(f\"document sentiment score for {orator['name']}: {response['sentiment']['document']['score']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for Annotated Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cndlib.visuals import display_side_by_side\n",
    "\n",
    "def get_group(orator, entity):\n",
    "\n",
    "    \"\"\"\n",
    "    function to get the grouping of an entity from the orator's sentiment scores\n",
    "    \"\"\"\n",
    "    if entity in docs[orator]['text']['groups']['ingroup']:\n",
    "        return \"ingroup\"\n",
    "    if entity in docs[orator]['text']['groups']['outgroup']:\n",
    "        return \"outgroup\"\n",
    "    return \"not found\"\n",
    "\n",
    "def assessment_test(col1, col2):\n",
    "\n",
    "    \"\"\"\n",
    "    function to test whether a sentiment scores matches ingroup/outgroup\n",
    "    \"\"\"\n",
    "\n",
    "    if col1 == \"positive\" or col1 == \"neutral\" and col2 == \"ingroup\":\n",
    "        return \"pass\"\n",
    "    if col1 == \"negative\" and col2 == \"ingroup\":\n",
    "        return \"fail\"\n",
    "    if col1 == \"negative\" and col2 == \"outgroup\":\n",
    "        return \"pass\"\n",
    "    if col1 == \"positive\" or col1 == \"neutral\" and col2 == \"outgroup\":\n",
    "        return \"fail\"\n",
    "    \n",
    "# create new dataframe based on filtered columns\n",
    "scores = lambda table, labels: table[table.label.isin(labels)].sort_values(\"score\", ascending = 'negative' in labels, ignore_index = True)\n",
    "\n",
    "## iterate through the docs\n",
    "for orator in docs:\n",
    "    \n",
    "    # capture results\n",
    "    results = pd.DataFrame(docs[orator][\"text\"]['analytics'][\"sentiment\"][\"targets\"])\n",
    "    \n",
    "    ## create a dataframe for positive and negative results\n",
    "    dfs = dict()\n",
    "    dfs = {\"ingroup\" : {\"result\" : None, \"df\" : scores(results, ['neutral', 'positive'])}, \n",
    "           \"outgroup\" : {\"result\" : None, \"df\" : scores(results, ['negative'])}}\n",
    "\n",
    "    for obj in dfs.values():\n",
    "        \n",
    "        df = obj[\"df\"]\n",
    "        df.style.set_table_styles(dfstyle)\n",
    "\n",
    "        # get the grouping for each entity\n",
    "        df[\"grouping\"] = df.apply(lambda x: get_group(orator, x[\"text\"]), axis = 1)\n",
    "        \n",
    "        # test whether sentiment score matches ingroup/outgroup        \n",
    "        df[\"test result\"] = df.apply(lambda x: assessment_test(x[\"label\"], x[\"grouping\"]), axis=1)\n",
    "        \n",
    "        # get the success scores for ingroup and outgroup\n",
    "        obj[\"result\"] = format(df[\"test result\"].value_counts(normalize = True)[\"pass\"], '.0%')\n",
    "        \n",
    "        # format dataframe\n",
    "\n",
    "        df.drop('mixed', axis = 1, inplace = True)\n",
    "        df['text'] = df['text'].str.title()\n",
    "        df.rename(columns = {\"score\" : \"sentiment score\", \"text\" : \"entity text\"}, inplace = True)\n",
    "        df.columns = df.columns.str.title()\n",
    "\n",
    "\n",
    "    docs[orator]['text']['analytics']['sentiment']['dfs'] = dfs\n",
    "    \n",
    "    # display the outputs\n",
    "    display_side_by_side([output[\"df\"].head(13) for output in dfs.values()],\n",
    "                         [f\"{key.title()} scores for {docs[orator]['name']} has a True Positive Score of {obj['result']} from a total of {len(obj['df'])} Entities\"\n",
    "                         for key, obj in dfs.items()])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In relation to the annotation methods, these are somewhat counter-intuitive results. For Bush’s outgroups the phrases ‘al Qaeda’, ‘Taliban’, ‘the Taliban Regime’, ‘al Qaeda’ and ‘Islamic Movement of Uzbekistan’ are annotated as outgroups and generate negative scores between -0.56 and -0.81 as expected. The phrases, ‘the United States’, ‘America’, ‘Americans’, ‘the United States of America’ and ‘United States Authorities’, however, are annotated as ingroups, but generate negative scores between -0.31 and -0.65. These overlapping range of scores do not correlate with how a President would refer to his country in a time of national mourning. \n",
    "\n",
    "The phrases, ‘Christians’, ‘Jews’, ‘Muslims’ and ‘Arlene’ generate the most negative results for Bush despite being annotated as ingroups. Of these ‘Arlene’ is the most negative with a score of -0.87 and occurs in the sentence, ‘It was given to me by his mom, Arlene, as a proud memorial to her son’. This mention is in reference to a Police Shield given to George Bush by Arlene Howard in memorial to her son George who was killed in the attacks. The context in which ‘Arlene’ is mentioned is entirely positive. \n",
    "There are 37 annotations for entities associated with elevation or othering seed terms. Any phrase containing the word, ‘enemy’ would reasonably be scored as negative. The phrase, ‘US Enemy’, nevertheless, generates a score of +0.42, which is higher than ‘Gabriel’ at +0.38, a reference to the Angel Gabriel who bin Laden repeatedly reveres. Equally, bin Laden refers to the ‘mujahidin’ as ‘our brothers the people’ yet generates a score of -0.47, which is more negative than both ‘Americans’ and ‘Marines’ at -0.28 and -0.36 respectively. Bush’s phrase, “The enemy of America is not our many Muslim friends” establishes Muslims as an ingroup, whereas the term, ‘Muslims’ generates the second most negative score of -0.83.\n",
    "\n",
    "There is also problem with how different mentions of the same entity are linked across a narrative.  Despite being an outgroup of bin Laden, ‘Israel’ receives the third highest score of +0.44 behind a reference to President Clinton at +0.84 and ‘brother Muslims’ at +0.64. “Israel” is mentioned once in bin Laden’s text, but there are two mentions of the “Israeli-American alliance”, one mention of “Israeli-American enemy alliance” and one mention of “Israelis” in the phrase, “their Jihad against their enemies and yours, the Israelis and Americans”. Where bin Laden’s counter-intuitively generates positive scores for Israel, the phrases, ‘Jewish-Crusade Alliance”, “Jew” and “Jews” each generate negative scores as expected. Against expectations, however, the phrases, ‘Muslim’, ‘Muslims’ and ‘Ulema’ generate negative scores of +0.60, +0.62 and +0.64 respectively despite the phrase, ‘brother Muslims’ receiving the second highest score for positivity. There are 43 annotations that rely upon linking entities to specific clauses of elevation or othering, resolving these different mentions of the same entity might produce more intuitive results.\n",
    "\n",
    "In addition to linking entities, there is also a problem with linking them to specific noun phrases. In Bush’s text, “Osama bin Laden” and “Egyptian Islamic Jihad” generate neutral scores, whereas they are annotated as his outgroups. They occur in the phrase, “This group and its leader -- a person named Usama bin Laden -- are linked to many other organizations in different countries, including the Egyptian Islamic Jihad and the Islamic Movement of Uzbekistan.”. The noun phrase, “this group” refers to a mention of al Qaeda in the previous paragraph who Bush variously others as “terrorists” and “murderers”. Nevertheless, there is no obvious way to resolve, ‘this group’ to ‘al Qaeda’ to establish group status of bin Laden or the Egyptian Islamic Jihad. Their annotation relies upon real-world knowledge for which there are 57 annotations in the dataset. Given these entities are only mentioned once, real-world knowledge is the only way to identify their group status.\n",
    "\n",
    "Beyond linking seed terms to entities, there is also a problem with linking entities to functional narrative clauses. For bin Laden, the phrase, ‘Prophet’ – a reference to Muhammed – generates a negative score of -0.53 despite being a religious figure of bin Laden’s ingroup ‘America’ generating a less negative score. 12 out of 28 mentions of ‘Prophet’ are followed by the phrase, ‘may God's prayers and blessings be upon him’, which is used as a religious narrative clause to elevate the entity associated with the pronoun, ‘him’. This clause is functionally similar to ‘God bless America’, which sanctifies the object of the clause, in this case ‘America’. Such as use of religion should attract high scores of positivity for sentiment analysis, which appears to be unlikely for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores for Watson Defined Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emotion scores\n",
    "# entry[\"emotion\"][\"sadness\"], entry[\"emotion\"][\"joy\"], entry[\"emotion\"][\"fear\"], entry[\"emotion\"][\"disgust\"], entry[\"emotion\"][\"anger\"]) \n",
    "\n",
    "columns = [\"Entity\", \"Sentiment Score\", \"Sentiment Label\"]\n",
    "\n",
    "scores = lambda labels, table: pd.DataFrame( # get sentiment scores\n",
    "                                            [(entry[\"text\"], entry[\"sentiment\"][\"score\"], entry[\"sentiment\"][\"label\"]) \n",
    "                                                                                  \n",
    "                                             # iterate through table if positive/negative\n",
    "                                             for entry in table if entry[\"sentiment\"][\"label\"] in labels], \n",
    "                                            \n",
    "                                            # set column names\n",
    "                                            columns = columns) \\\n",
    "                                            \\\n",
    "                                            .sort_values(\"Sentiment Score\", ascending = label not in labels, ignore_index = True) \n",
    "\n",
    "for orator in docs.values():\n",
    "    results = orator['text']['analytics']['entities']\n",
    "    n = 10\n",
    "    display_side_by_side([scores(['positive', 'neutral'], results), scores(['negative'], results)], \n",
    "                         [f\"Top 10 Positive Scores for API Defined Entities in {orator['name']}'s Dataset'\", \n",
    "                          f\"Top 10 Negative Scores for API Defined Entities in {orator['name']}'s Dataset'\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Proximity of Named Entities to Seed Terms\n",
    "\n",
    "Get the scores for the words co-occuring with seed terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "group = [\"the United States of America\", \"Americans\", \"America\", \"The United States\"]\n",
    "\n",
    "term = None\n",
    "results = dict()\n",
    "for sentence in doc.sents:\n",
    "    terms = set(group).intersection(set([token.text.strip() for token in sentence]))\n",
    "    if terms:\n",
    "        term = list(terms)[0]\n",
    "        terms = [token.text for token in sentence if token.pos_ in [\"NOUN\", \"VERB\"]]\n",
    "        if term and term in results.keys():\n",
    "            results[term].extend(terms)\n",
    "        elif term:\n",
    "            results[term] = terms\n",
    "            \n",
    "ibm_df = dict()\n",
    "n = 1\n",
    "for entity, terms in tqdm.tqdm(results.items()):\n",
    "    \n",
    "    ibm_df[entity] = {\"positive\" : list(), \"negative\" : list(), \"neutral\" : list()}\n",
    "    for term in terms:\n",
    "        analytics = service.analyze(text=term, features=Features(\n",
    "                                    sentiment=SentimentOptions()),\n",
    "                                    language = \"en\").get_result()\n",
    "        sentiment = analytics['sentiment']['document']\n",
    "        score = {term : round(sentiment['score'], 2)}\n",
    "                                    \n",
    "        if sentiment['label'] == \"positive\":\n",
    "#             print(f\"appending {score} to {entity}['positive']\")\n",
    "            ibm_df[entity]['positive'].append(score)\n",
    "        \n",
    "        elif sentiment['label'] == \"negative\":\n",
    "#             print(f\"appending {score} to {entity}['negative']\")\n",
    "            ibm_df[entity]['negative'].append(score)\n",
    "        \n",
    "        elif sentiment['label'] == \"neutral\":\n",
    "#             print(f\"appending {score} to {entity}['neutral']\")\n",
    "            ibm_df[entity]['neutral'].append(score)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), f\"manual_cooccurring_scores.json\"), \"wb\") as f:\n",
    "    f.write(json.dumps(ibm_df).encode(\"utf-8\"))\n",
    "    \n",
    "filepath = os.getcwd()\n",
    "pickle_filename = \"ibm_cooccuring_scores_2.pkl\"\n",
    "with open(os.path.join(filepath, pickle_filename), 'wb') as file:\n",
    "    pickle.dump(ibm_df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from cndlib.visuals import display_side_by_side\n",
    "\n",
    "# load file from disc\n",
    "filepath = os.getcwd()\n",
    "pickle_filename = \"ibm_cooccuring_scores.pkl\"\n",
    "with open(os.path.join(filepath, pickle_filename), 'rb') as file:\n",
    "    ibm_df = pickle.load(file)\n",
    "\n",
    "def get_sentiment(entity):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to get the sentiment score for the entity being assessed\n",
    "    \"\"\"\n",
    "    \n",
    "    for target in docs['bush'][\"text\"]['analytics'][\"sentiment\"][\"targets\"]:\n",
    "        if target['text'] == entity.lower():\n",
    "            return target['score']\n",
    "        \n",
    "def get_averages_row(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to get the average scores for each sentiment polarity\n",
    "    \"\"\"\n",
    "    \n",
    "    averages = list()\n",
    "    for result in df.columns:\n",
    "        averages.append(f\"Average ({round(mean([score.get(list(score.keys())[0]) for score in df[result].tolist() if isinstance(score, dict)]), 2)})\")\n",
    "    return pd.DataFrame(dict(zip(df.columns, averages)), index=[0])\n",
    "\n",
    "## create DataFrames from the results\n",
    "dfs = [pd.DataFrame(dict([(k, pd.Series(v, dtype='object')) \n",
    "                          for k,v in ibm_df[d].items() \n",
    "                          if k in ['positive', 'negative', 'neutral']])) \n",
    "       for d in ibm_df]\n",
    "\n",
    "## function to get the length of the greater number of positive or negative results\n",
    "get_table_size = lambda df: max([value.count() for key, value in df.items() if key in ['positive', 'negative']])\n",
    "\n",
    "## cell formatting function to convert the dictionary results to a string\n",
    "format_cell = lambda x: f\"{list(x.items())[0][0]} ({list(x.items())[0][1]})\"\n",
    "\n",
    "## get DataFrame caption\n",
    "captions = [f\"Entity: '{d}' - sentiment {round(get_sentiment(d), 2)}\" for d in ibm_df]\n",
    "    \n",
    "## display DataFrames\n",
    "display_side_by_side([df\n",
    "                      .head(get_table_size(df)) # shrink table to longest of either positivity or negativity\n",
    "                      .applymap(format_cell, na_action='ignore') # reformat dictionary results to strings\n",
    "                      .fillna('').append(get_averages_row(df), ignore_index = True) # append the average scores for each columns\n",
    "                      .rename(columns={key : f\"{key.title()}, ({value.count()} Terms)\" for key, value in df.items()}) # rename columns to include number of entities\n",
    "                      for df in dfs], captions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy_v3",
   "language": "python",
   "name": "spacy_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
